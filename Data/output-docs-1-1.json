[
  {
    "title": "Resources Overview - Configuration Language | Terraform | HashiCorp Developer",
    "url": "https://developer.hashicorp.com/terraform/language/v1.9.x/resources",
    "html": "Skip to main content\nNews\nHashiCorp to join IBM to accelerate multi-cloud automation.\nRead the blog →\nDismiss alert\nTerraform\nInstall\nTutorials\nDocumentation\nRegistry\n(opens in new tab)\nTry Cloud\n(opens in new tab)\nSearch\nCommand or control key\nK key\nTerraform Home\nConfiguration Language\nConfiguration Language\nTerraform Language\nOverview\nStyle Guide\nFiles and Directories\nSyntax\nResources\nOverview\nResource Blocks\nResource Behavior\nMeta-Arguments\nProvisioners\nThe terraform_data Resource Type\nData Sources\nProviders\nVariables and Outputs\nModules\nChecks\nImport\nExpressions\nFunctions\nTerraform Settings\nState\nTests\nUpgrading to Terraform v1.7\nv1.x Compatibility Promises\nTerraform Internals\nResources\nTutorial Library\nCertifications\nCommunity Forum\n(opens in new tab)\nSupport\n(opens in new tab)\nGitHub\n(opens in new tab)\nTerraform Registry\n(opens in new tab)\n\nYou are viewing documentation for pre-release version v1.9.0 (alpha). View latest version.\n\nDeveloper\nTerraform\nConfiguration Language v1.9.x\nResources\nv1.9.0 (alpha)\nResources\n\nHands-on: Try the Terraform: Get Started tutorials.\n\nResources are the most important element in the Terraform language. Each resource block describes one or more infrastructure objects, such as virtual networks, compute instances, or higher-level components such as DNS records.\n\nResource Blocks documents the syntax for declaring resources.\n\nResource Behavior explains in more detail how Terraform handles resource declarations when applying a configuration.\n\nThe Meta-Arguments section documents special arguments that can be used with every resource type, including depends_on, count, for_each, provider, and lifecycle.\n\nProvisioners documents configuring post-creation actions for a resource using the provisioner and connection blocks. Since provisioners are non-declarative and potentially unpredictable, we strongly recommend that you treat them as a last resort.\n\nTheme\nDark\nLight\nSystem\nCertifications\nSystem Status\nCookie Manager\nTerms of Use\nSecurity\nPrivacy\nTrademark Policy\nTrade Controls\nGive Feedback\n(opens in new tab)\nWe use cookies & other similar technology to collect data to improve your experience on our site, as described in our Privacy Policy and Cookie Policy.\nManage PreferencesACCEPT"
  },
  {
    "title": "Upgrading to Terraform v1.2 | Terraform | HashiCorp Developer",
    "url": "https://developer.hashicorp.com/terraform/language/v1.2.x/upgrade-guides",
    "html": "Skip to main content\nNews\nHashiCorp to join IBM to accelerate multi-cloud automation.\nRead the blog →\nDismiss alert\nTerraform\nInstall\nTutorials\nDocumentation\nRegistry\n(opens in new tab)\nTry Cloud\n(opens in new tab)\nSearch\nCommand or control key\nK key\nTerraform Home\nConfiguration Language\nConfiguration Language\nTerraform Language\nOverview\nStyle Guide\nFiles and Directories\nSyntax\nResources\nData Sources\nProviders\nVariables and Outputs\nModules\nExpressions\nFunctions\nTerraform Settings\nState\nUpgrading to Terraform v1.2\nv1.x Compatibility Promises\nTerraform Internals\nResources\nTutorial Library\nCertifications\nCommunity Forum\n(opens in new tab)\nSupport\n(opens in new tab)\nGitHub\n(opens in new tab)\nTerraform Registry\n(opens in new tab)\n\nYou are viewing documentation for version v1.2.x. View latest version.\n\nDeveloper\nTerraform\nConfiguration Language v1.2.x\nUpgrading to Terraform v1.2\nv1.2.x\nUpgrading to Terraform v1.2\n\nDo you need the upgrade guide for an earlier version of Terraform? Use the version selector in the navigation bar to select the version you are intending to upgrade to.\n\nTerraform v1.2 is a minor release in the stable Terraform v1.0 series.\n\nTerraform v1.2 continues to honor the Terraform v1.0 Compatibility Promises, but there are some behavior changes outside of those promises that may affect a small number of users. Specifically, the following updates may require additional upgrade steps:\n\nterraform-credentials-env functionality is built into the Terraform CLI\nTerraform requires Linux kernel 2.6.32 or later\nRemote servers must support TLSv1.2\nOutdated TLS features are no longer supported\nThe terraform-credentials-env Functionality is Built Into the Terraform CLI\n\nWe recommend disabling the third-party credentials helper plugin terraform-credentials-env when you upgrade to Terraform v1.2. Terraform now contains similar built-in functionality.\n\nThe new v1.2 functionality supports the same environment variable naming scheme as the credentials helper, but has a difference in priority order. Specifically, TF_TOKEN_... environment variables now take priority over both credentials blocks in CLI configuration and credentials stored automatically when you run terraform login. After upgrading, ensure you do not specify credentials for the same host in multiple locations.\n\nWe also recommend upgrading the hashicorp/tfe provider to version 0.31 if you currently use it with the credentials helper to manage Terraform Cloud or Terraform Enterprise objects. Version 0.31 contains built-in support for the built-in TF_TOKEN_... environment variables.\n\nTerraform Requires Linux kernel 2.6.32 or Later\n\nThe Terraform runtime no longer supports Linux kernels prior to 2.6.32, and official releases of Terraform v1.2 for Linux require distributions using kernel 2.6.32 or later. The CLI behavior on earlier kernel versions is undefined.\n\nOutdated TLS Features Are No Longer Supported\n\nTerraform no longer supports the following features when making outgoing HTTPS or other TLS connections as a client:\n\nTLS v1.0 and v1.1. Terraform now requires the server to support TLS v1.2. All up-to-date servers should support TLS 1.2, and mainstream web browsers have required it since 2020.\nCA certificates signed using the SHA-1 hash function. Publicly-trusted Certificate Authorities have not issued SHA-1 certificates since 2015.\n\nThis change only affects requests from Terraform, including provider installation, module installation, and interactions with your configured backend. Provider plugins are separate programs that have their own rules about which TLS versions are supported.\n\nOn this page:\n\nUpgrading to Terraform v1.2\nThe terraform-credentials-env Functionality is Built Into the Terraform CLI\nTerraform Requires Linux kernel 2.6.32 or Later\nOutdated TLS Features Are No Longer Supported\nTheme\nDark\nLight\nSystem\nCertifications\nSystem Status\nCookie Manager\nTerms of Use\nSecurity\nPrivacy\nTrademark Policy\nTrade Controls\nGive Feedback\n(opens in new tab)\nWe use cookies & other similar technology to collect data to improve your experience on our site, as described in our Privacy Policy and Cookie Policy.\nManage PreferencesACCEPT"
  },
  {
    "title": "Upgrade Guides | Terraform | HashiCorp Developer",
    "url": "https://developer.hashicorp.com/terraform/language/v1.1.x/upgrade-guides",
    "html": "Skip to main content\nNews\nHashiCorp to join IBM to accelerate multi-cloud automation.\nRead the blog →\nDismiss alert\nTerraform\nInstall\nTutorials\nDocumentation\nRegistry\n(opens in new tab)\nTry Cloud\n(opens in new tab)\nSearch\nCommand or control key\nK key\nTerraform Home\nConfiguration Language\nConfiguration Language\nTerraform Language\nOverview\nFiles and Directories\nSyntax\nResources\nData Sources\nProviders\nVariables and Outputs\nModules\nExpressions\nFunctions\nTerraform Settings\nState\nUpgrade Guides\nOverview\nUpgrading to Terraform v1.1\nUpgrading to Terraform v1.0\nv1.x Compatibility Promises\nUpgrading to Terraform v0.15\nUpgrading to Terraform v0.14\nUpgrading to Terraform v0.13\nUpgrading to Terraform v0.12\nUpgrading to Terraform v0.11\nUpgrading to Terraform v0.10\nUpgrading to Terraform v0.9\nUpgrading to Terraform v0.8\nUpgrading to Terraform v0.7\nHistorical docs: 0.11 and Older\nResources\nTutorial Library\nCertifications\nCommunity Forum\n(opens in new tab)\nSupport\n(opens in new tab)\nGitHub\n(opens in new tab)\nTerraform Registry\n(opens in new tab)\n\nYou are viewing documentation for version v1.1 and earlier. View latest version.\n\nDeveloper\nTerraform\nConfiguration Language v1.1.x\nUpgrade Guides\nv1.1 and earlier\nUpgrade Guides\n\nTerraform's major and minor releases each include an upgrade guide which discusses any special steps that might be needed when upgrading to that new version.\n\nThis collection of upgrade guides is for Terraform v1.1 and earlier, due to the documentation version selected in the navigation bar. To see upgrade guides for later versions of Terraform, use the version selector in the navigation bar to select the version you are interested in.\n\nThe following historical version upgrade guides are available here:\n\nTerraform v1.1\nTerraform v1.0\nTerraform v0.15\nTerraform v0.14\nTerraform v0.13\nTerraform v0.12\nTerraform v0.11\nTerraform v0.10\nTerraform v0.9\nTerraform v0.8\nTerraform v0.7\nTheme\nDark\nLight\nSystem\nCertifications\nSystem Status\nCookie Manager\nTerms of Use\nSecurity\nPrivacy\nTrademark Policy\nTrade Controls\nGive Feedback\n(opens in new tab)\nWe use cookies & other similar technology to collect data to improve your experience on our site, as described in our Privacy Policy and Cookie Policy.\nManage PreferencesACCEPT"
  },
  {
    "title": "Upgrading to Terraform v1.3 | Terraform | HashiCorp Developer",
    "url": "https://developer.hashicorp.com/terraform/language/v1.3.x/upgrade-guides",
    "html": "Skip to main content\nNews\nHashiCorp to join IBM to accelerate multi-cloud automation.\nRead the blog →\nDismiss alert\nTerraform\nInstall\nTutorials\nDocumentation\nRegistry\n(opens in new tab)\nTry Cloud\n(opens in new tab)\nSearch\nCommand or control key\nK key\nTerraform Home\nConfiguration Language\nConfiguration Language\nTerraform Language\nOverview\nStyle Guide\nFiles and Directories\nSyntax\nResources\nData Sources\nProviders\nVariables and Outputs\nModules\nMoved block\nExpressions\nFunctions\nTerraform Settings\nState\nUpgrading to Terraform v1.3\nv1.x Compatibility Promises\nTerraform Internals\nResources\nTutorial Library\nCertifications\nCommunity Forum\n(opens in new tab)\nSupport\n(opens in new tab)\nGitHub\n(opens in new tab)\nTerraform Registry\n(opens in new tab)\n\nYou are viewing documentation for version v1.3.x. View latest version.\n\nDeveloper\nTerraform\nConfiguration Language v1.3.x\nUpgrading to Terraform v1.3\nv1.3.x\nUpgrading to Terraform v1.3\n\nNote: Use the version selector to view the upgrade guides for older Terraform versions.\n\nTerraform v1.3 is a minor release in the stable Terraform v1.0 series.\n\nTerraform v1.3 continues to honor the Terraform v1.0 Compatibility Promises, but there are some behavior changes outside of those promises that may affect a small number of users. Specifically, the following updates may require additional upgrade steps:\n\nRemoval of Deprecated State Storage Backends\nConcluding the Optional Attributes Experiment\nAzureRM Backend Requires Microsoft Graph\nOther Small Changes\n\nIf you encounter any problems during upgrading which are not by this guide, or if the migration instructions don't work for you, please start a topic in the Terraform community forum to discuss it.\n\nRemoval of Deprecated State Storage Backends\n\nTerraform currently requires that all supported state storage backends be maintained in the Terraform codebase and compiled into Terraform CLI. Terraform therefore contains a mixture of backends maintained by the Terraform CLI team, backends maintained by other teams at HashiCorp, and backends maintained by third-party contributors.\n\nThere are a number of backends that we have so far preserved on a best-effort basis despite them not having any active maintainers. Due to the overhead of continuing to support them, we deprecated the following unmaintained backends in Terraform v1.2.3:\n\nartifactory\netcd\netcdv3\nmanta\nswift\n\nAll of these deprecated state storage backends are now removed in Terraform v1.3. If you are using any of these you will need to migrate to another state storage backend using Terraform v1.2 before you upgrade to Terraform v1.3.\n\nThe following sections describe some specific migration considerations for each removed backend.\n\nMigrating from the artifactory backend\n\nFrom JFrog Artifactory 7.38.4 or later, Artifactory has support for the state storage protocol used by Terraform's remote backend, using a special repository type called a Terraform Backend Repository.\n\nThe remote backend was available in Terraform v1.2 and remains available in Terraform v1.3. If you are using the artifactory backend then we recommend migrating to the remote backend, using the configuration instructions provided by JFrog, before upgrading to Terraform v1.3.\n\nMigrating from the etcd and etcdv3 backends\n\nThe two generations of state storage backend for etcd have been removed and have no direct replacement.\n\nIf you are using etcd in conjunction with Kubernetes, you might choose to migrate to the kubernetes state storage backend, which stores Terraform state snapshots under a Kubernetes secret.\n\nMigrating from the manta backend\n\nThe Manta backend was written for an object storage system developed by Joyent. However, the backend was targeting the original implementation of that system which shut down in November 2019.\n\nThis backend has therefore been unmaintained for several years and is now removed without replacement.\n\nMigrating from the swift backend\n\nThe swift backend was for OpenStack's object storage system, Swift. This backend has not had an active maintainer for some time and has not kept up with new features and changes to Swift itself, and so it is now removed.\n\nOpenStack Swift contains an implementation of the Amazon S3 API. Although Terraform's s3 backend officially supports only Amazon's implementation of that API, we have heard from users that they have had success using that backend to store Terraform state snapshots in Swift.\n\nIf you intend to migrate to the s3 backend then you should complete that migration with Terraform v1.2 before you upgrade to Terraform v1.3.\n\nConcluding the Optional Attributes Experiment\n\nTerraform v0.14.0 introduced a new experimental language feature for declaring object type constraints with optional attributes in your module's input variables. Thanks to feedback from those who tried the experiment, a refinement of that functionality is now stablized in Terraform v1.3.\n\nFor general information on this new feature, see Optional Object Type Attributes.\n\nIf you have any experimental modules that were using the feature in its previous form, you can now adapt those modules for production use with the final form of the feature by making the following changes:\n\nRemove the experiments = [module_variable_optional_attrs] experiment opt-in from your module, and replace it with a Terraform version constraint inside the same terraform block:\n\nterraform {\n\n  required_version = \">= 1.3.0\"\n\n}\n\nCopy\n\nThis version constraint makes it explicit that your module is using language features added in Terraform v1.3.0, which earlier versions of Terraform can use to give better feedback about the module not being supported there.\n\nIf you were using the experimental defaults function, you will need to replace your use of it with the new syntax for declaring defaults as part of your main type constraint.\n\nFor example, you can declare a default value for an optional string attribute using a second argument to the optional syntax, inline in your type constraint expression:\n\n  type = object({\n\n    example = optional(string, \"default value\")\n\n  })\n\nCopy\n\nBecause the experiment is concluded, the experimental implementation of this feature is no longer available and Terraform v1.3.0 and later will not accept any module that contains the explicit experiment opt-in.\n\nAs with all new language features, you should take care to upgrade Terraform for all configurations which use a shared module before you use optional attributes in that shared module. Any module which must remain compatible with older versions of Terraform must not declare any optional attributes. Once all users of a module are using Terraform v1.3.0 or later, you can safely begin using optional attribute declarations.\n\nAzureRM Backend Requires Microsoft Graph\n\nIn response to Microsoft's deprecation of Azure AD Graph, Terraform v1.1 marked the beginning of a deprecation cycle for support of Azure AD Graph in Terraform's azurerm backend.\n\nThat deprecation cycle has now concluded with the total removal of Azure AD Graph support in Terraform v1.3. The AzureRM backend now supports only Microsoft Graph.\n\nIf you previously set use_microsoft_graph = true in your backend configuration to explicitly opt in to using the Microsoft Graph client instead of Azure AD Graph, you will need to now remove that argument from your backend configuration.\n\nIf you remove this setting in an already-initialized Terraform working directory then Terraform will detect it as a configuration change and prompt you to decide whether to migrate state to a new location. Because removing that setting does not change the physical location of the state snapshots, you should not tell Terraform to migrate the state to a new location and should instead use the -reconfigure option to terraform init:\n\nterraform init -reconfigure\n\nCopy\n\nIf you did not previously set the use_microsoft_graph argument then you do not need to make any changes. Microsoft Graph is now used by default and is the only available implementation.\n\nOther Small Changes\n\nThere are some other changes in Terraform v1.3 that we don't expect to have a great impact but may affect a small number of users:\n\nterraform import no longer supports the option -allow-missing-config. This option was originally added as a backward-compatibility helper when Terraform first began making use of the configuration during import, but the behavior of the import command was significantly limited by the requirement to be able to work without configuration, and so configuration is now required.\n\nIn most cases it is sufficient to write just an empty resource block whose resource type and name matches the address given on the terraform import command line. This will cause Terraform to associate the import operation with the default provider configuration for the provider that the resource belongs to.\n\nterraform show -json previously simplified the \"unknown\" status for all output values to be a single boolean value, even though an output value of a collection or structural type can potentially be only partially unknown.\n\nThe JSON output now accurately describes partially-unknown output values in the same way as it describes partially-unknown values in resource attributes. Any consumer of the plan JSON format which was relying on output values always being either known or entirely unknown must be changed to support more complex situations in the after_unknown property of the JSON Change Representation.\n\nWhen making requests to HTTPS servers, Terraform now rejects invalid TLS handshakes that have duplicate extensions, as required by RFC 5246 section 7.4.1.4 and RFC 8446 section 4.2. This action may cause new errors when interacting with existing buggy or misconfigured TLS servers, but should not affect correct servers.\n\nIf you see new HTTPS, TLS, or SSL-related error messages after upgrading to Terraform v1.3, that may mean that the server that Terraform tried to access has an incorrect implementation of the relevant protocols and needs an upgrade to a correct version for continued use with Terraform.\n\nSimilar problems can also arise on networks that use HTTPS-intercepting middleboxes, such as deep packet inspection firewalls. In that case, the protocol implementation of the middlebox must also be correct in order for Terraform to successfully access HTTPS servers through it.\n\nThis only applies to requests made directly by Terraform CLI, such as provider installation and remote state storage. Terraform providers are separate programs which decide their own policy for handling of TLS handshakes.\n\nOn this page:\n\nUpgrading to Terraform v1.3\nRemoval of Deprecated State Storage Backends\nConcluding the Optional Attributes Experiment\nAzureRM Backend Requires Microsoft Graph\nOther Small Changes\nTheme\nDark\nLight\nSystem\nCertifications\nSystem Status\nCookie Manager\nTerms of Use\nSecurity\nPrivacy\nTrademark Policy\nTrade Controls\nGive Feedback\n(opens in new tab)\nWe use cookies & other similar technology to collect data to improve your experience on our site, as described in our Privacy Policy and Cookie Policy.\nManage PreferencesACCEPT"
  },
  {
    "title": "Resources Overview - Configuration Language | Terraform | HashiCorp Developer",
    "url": "https://developer.hashicorp.com/terraform/language/v1.2.x/resources",
    "html": "Skip to main content\nNews\nHashiCorp to join IBM to accelerate multi-cloud automation.\nRead the blog →\nDismiss alert\nTerraform\nInstall\nTutorials\nDocumentation\nRegistry\n(opens in new tab)\nTry Cloud\n(opens in new tab)\nSearch\nCommand or control key\nK key\nTerraform Home\nConfiguration Language\nConfiguration Language\nTerraform Language\nOverview\nStyle Guide\nFiles and Directories\nSyntax\nResources\nOverview\nResource Blocks\nResource Behavior\nMeta-Arguments\nProvisioners\nData Sources\nProviders\nVariables and Outputs\nModules\nExpressions\nFunctions\nTerraform Settings\nState\nUpgrading to Terraform v1.2\nv1.x Compatibility Promises\nTerraform Internals\nResources\nTutorial Library\nCertifications\nCommunity Forum\n(opens in new tab)\nSupport\n(opens in new tab)\nGitHub\n(opens in new tab)\nTerraform Registry\n(opens in new tab)\n\nYou are viewing documentation for version v1.2.x. View latest version.\n\nDeveloper\nTerraform\nConfiguration Language v1.2.x\nResources\nv1.2.x\nResources\n\nHands-on: Try the Terraform: Get Started tutorials.\n\nResources are the most important element in the Terraform language. Each resource block describes one or more infrastructure objects, such as virtual networks, compute instances, or higher-level components such as DNS records.\n\nResource Blocks documents the syntax for declaring resources.\n\nResource Behavior explains in more detail how Terraform handles resource declarations when applying a configuration.\n\nThe Meta-Arguments section documents special arguments that can be used with every resource type, including depends_on, count, for_each, provider, and lifecycle.\n\nProvisioners documents configuring post-creation actions for a resource using the provisioner and connection blocks. Since provisioners are non-declarative and potentially unpredictable, we strongly recommend that you treat them as a last resort.\n\nTheme\nDark\nLight\nSystem\nCertifications\nSystem Status\nCookie Manager\nTerms of Use\nSecurity\nPrivacy\nTrademark Policy\nTrade Controls\nGive Feedback\n(opens in new tab)\nWe use cookies & other similar technology to collect data to improve your experience on our site, as described in our Privacy Policy and Cookie Policy.\nManage PreferencesACCEPT"
  },
  {
    "title": "Resources Overview - Configuration Language | Terraform | HashiCorp Developer",
    "url": "https://developer.hashicorp.com/terraform/language/v1.4.x/resources",
    "html": "Skip to main content\nNews\nHashiCorp to join IBM to accelerate multi-cloud automation.\nRead the blog →\nDismiss alert\nTerraform\nInstall\nTutorials\nDocumentation\nRegistry\n(opens in new tab)\nTry Cloud\n(opens in new tab)\nSearch\nCommand or control key\nK key\nTerraform Home\nConfiguration Language\nConfiguration Language\nTerraform Language\nOverview\nStyle Guide\nFiles and Directories\nSyntax\nResources\nOverview\nResource Blocks\nResource Behavior\nMeta-Arguments\nProvisioners\nThe terraform_data Resource Type\nData Sources\nProviders\nVariables and Outputs\nModules\nExpressions\nFunctions\nTerraform Settings\nState\nUpgrading to Terraform v1.4\nv1.x Compatibility Promises\nTerraform Internals\nResources\nTutorial Library\nCertifications\nCommunity Forum\n(opens in new tab)\nSupport\n(opens in new tab)\nGitHub\n(opens in new tab)\nTerraform Registry\n(opens in new tab)\n\nYou are viewing documentation for version v1.4.x. View latest version.\n\nDeveloper\nTerraform\nConfiguration Language v1.4.x\nResources\nv1.4.x\nResources\n\nHands-on: Try the Terraform: Get Started tutorials.\n\nResources are the most important element in the Terraform language. Each resource block describes one or more infrastructure objects, such as virtual networks, compute instances, or higher-level components such as DNS records.\n\nResource Blocks documents the syntax for declaring resources.\n\nResource Behavior explains in more detail how Terraform handles resource declarations when applying a configuration.\n\nThe Meta-Arguments section documents special arguments that can be used with every resource type, including depends_on, count, for_each, provider, and lifecycle.\n\nProvisioners documents configuring post-creation actions for a resource using the provisioner and connection blocks. Since provisioners are non-declarative and potentially unpredictable, we strongly recommend that you treat them as a last resort.\n\nTheme\nDark\nLight\nSystem\nCertifications\nSystem Status\nCookie Manager\nTerms of Use\nSecurity\nPrivacy\nTrademark Policy\nTrade Controls\nGive Feedback\n(opens in new tab)\nWe use cookies & other similar technology to collect data to improve your experience on our site, as described in our Privacy Policy and Cookie Policy.\nManage PreferencesACCEPT"
  },
  {
    "title": "Resources Overview - Configuration Language | Terraform | HashiCorp Developer",
    "url": "https://developer.hashicorp.com/terraform/language/v1.5.x/resources",
    "html": "Skip to main content\nNews\nHashiCorp to join IBM to accelerate multi-cloud automation.\nRead the blog →\nDismiss alert\nTerraform\nInstall\nTutorials\nDocumentation\nRegistry\n(opens in new tab)\nTry Cloud\n(opens in new tab)\nSearch\nCommand or control key\nK key\nTerraform Home\nConfiguration Language\nConfiguration Language\nTerraform Language\nOverview\nFiles and Directories\nSyntax\nResources\nOverview\nResource Blocks\nResource Behavior\nMeta-Arguments\nProvisioners\nThe terraform_data Resource Type\nData Sources\nProviders\nVariables and Outputs\nModules\nChecks\nImport\nExpressions\nFunctions\nTerraform Settings\nState\nUpgrading to Terraform v1.5\nv1.x Compatibility Promises\nTerraform Internals\nResources\nTutorial Library\nCertifications\nCommunity Forum\n(opens in new tab)\nSupport\n(opens in new tab)\nGitHub\n(opens in new tab)\nTerraform Registry\n(opens in new tab)\n\nYou are viewing documentation for version v1.5.x. View latest version.\n\nDeveloper\nTerraform\nConfiguration Language v1.5.x\nResources\nv1.5.x\nResources\n\nHands-on: Try the Terraform: Get Started tutorials.\n\nResources are the most important element in the Terraform language. Each resource block describes one or more infrastructure objects, such as virtual networks, compute instances, or higher-level components such as DNS records.\n\nResource Blocks documents the syntax for declaring resources.\n\nResource Behavior explains in more detail how Terraform handles resource declarations when applying a configuration.\n\nThe Meta-Arguments section documents special arguments that can be used with every resource type, including depends_on, count, for_each, provider, and lifecycle.\n\nProvisioners documents configuring post-creation actions for a resource using the provisioner and connection blocks. Since provisioners are non-declarative and potentially unpredictable, we strongly recommend that you treat them as a last resort.\n\nTheme\nDark\nLight\nSystem\nCertifications\nSystem Status\nCookie Manager\nTerms of Use\nSecurity\nPrivacy\nTrademark Policy\nTrade Controls\nGive Feedback\n(opens in new tab)\nWe use cookies & other similar technology to collect data to improve your experience on our site, as described in our Privacy Policy and Cookie Policy.\nManage PreferencesACCEPT"
  },
  {
    "title": "Tests - Configuration Language | Terraform | HashiCorp Developer",
    "url": "https://developer.hashicorp.com/terraform/language/v1.6.x/tests",
    "html": "Skip to main content\nNews\nHashiCorp to join IBM to accelerate multi-cloud automation.\nRead the blog →\nDismiss alert\nTerraform\nInstall\nTutorials\nDocumentation\nRegistry\n(opens in new tab)\nTry Cloud\n(opens in new tab)\nSearch\nCommand or control key\nK key\nTerraform Home\nConfiguration Language\nConfiguration Language\nTerraform Language\nOverview\nStyle Guide\nFiles and Directories\nSyntax\nResources\nData Sources\nProviders\nVariables and Outputs\nModules\nMoved block\nChecks\nImport\nExpressions\nFunctions\nTerraform Settings\nState\nTests\nUpgrading to Terraform v1.6\nv1.x Compatibility Promises\nTerraform Internals\nResources\nTutorial Library\nCertifications\nCommunity Forum\n(opens in new tab)\nSupport\n(opens in new tab)\nGitHub\n(opens in new tab)\nTerraform Registry\n(opens in new tab)\n\nYou are viewing documentation for version v1.6.x. View latest version.\n\nDeveloper\nTerraform\nConfiguration Language v1.6.x\nTests\nv1.6.x\nTests\n\nNote: This testing framework is available in Terraform v1.6.0 and later.\n\nTerraform tests let authors validate that module configuration updates do not introduce breaking changes. Tests run against test-specific, short-lived resources, preventing any risk to your existing infrastructure or state.\n\nIntegration or Unit testing\n\nBy default, tests within Terraform create real infrastructure and can run assertions and validations against that infrastructure. This is analogous to integration testing because you are testing Terraform's core functionality by executing operations and validating the infrastructure Terraform creates.\n\nYou can override the normal testing behavior by updating the command attribute within a run block (examples below). By default, each run block executes with command = apply instructing Terraform to execute a complete apply operation against your configuration. Replacing the command value with command = plan instructs Terraform to not create new infrastructure for this run block. This allows test authors to validate logical operations and custom conditions within their infrastructure in a process analogous to unit testing.\n\nSyntax\n\nEach Terraform test lives in a test file. Terraform discovers test files are based on their file extension: .tftest.hcl or .tftest.json.\n\nEach test file contains the following root level attributes and blocks:\n\nOne to many run blocks.\nZero to one variables block.\nZero to many provider blocks.\n\nTerraform executes run blocks in order, simulating a series of Terraform commands executing directly within the configuration directory. The order of the variables and provider blocks doesn't matter, Terraform processes all the values within these blocks at the beginning of the test operation. We recommend defining your variables and provider blocks first, at the beginning of the test file.\n\nExample\n\nThe following example demonstrates a simple Terraform configuration that creates an AWS S3 bucket, using an input variable to modify the name. We will create an example test file (below) that validates the buckets name is created as expected.\n\n# main.tf\n\n\n\nprovider \"aws\" {\n\n    region = \"eu-central-1\"\n\n}\n\n\n\nvariable \"bucket_prefix\" {\n\n  type = string\n\n}\n\n\n\nresource \"aws_s3_bucket\" \"bucket\" {\n\n  bucket = \"${var.bucket_prefix}-bucket\"\n\n}\n\n\n\noutput \"bucket_name\" {\n\n  value = aws_s3_bucket.bucket.bucket\n\n}\n\nCopy\n\nThe following test file runs a single Terraform plan command which creates the S3 bucket, and then validates the logic for calculating the name is correct by checking the actual name matches the expected name.\n\n# valid_string_concat.tftest.hcl\n\n\n\nvariables {\n\n  bucket_prefix = \"test\"\n\n}\n\n\n\nrun \"valid_string_concat\" {\n\n\n\n  command = plan\n\n\n\n  assert {\n\n    condition     = aws_s3_bucket.bucket.bucket == \"test-bucket\"\n\n    error_message = \"S3 bucket name did not match expected\"\n\n  }\n\n\n\n}\n\nCopy\nRun blocks\n\nEach run block has the following fields and blocks:\n\nField or Block Name\tDescription\tDefault Value\ncommand\tAn optional attribute, which is either apply or plan.\tapply\nplan_options.mode\tAn optional attribute, which is either normal or refresh-only.\tnormal\nplan_options.refresh\tAn optional boolean attribute.\ttrue\nplan_options.replace\tAn optional attribute containing a list of resource addresses referencing resources within the configuration under test.\t\nplan_options.target\tAn optional attribute containing a list of resource addresses referencing resources within the configuration under test.\t\nvariables\tAn optional variables block.\t\nmodule\tAn optional module block.\t\nproviders\tAn optional providers attribute.\t\nassert\tOptional assert blocks.\t\nexpect_failures\tAn optional attribute.\t\n\nThe command attribute and plan_options block tell Terraform which command and options to execute for each run block. The default operation, if you do not specify a command attribute or the plan_options block, is a normal Terraform apply operation.\n\nThe command attribute states whether the operation should be a plan or an apply operation.\n\nThe plan_options block allows test authors to customize the planning mode and options they would typically need to edit via command-line flags and options. We cover the -var and -var-file options in the Variables section.\n\nAssertions\n\nTerraform run block assertions are Custom Conditions, consisting of a condition and an error message.\n\nAt the conclusion of a Terraform test command execution, Terraform presents any failed assertions as part of a tests passed or failed status.\n\nAssertion References\n\nAssertions within tests can reference any existing named values that are available to other custom conditions within the main Terraform configuration.\n\nAdditionally, test assertions can directly reference outputs from current and previous run blocks. Pulling from the previous example, this is a valid condition: condition = output.bucket_name == \"test_bucket\".\n\nVariables\n\nYou can provide values for Input Variables within your configuration directly from your test files.\n\nThe test file syntax supports variables blocks at both the root level and within run blocks. Terraform passes all variable values from the test file into all run blocks within the file. You can override variable values for a particular run block with values provided directly within that run block.\n\nAdding to the test file from the example above:\n\n# variable_precedence.tftest.hcl\n\n\n\nvariables {\n\n  bucket_prefix = \"test\"\n\n}\n\n\n\nrun \"uses_root_level_value\" {\n\n\n\n  command = plan\n\n\n\n  assert {\n\n    condition     = aws_s3_bucket.bucket.bucket == \"test-bucket\"\n\n    error_message = \"S3 bucket name did not match expected\"\n\n  }\n\n\n\n}\n\n\n\nrun \"overrides_root_level_value\" {\n\n\n\n  command = plan\n\n\n\n  variables {\n\n    bucket_prefix = \"other\"\n\n  }\n\n\n\n  assert {\n\n    condition     = aws_s3_bucket.bucket.bucket == \"other-bucket\"\n\n    error_message = \"S3 bucket name did not match expected\"\n\n  }\n\n\n\n}\n\nCopy\n\nWe've added a second run block that specifies the bucket_prefix variable value as other, overriding the value test that is provided by the test file and used during the first run block.\n\nSpecify variables with the Command Line or definition files\n\nIn addition to specifying variable values via test files, the Terraform test command also supports the other typical mechanisms for specifying variable values.\n\nYou can specify values for variables across all tests with the Command Line and with Variable Definition Files.\n\nThis is particularly useful for using sensitive variables values and for configuring providers. Otherwise, testing files could directly expose those sensitive values.\n\nVariable definition precedence\n\nVariable Definition Precedence remains the same within tests, except for variable values that test files provide. The variables defined in test files take the highest precedence, overriding environment variables, variables files, or command-line input.\n\nVariable References\n\nVariables you define within run blocks can refer to outputs from modules executed in earlier run blocks and variables defined at higher precedence levels.\n\nFor example, the following code block shows how a variable can refer to higher precedence variables and previous run blocks:\n\nvariables {\n\n  global_value = \"some value\"\n\n}\n\n\n\nrun \"run_block_one\" {\n\n  variables {\n\n    local_value = var.global_value\n\n  }\n\n\n\n  # ...\n\n  # Some test assertions should go here.\n\n  # ...\n\n}\n\n\n\nrun \"run_block_two\" {\n\n  variables {\n\n    local_value = run.run_block_one.output_one\n\n  }\n\n\n\n  # ...\n\n  # Some test assertions should go here.\n\n  # ...\n\n}\n\nCopy\n\nAbove, the local_value in run_block_one gets its value from the global_value variable. This pattern is useful if you want to assign multiple variables the same value. You can specify a variable value once at the file level and then share it with different variables.\n\nIn comparison, local_value in run_block_two takes its value from the output value of output_one from run_block_one. This pattern is useful for passing values between run blocks, particularly if run blocks are executing different modules as detailed in the Modules section.\n\nProviders\n\nYou can set or override the required providers within the main configuration from your testing files by using provider and providers blocks and attributes.\n\nAt the root level of a Terraform testing file, you can define provider blocks as if Terraform were creating them within the main configuration. Terraform will then pass these provider blocks into its configuration as each run block executes.\n\nBy default, each provider you specify is directly available within each run block. You can customize the availability of providers within a given run block by using a providers attribute. The behavior and syntax for this block match the behavior of providers meta-argument.\n\nIf you do not provide provider configuration within a testing file, Terraform attempts to initialize any providers within its configuration using the provider's default settings. For example, any environment variables aimed at configuring providers are still available, and Terraform can use them to create default providers.\n\nBelow, we expand on our previous example to allow tests, instead of the configuration, to specify the region. In this example, we are going to test the following configuration file:\n\n# main.tf\n\n\n\nterraform {\n\n  required_providers {\n\n    aws = {\n\n      source = \"hashicorp/aws\"\n\n    }\n\n  }\n\n}\n\n\n\nvariable \"bucket_prefix\" {\n\n  type = string\n\n}\n\n\n\nresource \"aws_s3_bucket\" \"bucket\" {\n\n  bucket = \"${var.bucket_prefix}-bucket\"\n\n}\n\n\n\noutput \"bucket_name\" {\n\n  value = aws_s3_bucket.bucket.bucket\n\n}\n\nCopy\n\nWe can now define our provider blocks within the following test file:\n\n# customised_provider.tftest.hcl\n\n\n\nprovider \"aws\" {\n\n    region = \"eu-central-1\"\n\n}\n\n\n\nvariables {\n\n  bucket_prefix = \"test\"\n\n}\n\n\n\nrun \"valid_string_concat\" {\n\n\n\n  command = plan\n\n\n\n  assert {\n\n    condition     = aws_s3_bucket.bucket.bucket == \"test-bucket\"\n\n    error_message = \"S3 bucket name did not match expected\"\n\n  }\n\n\n\n}\n\nCopy\n\nWe can also create a more complex example configuration, that makes use of multiple providers and aliases:\n\n# main.tf\n\n\n\nterraform {\n\n  required_providers {\n\n    aws = {\n\n      source                = \"hashicorp/aws\"\n\n      configuration_aliases = [aws.secondary]\n\n    }\n\n  }\n\n}\n\n\n\nvariable \"bucket_prefix\" {\n\n  default = \"test\"\n\n  type    = string\n\n}\n\n\n\nresource \"aws_s3_bucket\" \"primary_bucket\" {\n\n  bucket = \"${var.bucket_prefix}-primary\"\n\n}\n\n\n\nresource \"aws_s3_bucket\" \"secondary_bucket\" {\n\n  provider = aws.secondary\n\n  bucket   = \"${var.bucket_prefix}-secondary\"\n\n}\n\nCopy\n\nWithin our test file we can specify multiple providers:\n\n# customised_providers.tftest.hcl\n\n\n\nprovider \"aws\" {\n\n  region = \"us-east-1\"\n\n}\n\n\n\nprovider \"aws\" {\n\n  alias  = \"secondary\"\n\n  region = \"eu-central-1\"\n\n}\n\n\n\nrun \"providers\" {\n\n\n\n  command = plan\n\n\n\n  assert {\n\n    condition     = aws_s3_bucket.primary_bucket.bucket == \"test-primary\"\n\n    error_message = \"invalid value for primary S3 bucket\"\n\n  }\n\n\n\n  assert {\n\n    condition     = aws_s3_bucket.secondary_bucket.bucket == \"test-secondary\"\n\n    error_message = \"invalid value for secondary S3 bucket\"\n\n  }\n\n}\n\nCopy\n\nIt is also possible to define specific providers you want to use in specific run blocks:\n\n# main.tf\n\n\n\nterraform {\n\n  required_providers {\n\n    aws = {\n\n      source                = \"hashicorp/aws\"\n\n      configuration_aliases = [aws.secondary]\n\n    }\n\n  }\n\n}\n\n\n\ndata \"aws_region\" \"primary\" {}\n\n\n\ndata \"aws_region\" \"secondary\" {\n\n  provider = aws.secondary\n\n}\n\n\n\nvariable \"bucket_prefix\" {\n\n  default = \"test\"\n\n  type    = string\n\n}\n\n\n\nresource \"aws_s3_bucket\" \"primary_bucket\" {\n\n  bucket = \"${var.bucket_prefix}-${data.aws_region.primary.name}-primary\"\n\n}\n\n\n\nresource \"aws_s3_bucket\" \"secondary_bucket\" {\n\n  provider = aws.secondary\n\n  bucket   = \"${var.bucket_prefix}-${data.aws_region.secondary.name}-secondary\"\n\n}\n\nCopy\n\nOur test file can pass in specific providers for each different run block:\n\n# customised_providers.tftest.hcl\n\n\n\nprovider \"aws\" {\n\n  region = \"us-east-1\"\n\n}\n\n\n\nprovider \"aws\" {\n\n  alias  = \"secondary\"\n\n  region = \"eu-central-1\"\n\n}\n\n\n\nprovider \"aws\" {\n\n  alias  = \"tertiary\"\n\n  region = \"eu-west-2\"\n\n}\n\n\n\nrun \"default_providers\" {\n\n\n\n  command = plan\n\n\n\n  assert {\n\n    condition     = aws_s3_bucket.primary_bucket.bucket == \"test-us-east-1-primary\"\n\n    error_message = \"invalid value for primary S3 bucket\"\n\n  }\n\n\n\n  assert {\n\n    condition     = aws_s3_bucket.secondary_bucket.bucket == \"test-eu-central-1-secondary\"\n\n    error_message = \"invalid value for secondary S3 bucket\"\n\n  }\n\n}\n\n\n\nrun \"customised_providers\" {\n\n\n\n  command = plan\n\n\n\n  providers = {\n\n    aws           = aws\n\n    aws.secondary = aws.tertiary\n\n  }\n\n\n\n  assert {\n\n    condition     = aws_s3_bucket.primary_bucket.bucket == \"test-us-east-1-primary\"\n\n    error_message = \"invalid value for primary S3 bucket\"\n\n  }\n\n\n\n  assert {\n\n    condition     = aws_s3_bucket.secondary_bucket.bucket == \"test-eu-west-2-secondary\"\n\n    error_message = \"invalid value for secondary S3 bucket\"\n\n  }\n\n}\n\nCopy\n\nNote: When running tests with command = apply, switching providers between run blocks can result in failed operations and tests because resources created by one provider definition will be unusable when modified by a second.\n\nModules\n\nYou can modify the module that a given run block executes.\n\nBy default, Terraform executes the given command against the configuration being tested for each run block. Terraform tests the configuration within the directory you execute the terraform test command from (or the directory you point to with the -chdir argument). Each run block also allows the user to change the targeted configuration using the module block.\n\nUnlike the traditional module block, the module block within test files only supports the source attribute and the version attribute. The remaining attributes that are typically supplied via the traditional module block should be supplied by the alternate attributes and blocks within the run block.\n\nNote: Terraform test files only support local and registry modules within the source attribute.\n\nAll other blocks and attributes within the run block are supported when executing an alternate module, with assert blocks executing against values from the alternate module. This is discussed more in Modules State.\n\nTwo example use cases for the modules block within a testing file are:\n\nA setup module that creates the infrastructure the main configuration requires for testing.\nA loading module to load and validate secondary infrastructure (such as data sources) that are not created directly by the main configuration being tested.\n\nThe following examples demonstrate both of these use cases.\n\nFirst, we have a module that will create and load several files into an already created S3 bucket. This is the configuration we want to test.\n\n# main.tf\n\n\n\nvariable \"bucket\" {\n\n  type = string\n\n}\n\n\n\nvariable \"files\" {\n\n  type = map(string)\n\n}\n\n\n\ndata \"aws_s3_bucket\" \"bucket\" {\n\n  bucket = var.bucket\n\n}\n\n\n\nresource \"aws_s3_object\" \"object\" {\n\n  for_each = var.files\n\n\n\n  bucket = data.aws_s3_bucket.bucket.id\n\n  key = each.key\n\n  source = each.value\n\n\n\n  etag = filemd5(each.value)\n\n}\n\nCopy\n\nSecond, we have a setup module that will create the S3 bucket, so it is available to the configuration under test.\n\n# testing/setup/main.tf\n\n\n\nvariable \"bucket\" {\n\n  type = string\n\n}\n\n\n\nresource \"aws_s3_bucket\" \"bucket\" {\n\n  bucket = var.bucket\n\n}\n\nCopy\n\nThird, we have a loading module, that will load the files in the s3 bucket. This is a fairly contrived example, as it is definitely possible just to validate the files directly when they are created in the module under test. It is, however, good for demonstrating the use case.\n\n# testing/loader/main.tf\n\n\n\nvariable \"bucket\" {\n\n  type = string\n\n}\n\n\n\ndata \"aws_s3_objects\" \"objects\" {\n\n  bucket = var.bucket\n\n}\n\nCopy\n\nFinally, we have the test file itself which configures everything and calls out to the various helper modules we have created.\n\n# file_count.tftest.hcl\n\n\n\nvariables {\n\n  bucket = \"my_test_bucket\"\n\n  files = {\n\n    \"file-one.txt\": \"data/files/file_one.txt\"\n\n    \"file-two.txt\": \"data/files/file_two.txt\"\n\n  }\n\n}\n\n\n\nprovider \"aws\" {\n\n  region = \"us-east-1\"\n\n}\n\n\n\nrun \"setup\" {\n\n  # Create the S3 bucket we will use later.\n\n\n\n  module {\n\n    source = \"./testing/setup\"\n\n  }\n\n}\n\n\n\nrun \"execute\" {\n\n  # This is empty, we just run the configuration under test using all the default settings.\n\n}\n\n\n\nrun \"verify\" {\n\n  # Load and count the objects created in the \"execute\" run block.\n\n\n\n  module {\n\n    source = \"./testing/loader\"\n\n  }\n\n\n\n  assert {\n\n    condition = length(data.aws_s3_objects.objects.keys) == 2\n\n    error_message = \"created the wrong number of s3 objects\"\n\n  }\n\n}\n\nCopy\n\nNote: The loader module only uses data sources for resources created by the setup module, and does not load data sources based on the main configuration.\n\nThe state for the main configuration is destroyed first when Terraform performs the cleanup operations after the test has completed. Any data sources in alternate modules based on the main configuration will likely error during the cleanup operation as they will attempt to be refreshed, and will return unavailable or not found errors as the underlying infrastructure has already been destroyed.\n\nThe order of the destroy operations changed starting with the Terraform v1.7 series and tests written for future versions do not need this consideration.\n\nModules state\n\nWhile Terraform executes a terraform test command, Terraform maintains at least one, but possibly many, state files within memory for each test file.\n\nThere is always at least one state file that maintains the state of the main configuration under test. This state file is shared by all run blocks that do not have a module block specifying an alternate module to load.\n\nAdditionally, there is one state file per alternate module that Terraform loads. An alternate module state file is shared by all run blocks that execute the given module.\n\nThe Terraform team is interested in any use cases requiring manual state management or the ability to execute different configurations against the same state within the test command. If you have a use case, please file an issue and share it with us.\n\nThe following example uses comments to explain where the state files for each run block originate. In the below example Terraform creates and manages a total of three state files. The first state file is for the main configuration under test, the second for the setup module, and the third for the loader module.\n\nrun \"setup\" {\n\n\n\n  # This run block references an alternate module and is the first run block\n\n  # to reference this particular alternate module. Therefore, Terraform creates\n\n  # and populates a new empty state file for this run block.\n\n\n\n  module {\n\n    source = \"./testing/setup\"\n\n  }\n\n}\n\n\n\nrun \"init\" {\n\n\n\n  # This run block does not reference an alternate module, so it uses the main\n\n  # state file for the configuration under test. As this is the first run block\n\n  # to reference the main configuration, the previously empty state file now\n\n  # contains the resources created by this run block.\n\n\n\n  assert {\n\n    # In practice we'd do some interesting checks and tests here but the\n\n    # assertions aren't important for this example.\n\n  }\n\n\n\n  # ... more assertions ...\n\n}\n\n\n\nrun \"update_setup\" {\n\n\n\n  # We've now re-referenced the setup module, so the state file that was created\n\n  # for the first \"setup\" run block will be reused. It will contain any\n\n  # resources that were created as part of the other run block before this run\n\n  # block executes and will be updated with any changes made by this run block\n\n  # after.\n\n\n\n  module {\n\n    source = \"./testing/setup\"\n\n  }\n\n\n\n  variables {\n\n    # In practice, we'd likely make some changes to the module compared to the\n\n    # first run block here. Otherwise, there would be no point recalling the\n\n    # module.\n\n  }\n\n}\n\n\n\nrun \"update\" {\n\n\n\n  # As with the \"init\" run block, we are executing against the main configuration\n\n  # again. This means we'd load the main state file that was initially populated\n\n  # by the \"init\" run block, and any changes made by this \"run\" block will be\n\n  # carried forward to any future run blocks that execute against the main\n\n  # configuration.\n\n\n\n  # ... updated variables ...\n\n\n\n  # ... assertions ...\n\n}\n\n\n\nrun \"loader\" {\n\n\n\n  # This run block is now referencing our second alternate module so will create\n\n  # our third and final state file. The other two state files are managing\n\n  # resources from the main configuration and resources from the setup module.\n\n  # We are getting a new state file for this run block as the loader module has\n\n  # not previously been referenced by any run blocks.\n\n\n\n  module {\n\n    source = \"./testing/loader\"\n\n  }\n\n}\n\n\n\nCopy\nModules Cleanup\n\nAt the conclusion of a test file, Terraform attempts to destroy every resource it created during the execution of that test file. When Terraform loads alternate modules, the order in which Terraform destroys those objects in is important. For example, in the first Modules example, we could not destroy the resources created in the \"setup\" run block before the objects created in the \"execute\" run block, because the S3 bucket we created in the \"setup\" step can not be destroyed while it contains objects.\n\nTerraform destroys resources in the following order, and this order is important because it affects the structure of your testing files:\n\nResources in the main state file. Do not create resources in alternate modules that depend on resources from your main configuration.\nData sources can refer to objects in your main configuration, because Terraform does not have to destroy data sources.\nResources created by alternate modules in reverse run block order.\n\nFrom our example, any resources created in the \"verify\" run block would be destroyed before resources created in the \"setup\" run block. Note, that in our example this doesn't particularly matter as our \"verify\" run block only loads a data source and creates no resources.\n\nIf you use a single setup module as an alternate module, and it executes first, or you use no alternate modules, then the order of destruction does not affect you. Anything more complex may require careful consideration to make sure the destruction of resources can complete automatically.\n\nExpecting failures\n\nBy default, if any Custom Conditions, including check block assertions, fail during the execution of a Terraform test file then the overall command reports the test as a failure.\n\nHowever, it is a common testing paradigm to want to test failure cases. Terraform supports the expect_failures attribute for this use case.\n\nIn each run block the expect_failures attribute can provide a list of checkable objects (resources, data sources, check blocks, input variables, and outputs) that should fail their custom conditions. The test passes if the checkable objects you specify report an issue, and the test fails overall if they do not.\n\nYou can still write assertions alongside an expect_failures block, but you should be mindful that all custom conditions, except check block assertions, halt the execution of Terraform. This still applies during test execution, so your assertions should only consider values that you are sure will be computed before the checkable object is due to fail. You can manage this using references, or the depends_on meta-argument within your main configuration.\n\nThis also means that, with the exception of check blocks, you can only reliably include a single checkable object. We support a list of checkable objects within the expect_failures attribute purely for check blocks.\n\nA quick example below demonstrates testing the validation block on an input variable. The configuration file accepts a single input variable that must be even number.\n\n# main.tf\n\n\n\nvariable \"input\" {\n\n  type = number\n\n\n\n  validation {\n\n    condition = var.input % 2 == 0\n\n    error_message = \"must be even number\"\n\n  }\n\n}\n\nCopy\n\nThe test file contains two run blocks. One that validates that our custom condition passes on an even number and one that validates our custom condition fails on an odd number.\n\n# input_validation.tftest.hcl\n\n\n\nvariables {\n\n  input = 0\n\n}\n\n\n\nrun \"zero\" {\n\n  # The variable defined above is even, so we expect the validation to pass.\n\n\n\n  command = plan\n\n}\n\n\n\nrun \"one\" {\n\n  # This time we set the variable is odd, so we expect the validation to fail.\n\n\n\n  command = plan\n\n\n\n  variables {\n\n    input = 1\n\n  }\n\n\n\n  expect_failures = [\n\n    var.input,\n\n  ]\n\n}\n\nCopy\n\nNote: Terraform only expects failures in the operation specified by the command attribute of the run block.\n\nBe careful when using expect_failures in run blocks with command = apply. A run block with command = apply that expects a custom condition failure will fail overall if that custom condition fails during the plan.\n\nThis is logically consistent, as the run block is expecting to be able to run an apply operation but can not because the plan failed. It is also potentially confusing, as you will see the failure in the diagnostics as the reason the test failed, even though that failure was marked as being expected.\n\nThere are instances when Terraform does not execute a custom condition during the planning stage, because that condition is relying on computed attributes that are only available after Terraform creates the referenced resource. In these cases, you could use an expect_failures block alongside a command = apply attribute and value. However, in most cases we recommend only using expect_failures alongside command = plan operations.\n\nNote: Expected failures only apply to user-defined custom conditions.\n\nOther kinds of failure besides the specified expected failures in the checkable object still result in the overall test failing. For example, a variable that expects a boolean value as input fails the surrounding test if Terraform provides the wrong kind of value, even if that variable is included in an expect_failures attribute.\n\nThe expect_failures attribute is included to allow authors to test their configuration and any logic defined within. A type mismatch, as in the previous example, is not something Terraform authors should have to worry about testing as Terraform itself will handle enforce type constraints. As such, you can only expect_failures in custom conditions.\n\nOn this page:\n\nTests\nIntegration or Unit testing\nSyntax\nRun blocks\nVariables\nProviders\nModules\nExpecting failures\nTheme\nDark\nLight\nSystem\nCertifications\nSystem Status\nCookie Manager\nTerms of Use\nSecurity\nPrivacy\nTrademark Policy\nTrade Controls\nGive Feedback\n(opens in new tab)\nWe use cookies & other similar technology to collect data to improve your experience on our site, as described in our Privacy Policy and Cookie Policy.\nManage PreferencesACCEPT"
  },
  {
    "title": "Resources Overview - Configuration Language | Terraform | HashiCorp Developer",
    "url": "https://developer.hashicorp.com/terraform/language/v1.7.x/resources",
    "html": "Skip to main content\nNews\nHashiCorp to join IBM to accelerate multi-cloud automation.\nRead the blog →\nDismiss alert\nTerraform\nInstall\nTutorials\nDocumentation\nRegistry\n(opens in new tab)\nTry Cloud\n(opens in new tab)\nSearch\nCommand or control key\nK key\nTerraform Home\nConfiguration Language\nConfiguration Language\nTerraform Language\nOverview\nStyle Guide\nFiles and Directories\nSyntax\nResources\nOverview\nResource Blocks\nResource Behavior\nMeta-Arguments\nProvisioners\nThe terraform_data Resource Type\nData Sources\nProviders\nVariables and Outputs\nModules\nMoved block\nChecks\nImport\nExpressions\nFunctions\nTerraform Settings\nState\nTests\nUpgrading to Terraform v1.7\nv1.x Compatibility Promises\nTerraform Internals\nResources\nTutorial Library\nCertifications\nCommunity Forum\n(opens in new tab)\nSupport\n(opens in new tab)\nGitHub\n(opens in new tab)\nTerraform Registry\n(opens in new tab)\n\nYou are viewing documentation for version v1.7.x. View latest version.\n\nDeveloper\nTerraform\nConfiguration Language v1.7.x\nResources\nv1.7.x\nResources\n\nHands-on: Try the Terraform: Get Started tutorials.\n\nResources are the most important element in the Terraform language. Each resource block describes one or more infrastructure objects, such as virtual networks, compute instances, or higher-level components such as DNS records.\n\nResource Blocks documents the syntax for declaring resources.\n\nResource Behavior explains in more detail how Terraform handles resource declarations when applying a configuration.\n\nThe Meta-Arguments section documents special arguments that can be used with every resource type, including depends_on, count, for_each, provider, and lifecycle.\n\nProvisioners documents configuring post-creation actions for a resource using the provisioner and connection blocks. Since provisioners are non-declarative and potentially unpredictable, we strongly recommend that you treat them as a last resort.\n\nTheme\nDark\nLight\nSystem\nCertifications\nSystem Status\nCookie Manager\nTerms of Use\nSecurity\nPrivacy\nTrademark Policy\nTrade Controls\nGive Feedback\n(opens in new tab)\nWe use cookies & other similar technology to collect data to improve your experience on our site, as described in our Privacy Policy and Cookie Policy.\nManage PreferencesACCEPT"
  },
  {
    "title": "Resources Overview - Configuration Language | Terraform | HashiCorp Developer",
    "url": "https://developer.hashicorp.com/terraform/language/v1.6.x/resources",
    "html": "Skip to main content\nNews\nHashiCorp to join IBM to accelerate multi-cloud automation.\nRead the blog →\nDismiss alert\nTerraform\nInstall\nTutorials\nDocumentation\nRegistry\n(opens in new tab)\nTry Cloud\n(opens in new tab)\nSearch\nCommand or control key\nK key\nTerraform Home\nConfiguration Language\nConfiguration Language\nTerraform Language\nOverview\nStyle Guide\nFiles and Directories\nSyntax\nResources\nOverview\nResource Blocks\nResource Behavior\nMeta-Arguments\nProvisioners\nThe terraform_data Resource Type\nData Sources\nProviders\nVariables and Outputs\nModules\nMoved block\nChecks\nImport\nExpressions\nFunctions\nTerraform Settings\nState\nTests\nUpgrading to Terraform v1.6\nv1.x Compatibility Promises\nTerraform Internals\nResources\nTutorial Library\nCertifications\nCommunity Forum\n(opens in new tab)\nSupport\n(opens in new tab)\nGitHub\n(opens in new tab)\nTerraform Registry\n(opens in new tab)\n\nYou are viewing documentation for version v1.6.x. View latest version.\n\nDeveloper\nTerraform\nConfiguration Language v1.6.x\nResources\nv1.6.x\nResources\n\nHands-on: Try the Terraform: Get Started tutorials.\n\nResources are the most important element in the Terraform language. Each resource block describes one or more infrastructure objects, such as virtual networks, compute instances, or higher-level components such as DNS records.\n\nResource Blocks documents the syntax for declaring resources.\n\nResource Behavior explains in more detail how Terraform handles resource declarations when applying a configuration.\n\nThe Meta-Arguments section documents special arguments that can be used with every resource type, including depends_on, count, for_each, provider, and lifecycle.\n\nProvisioners documents configuring post-creation actions for a resource using the provisioner and connection blocks. Since provisioners are non-declarative and potentially unpredictable, we strongly recommend that you treat them as a last resort.\n\nTheme\nDark\nLight\nSystem\nCertifications\nSystem Status\nCookie Manager\nTerms of Use\nSecurity\nPrivacy\nTrademark Policy\nTrade Controls\nGive Feedback\n(opens in new tab)\nWe use cookies & other similar technology to collect data to improve your experience on our site, as described in our Privacy Policy and Cookie Policy.\nManage PreferencesACCEPT"
  },
  {
    "title": "Module Sources | Terraform | HashiCorp Developer",
    "url": "https://developer.hashicorp.com/terraform/language/modules/sources",
    "html": "Skip to main content\nNews\nHashiCorp to join IBM to accelerate multi-cloud automation.\nRead the blog →\nDismiss alert\nTerraform\nInstall\nTutorials\nDocumentation\nRegistry\n(opens in new tab)\nTry Cloud\n(opens in new tab)\nSearch\nCommand or control key\nK key\nTerraform Home\nConfiguration Language\nConfiguration Language\nStyle Guide\nFiles and Directories\nSyntax\nResources\nData Sources\nProviders\nVariables and Outputs\nModules\nOverview\nModule Blocks\nModule Sources\nMeta-Arguments\nModule Development\nMoved block\nChecks\nImport\nExpressions\nFunctions\nTerraform Settings\nState\nTests\nUpgrading to Terraform v1.7\nv1.x Compatibility Promises\nTerraform Internals\nResources\nTutorial Library\nCertifications\nCommunity Forum\n(opens in new tab)\nSupport\n(opens in new tab)\nGitHub\n(opens in new tab)\nTerraform Registry\n(opens in new tab)\nDeveloper\nTerraform\nConfiguration Language\nModules\nModule Sources\nv1.8.x (latest)\nModule Sources\n\nThe source argument in a module block tells Terraform where to find the source code for the desired child module.\n\nTerraform uses this during the module installation step of terraform init to download the source code to a directory on local disk so that other Terraform commands can use it.\n\nHands-on: Try the Use Modules From the Registry or Build and Use a Local Module tutorials.\n\nThe module installer supports installation from a number of different source types.\n\nLocal paths\n\nTerraform Registry\n\nGitHub\n\nBitbucket\n\nGeneric Git, Mercurial repositories\n\nHTTP URLs\n\nS3 buckets\n\nGCS buckets\n\nModules in Package Sub-directories\n\nEach of these is described in the following sections. Module source addresses use a URL-like syntax, but with extensions to support unambiguous selection of sources and additional features.\n\nWe recommend using local file paths for closely-related modules used primarily for the purpose of factoring out repeated code elements, and using a native Terraform module registry for modules intended to be shared by multiple calling configurations. We support other sources so that you can potentially distribute Terraform modules internally with existing infrastructure.\n\nMany of the source types will make use of \"ambient\" credentials available when Terraform is run, such as from environment variables or credentials files in your home directory. This is covered in more detail in each of the following sections.\n\nWe recommend placing each module that is intended to be re-usable in the root of its own repository or archive file, but it is also possible to reference modules from subdirectories.\n\nLocal Paths\n\nLocal path references allow for factoring out portions of a configuration within a single source repository.\n\nmodule \"consul\" {\n\n  source = \"./consul\"\n\n}\n\nCopy\n\nA local path must begin with either ./ or ../ to indicate that a local path is intended, to distinguish from a module registry address.\n\nLocal paths are special in that they are not \"installed\" in the same sense that other sources are: the files are already present on local disk (possibly as a result of installing a parent module) and so can just be used directly. Their source code is automatically updated if the parent module is upgraded.\n\nNote that Terraform does not consider an absolute filesystem path (starting with a slash, a drive letter, or similar) to be a local path. Instead, Terraform will treat that in a similar way as a remote module and copy it into the local module cache. An absolute path is a \"package\" in the sense described in Modules in Package Sub-directories. We don't recommend using absolute filesystem paths to refer to Terraform modules, because it will tend to couple your configuration to the filesystem layout of a particular computer.\n\nTerraform Registry\n\nA module registry is the native way of distributing Terraform modules for use across multiple configurations, using a Terraform-specific protocol that has full support for module versioning.\n\nTerraform Registry is an index of modules shared publicly using this protocol. This public registry is the easiest way to get started with Terraform and find modules created by others in the community.\n\nYou can also use a private registry, either via the built-in feature from HCP Terraform, or by running a custom service that implements the module registry protocol.\n\nModules on the public Terraform Registry can be referenced using a registry source address of the form <NAMESPACE>/<NAME>/<PROVIDER>, with each module's information page on the registry site including the exact address to use.\n\nmodule \"consul\" {\n\n  source = \"hashicorp/consul/aws\"\n\n  version = \"0.1.0\"\n\n}\n\nCopy\n\nThe above example will use the Consul module for AWS from the public registry.\n\nFor modules hosted in other registries, prefix the source address with an additional <HOSTNAME>/ portion, giving the hostname of the private registry:\n\nmodule \"consul\" {\n\n  source = \"app.terraform.io/example-corp/k8s-cluster/azurerm\"\n\n  version = \"1.1.0\"\n\n}\n\nCopy\n\nIf you are using the SaaS version of HCP Terraform, its private registry hostname is app.terraform.io. If you use a self-hosted Terraform Enterprise instance, its private registry hostname is the same as the host where you'd access the web UI and the host you'd use when configuring the HCP Terraform CLI integration.\n\nBoth HCP Terraform and self-hosted Terraform Enterprise support a generic hostname localterraform.com.\n\nRegistry modules support versioning. You can provide a specific version as shown in the above examples, or use flexible version constraints.\n\nYou can learn more about the registry at the Terraform Registry documentation.\n\nTo access modules from a private registry, you may need to configure an access token in the CLI config. Use the same hostname as used in the module source string. For a private registry within HCP Terraform, use the same authentication token as you would use with the Enterprise API or command-line clients.\n\nGitHub\n\nTerraform will recognize unprefixed github.com URLs and interpret them automatically as Git repository sources.\n\nmodule \"consul\" {\n\n  source = \"github.com/hashicorp/example\"\n\n}\n\nCopy\n\nThe above address scheme will clone over HTTPS. To clone over SSH, use the following form:\n\nmodule \"consul\" {\n\n  source = \"git@github.com:hashicorp/example.git\"\n\n}\n\nCopy\n\nThese GitHub schemes are treated as convenient aliases for the general Git repository address scheme, and so they obtain credentials in the same way and support the ref argument for selecting a specific revision. You will need to configure credentials in particular to access private repositories.\n\nBitbucket\n\nTerraform will recognize unprefixed bitbucket.org URLs and interpret them automatically as BitBucket repositories:\n\nmodule \"consul\" {\n\n  source = \"bitbucket.org/hashicorp/terraform-consul-aws\"\n\n}\n\nCopy\n\nThis shorthand works only for public repositories, because Terraform must access the BitBucket API to learn if the given repository uses Git or Mercurial.\n\nTerraform treats the result either as a Git source or a Mercurial source depending on the repository type. See the sections on each version control type for information on how to configure credentials for private repositories and how to specify a specific revision to install.\n\nGeneric Git Repository\n\nArbitrary Git repositories can be used by prefixing the address with the special git:: prefix. After this prefix, any valid Git URL can be specified to select one of the protocols supported by Git.\n\nFor example, to use HTTPS or SSH:\n\nmodule \"vpc\" {\n\n  source = \"git::https://example.com/vpc.git\"\n\n}\n\n\n\nmodule \"storage\" {\n\n  source = \"git::ssh://username@example.com/storage.git\"\n\n}\n\nCopy\n\nTerraform installs modules from Git repositories by running git clone, and so it will respect any local Git configuration set on your system, including credentials. To access a non-public Git repository, configure Git with suitable credentials for that repository.\n\nIf you use the SSH protocol then any configured SSH keys will be used automatically. This is the most common way to access non-public Git repositories from automated systems because it allows access to private repositories without interactive prompts.\n\nIf using the HTTP/HTTPS protocol, or any other protocol that uses username/password credentials, configure Git Credentials Storage to select a suitable source of credentials for your environment.\n\nIf your Terraform configuration will be used within HCP Terraform, only SSH key authentication is supported, and keys can be configured on a per-workspace basis.\n\nSelecting a Revision\n\nBy default, Terraform will clone and use the default branch (referenced by HEAD) in the selected repository. You can override this using the ref argument. The value of the ref argument can be any reference that would be accepted by the git checkout command, such as branch, SHA-1 hash (short or full), or tag names. For a full list of the possible values, see Git Tools - Revision Selection in the Git Book.\n\n# select a specific tag\n\nmodule \"vpc\" {\n\n  source = \"git::https://example.com/vpc.git?ref=v1.2.0\"\n\n}\n\n\n\n# directly select a commit using its SHA-1 hash\n\nmodule \"storage\" {\n\n  source = \"git::https://example.com/storage.git?ref=51d462976d84fdea54b47d80dcabbf680badcdb8\"\n\n}\n\nCopy\nShallow Clone\n\nFor larger repositories you may prefer to make only a shallow clone in order to reduce the time taken to retrieve the remote repository.\n\nThe depth URL argument corresponds to the --depth argument to git clone, which instructs Git to create a shallow clone that includes only the specified number of commits in the history. Setting depth to 1 is suitable for most cases. This is because Terraform only uses the most recently selected commit to find the source.\n\nmodule \"vpc\" {\n\n  source = \"git::https://example.com/vpc.git?depth=1&ref=v1.2.0\"\n\n}\n\nCopy\n\nHowever, because shallow clone requires different Git protocol behavior, setting the depth argument makes Terraform pass your ref argument, if any, to the --branch argument to git clone instead. That means it must specify a named branch or tag known to the remote repository, and that raw commit IDs are not acceptable.\n\n\"scp-like\" address syntax\n\nWhen using Git over SSH, we recommend using the ssh://-prefixed URL form for consistency with all of the other URL-like git address forms. You may opt to use the alternative \"scp-like\" syntax instead, in which case you must omit the ssh:// scheme part and include only the git:: part. For example:\n\nmodule \"storage\" {\n\n  source = \"git::username@example.com:storage.git\"\n\n}\n\nCopy\n\nIf you use the ssh:// URL scheme then Terraform will assume that the colon marks the beginning of a port number, rather than the beginning of the path. This matches how Git itself interprets these different forms, aside from the Terraform-specific git:: selector prefix.\n\nGeneric Mercurial Repository\n\nYou can use arbitrary Mercurial repositories by prefixing the address with the special hg:: prefix. After this prefix, any valid Mercurial URL can be specified to select one of the protocols supported by Mercurial.\n\nmodule \"vpc\" {\n\n  source = \"hg::http://example.com/vpc.hg\"\n\n}\n\nCopy\n\nTerraform installs modules from Mercurial repositories by running hg clone, and so it will respect any local Mercurial configuration set on your system, including credentials. To access a non-public repository, configure Mercurial with suitable credentials for that repository.\n\nIf you use the SSH protocol then any configured SSH keys will be used automatically. This is the most common way to access non-public Mercurial repositories from automated systems because it allows access to private repositories without interactive prompts.\n\nIf your Terraform configuration will be used within HCP Terraform, only SSH key authentication is supported, and keys can be configured on a per-workspace basis.\n\nSelecting a Revision\n\nYou can select a non-default branch or tag using the optional ref argument:\n\nmodule \"vpc\" {\n\n  source = \"hg::http://example.com/vpc.hg?ref=v1.2.0\"\n\n}\n\nCopy\nHTTP URLs\n\nWhen you use an HTTP or HTTPS URL, Terraform will make a GET request to the given URL, which can return another source address. This indirection allows using HTTP URLs as a sort of \"vanity redirect\" over a more complicated module source address.\n\nTerraform will append an additional query string argument terraform-get=1 to the given URL before sending the GET request, allowing the server to optionally return a different result when Terraform is requesting it.\n\nIf the response is successful (200-range status code), Terraform looks in the following locations in order for the next address to access:\n\nThe value of a response header field named X-Terraform-Get.\n\nIf the response is an HTML page, a meta element with the name terraform-get:\n\n<meta name=\"terraform-get\" content=\"github.com/hashicorp/example\" />\n\nCopy\n\nIn either case, the result is interpreted as another module source address using one of the forms documented elsewhere on this page.\n\nIf an HTTP/HTTPS URL requires authentication credentials, use a .netrc file to configure the credentials. By default, Terraform searches for the .netrc file in your HOME directory. However, you can override the default filesystem location by setting the NETRC environment variable. For information on the .netrc format, refer to the documentation for using it in curl.\n\nFetching archives over HTTP\n\nAs a special case, if Terraform detects that the URL has a common file extension associated with an archive file format then it will bypass the special terraform-get=1 redirection described above and instead just use the contents of the referenced archive as the module source code:\n\nmodule \"vpc\" {\n\n  source = \"https://example.com/vpc-module.zip\"\n\n}\n\nCopy\n\nThe extensions that Terraform recognizes for this special behavior are:\n\nzip\ntar.bz2 and tbz2\ntar.gz and tgz\ntar.xz and txz\n\nIf your URL doesn't have one of these extensions but refers to an archive anyway, use the archive argument to force this interpretation:\n\nmodule \"vpc\" {\n\n  source = \"https://example.com/vpc-module?archive=zip\"\n\n}\n\nCopy\n\nNote: If the content of the archive file is a directory, you will need to include that directory in the module source. Read the section on Modules in Package Sub-directories for more information.\n\nS3 Bucket\n\nYou can use archives stored in S3 as module sources using the special s3:: prefix, followed by an S3 bucket object URL.\n\nmodule \"consul\" {\n\n  source = \"s3::https://s3-eu-west-1.amazonaws.com/examplecorp-terraform-modules/vpc.zip\"\n\n}\n\nCopy\n\nNote: Buckets in AWS's us-east-1 region must use the hostname s3.amazonaws.com (instead of s3-us-east-1.amazonaws.com).\n\nThe s3:: prefix causes Terraform to use AWS-style authentication when accessing the given URL. As a result, this scheme may also work for other services that mimic the S3 API, as long as they handle authentication in the same way as AWS.\n\nThe resulting object must be an archive with one of the same file extensions as for archives over standard HTTP. Terraform will extract the archive to obtain the module source tree.\n\nThe module installer looks for AWS credentials in the following locations, preferring those earlier in the list when multiple are available:\n\nThe AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY environment variables.\nThe default profile in the .aws/credentials file in your home directory.\nIf running on an EC2 instance, temporary credentials associated with the instance's IAM Instance Profile.\nGCS Bucket\n\nYou can use archives stored in Google Cloud Storage as module sources using the special gcs:: prefix, followed by a GCS bucket object URL.\n\nFor example\n\ngcs::https://www.googleapis.com/storage/v1/BUCKET_NAME/PATH_TO_MODULE\ngcs::https://www.googleapis.com/storage/v1/BUCKET_NAME/PATH/TO/module.zip\nmodule \"consul\" {\n\n  source = \"gcs::https://www.googleapis.com/storage/v1/modules/foomodule.zip\"\n\n}\n\nCopy\n\nThe module installer uses Google Cloud SDK to authenticate with GCS. You can use any of the following methods to set Google Cloud Platform credentials:\n\nSet the GOOGLE_OAUTH_ACCESS_TOKEN environment variable to a raw Google Cloud Platform OAuth access token.\nEnter the path of your service account key file in the GOOGLE_APPLICATION_CREDENTIALS environment variable.\nIf you're running Terraform from a GCE instance, default credentials are automatically available. See Creating and Enabling Service Accounts for Instances for more details.\nOn your computer, you can make your Google identity available by running gcloud auth application-default login.\nModules in Package Sub-directories\n\nWhen the source of a module is a version control repository or archive file (generically, a \"package\"), the module itself may be in a sub-directory relative to the root of the package.\n\nA special double-slash syntax is interpreted by Terraform to indicate that the remaining path after that point is a sub-directory within the package. For example:\n\nhashicorp/consul/aws//modules/consul-cluster\ngit::https://example.com/network.git//modules/vpc\nhttps://example.com/network-module.zip//modules/vpc\ns3::https://s3-eu-west-1.amazonaws.com/examplecorp-terraform-modules/network.zip//modules/vpc\n\nIf the source address has arguments, such as the ref argument supported for the version control sources, the sub-directory portion must be before those arguments:\n\ngit::https://example.com/network.git//modules/vpc?ref=v1.2.0\ngithub.com/hashicorp/example//modules/vpc?ref=v1.2.0\n\nTerraform will still extract the entire package to local disk, but will read the module from the subdirectory. As a result, it is safe for a module in a sub-directory of a package to use a local path to another module as long as it is in the same package.\n\nEdit this page on GitHub\n\nOn this page:\n\nModule Sources\nLocal Paths\nTerraform Registry\nGitHub\nBitbucket\nGeneric Git Repository\nGeneric Mercurial Repository\nHTTP URLs\nS3 Bucket\nGCS Bucket\nModules in Package Sub-directories\nTheme\nDark\nLight\nSystem\nCertifications\nSystem Status\nCookie Manager\nTerms of Use\nSecurity\nPrivacy\nTrademark Policy\nTrade Controls\nGive Feedback\n(opens in new tab)\nWe use cookies & other similar technology to collect data to improve your experience on our site, as described in our Privacy Policy and Cookie Policy.\nManage PreferencesACCEPT"
  },
  {
    "title": "References to Values - Configuration Language | Terraform | HashiCorp Developer",
    "url": "https://developer.hashicorp.com/terraform/language/expressions/references#terraform-workspace",
    "html": "Skip to main content\nNews\nHashiCorp to join IBM to accelerate multi-cloud automation.\nRead the blog →\nDismiss alert\nTerraform\nInstall\nTutorials\nDocumentation\nRegistry\n(opens in new tab)\nTry Cloud\n(opens in new tab)\nSearch\nCommand or control key\nK key\nTerraform Home\nConfiguration Language\nConfiguration Language\nStyle Guide\nFiles and Directories\nSyntax\nResources\nData Sources\nProviders\nVariables and Outputs\nModules\nMoved block\nChecks\nImport\nExpressions\nOverview\nTypes and Values\nStrings and Templates\nReferences to Values\nOperators\nFunction Calls\nConditional Expressions\nFor Expressions\nSplat Expressions\nDynamic Blocks\nCustom Conditions\nType Constraints\nVersion Constraints\nFunctions\nTerraform Settings\nState\nTests\nUpgrading to Terraform v1.7\nv1.x Compatibility Promises\nTerraform Internals\nResources\nTutorial Library\nCertifications\nCommunity Forum\n(opens in new tab)\nSupport\n(opens in new tab)\nGitHub\n(opens in new tab)\nTerraform Registry\n(opens in new tab)\nDeveloper\nTerraform\nConfiguration Language\nExpressions\nReferences to Values\nv1.8.x (latest)\nReferences to Named Values\n\nHands-on: Try the Create Dynamic Expressions tutorial.\n\nTerraform makes several kinds of named values available. Each of these names is an expression that references the associated value. You can use them as standalone expressions, or combine them with other expressions to compute new values.\n\nTypes of Named Values\n\nThe main kinds of named values available in Terraform are:\n\nResources\nInput variables\nLocal values\nChild module outputs\nData sources\nFilesystem and workspace info\nBlock-local values\n\nThe sections below explain each kind of named value in detail.\n\nAlthough many of these names use dot-separated paths that resemble attribute notation for elements of object values, they are not implemented as real objects. This means you must use them exactly as written: you cannot use square-bracket notation to replace the dot-separated paths, and you cannot iterate over the \"parent object\" of a named entity; for example, you cannot use aws_instance in a for expression to iterate over every AWS instance resource.\n\nResources\n\n<RESOURCE TYPE>.<NAME> represents a managed resource of the given type and name.\n\nThe value of a resource reference can vary, depending on whether the resource uses count or for_each:\n\nIf the resource doesn't use count or for_each, the reference's value is an object. The resource's attributes are elements of the object, and you can access them using dot or square bracket notation.\nIf the resource has the count argument set, the reference's value is a list of objects representing its instances.\nIf the resource has the for_each argument set, the reference's value is a map of objects representing its instances.\n\nAny named value that does not match another pattern listed below will be interpreted by Terraform as a reference to a managed resource.\n\nFor more information about how to use resource references, see references to resource attributes below.\n\nInput Variables\n\nvar.<NAME> is the value of the input variable of the given name.\n\nIf the variable has a type constraint (type argument) as part of its declaration, Terraform will automatically convert the caller's given value to conform to the type constraint.\n\nFor that reason, you can safely assume that a reference using var. will always produce a value that conforms to the type constraint, even if the caller provided a value of a different type that was automatically converted.\n\nIn particular, note that if you define a variable as being of an object type with particular attributes then only those specific attributes will be available in expressions elsewhere in the module, even if the caller actually passed in a value with additional attributes. You must define in the type constraint all of the attributes you intend to use elsewhere in your module.\n\nLocal Values\n\nlocal.<NAME> is the value of the local value of the given name.\n\nLocal values can refer to other local values, even within the same locals block, as long as you don't introduce circular dependencies.\n\nChild Module Outputs\n\nmodule.<MODULE NAME> is an value representing the results of a module block.\n\nIf the corresponding module block does not have either count nor for_each set then the value will be an object with one attribute for each output value defined in the child module. To access one of the module's output values, use module.<MODULE NAME>.<OUTPUT NAME>.\n\nIf the corresponding module uses for_each then the value will be a map of objects whose keys correspond with the keys in the for_each expression, and whose values are each objects with one attribute for each output value defined in the child module, each representing one module instance.\n\nIf the corresponding module uses count then the result is similar to for for_each except that the value is a list with the requested number of elements, each one representing one module instance.\n\nData Sources\n\ndata.<DATA TYPE>.<NAME> is an object representing a data resource of the given data source type and name. If the resource has the count argument set, the value is a list of objects representing its instances. If the resource has the for_each argument set, the value is a map of objects representing its instances.\n\nFor more information, see References to Resource Attributes, which also applies to data resources aside from the addition of the data. prefix to mark the reference as for a data resource.\n\nFilesystem and Workspace Info\n\nThe following values are available:\n\npath.module is the filesystem path of the module where the expression is placed. We do not recommend using path.module in write operations because it can produce different behavior depending on whether you use remote or local module sources. Multiple invocations of local modules use the same source directory, overwriting the data in path.module during each call. This can lead to race conditions and unexpected results.\npath.root is the filesystem path of the root module of the configuration.\npath.cwd is the filesystem path of the original working directory from where you ran Terraform before applying any -chdir argument. This path is an absolute path that includes details about the filesystem structure. It is also useful in some advanced cases where Terraform is run from a directory other than the root module directory. We recommend using path.root or path.module over path.cwd where possible.\nterraform.workspace is the name of the currently selected workspace.\n\nUse the values in this section carefully, because they include information about the context in which a configuration is being applied and so may inadvertently hurt the portability or composability of a module.\n\nFor example, if you use path.cwd directly to populate a path into a resource argument then later applying the same configuration from a different directory or on a different computer with a different directory structure will cause the provider to consider the change of path to be a change to be applied, even if the path still refers to the same file.\n\nSimilarly, if you use any of these values as a form of namespacing in a shared module, such as using terraform.workspace as a prefix for globally-unique object names, it may not be possible to call your module more than once in the same configuration.\n\nAside from path.module, we recommend using the values in this section only in the root module of your configuration. If you are writing a shared module which needs a prefix to help create unique names, define an input variable for your module and allow the calling module to define the prefix. The calling module can then use terraform.workspace to define it if appropriate, or some other value if not:\n\nmodule \"example\" {\n\n  # ...\n\n\n\n  name_prefix = \"app-${terraform.workspace}\"\n\n}\n\nCopy\nBlock-Local Values\n\nWithin the bodies of certain blocks, or in some other specific contexts, there are other named values available beyond the global values listed above. These local names are described in the documentation for the specific contexts where they appear. Some of most common local names are:\n\ncount.index, in resources that use the count meta-argument.\neach.key / each.value, in resources that use the for_each meta-argument.\nself, in provisioner and connection blocks.\n\nNote: Local names are often referred to as variables or temporary variables in their documentation. These are not input variables; they are just arbitrary names that temporarily represent a value.\n\nThe names in this section relate to top-level configuration blocks only. If you use dynamic blocks to dynamically generate resource-type-specific nested blocks within resource and data blocks then you'll refer to the key and value of each element differently. See the dynamic blocks documentation for details.\n\nNamed Values and Dependencies\n\nConstructs like resources and module calls often use references to named values in their block bodies, and Terraform analyzes these expressions to automatically infer dependencies between objects. For example, an expression in a resource argument that refers to another managed resource creates an implicit dependency between the two resources.\n\nReferences to Resource Attributes\n\nThe most common reference type is a reference to an attribute of a resource which has been declared either with a resource or data block. Because the contents of such blocks can be quite complicated themselves, expressions referring to these contents can also be complicated.\n\nConsider the following example resource block:\n\nresource \"aws_instance\" \"example\" {\n\n  ami           = \"ami-abc123\"\n\n  instance_type = \"t2.micro\"\n\n\n\n  ebs_block_device {\n\n    device_name = \"sda2\"\n\n    volume_size = 16\n\n  }\n\n  ebs_block_device {\n\n    device_name = \"sda3\"\n\n    volume_size = 20\n\n  }\n\n}\n\nCopy\n\nThe documentation for aws_instance lists all of the arguments and nested blocks supported for this resource type, and also lists a number of attributes that are exported by this resource type. All of these different resource type schema constructs are available for use in references, as follows:\n\nThe ami argument set in the configuration can be used elsewhere with the reference expression aws_instance.example.ami.\n\nThe id attribute exported by this resource type can be read using the same syntax, giving aws_instance.example.id.\n\nThe arguments of the ebs_block_device nested blocks can be accessed using a splat expression. For example, to obtain a list of all of the device_name values, use aws_instance.example.ebs_block_device[*].device_name.\n\nThe nested blocks in this particular resource type do not have any exported attributes, but if ebs_block_device were to have a documented id attribute then a list of them could be accessed similarly as aws_instance.example.ebs_block_device[*].id.\n\nSometimes nested blocks are defined as taking a logical key to identify each block, which serves a similar purpose as the resource's own name by providing a convenient way to refer to that single block in expressions. If aws_instance had a hypothetical nested block type device that accepted such a key, it would look like this in configuration:\n\n  device \"foo\" {\n\n    size = 2\n\n  }\n\n  device \"bar\" {\n\n    size = 4\n\n  }\n\nCopy\n\nArguments inside blocks with keys can be accessed using index syntax, such as aws_instance.example.device[\"foo\"].size.\n\nTo obtain a map of values of a particular argument for labelled nested block types, use a for expression: {for k, device in aws_instance.example.device : k => device.size}.\n\nWhen a resource has the count argument set, the resource itself becomes a list of instance objects rather than a single object. In that case, access the attributes of the instances using either splat expressions or index syntax:\n\naws_instance.example[*].id returns a list of all of the ids of each of the instances.\naws_instance.example[0].id returns just the id of the first instance.\n\nWhen a resource has the for_each argument set, the resource itself becomes a map of instance objects rather than a single object, and attributes of instances must be specified by key, or can be accessed using a for expression.\n\naws_instance.example[\"a\"].id returns the id of the \"a\"-keyed resource.\n[for value in aws_instance.example: value.id] returns a list of all of the ids of each of the instances.\n\nNote that unlike count, splat expressions are not directly applicable to resources managed with for_each, as splat expressions must act on a list value. However, you can use the values() function to extract the instances as a list and use that list value in a splat expression:\n\nvalues(aws_instance.example)[*].id\nSensitive Resource Attributes\n\nWhen defining the schema for a resource type, a provider developer can mark certain attributes as sensitive, in which case Terraform will show a placeholder marker (sensitive value) instead of the actual value when rendering a plan involving that attribute.\n\nA provider attribute marked as sensitive behaves similarly to an an input variable declared as sensitive, where Terraform will hide the value in the plan and apply messages and will also hide any other values you derive from it as sensitive. However, there are some limitations to that behavior as described in Cases where Terraform may disclose a sensitive variable.\n\nIf you use a sensitive value from a resource attribute as part of an output value then Terraform will require you to also mark the output value itself as sensitive, to confirm that you intended to export it.\n\nTerraform will still record sensitive values in the state, and so anyone who can access the state data will have access to the sensitive values in cleartext. For more information, see Sensitive Data in State.\n\nNote: Treating values derived from a sensitive resource attribute as sensitive themselves was introduced in Terraform v0.15. Earlier versions of Terraform will obscure the direct value of a sensitive resource attribute, but will not automatically obscure other values derived from sensitive resource attributes.\n\nValues Not Yet Known\n\nWhen Terraform is planning a set of changes that will apply your configuration, some resource attribute values cannot be populated immediately because their values are decided dynamically by the remote system. For example, if a particular remote object type is assigned a generated unique id on creation, Terraform cannot predict the value of this id until the object has been created.\n\nTerraform uses special unknown value placeholders for information that it cannot predict during the plan phase. The Terraform language automatically handles unknown values in expressions. For example, adding a known value to an unknown value automatically produces an unknown value as a result.\n\nHowever, there are some situations where unknown values do have a significant effect:\n\nThe count meta-argument for resources cannot be unknown, since it must be evaluated during the plan phase to determine how many instances are to be created.\n\nIf unknown values are used in the configuration of a data resource, that data resource cannot be read during the plan phase and so it will be deferred until the apply phase. In this case, the results of the data resource will also be unknown values.\n\nIf an unknown value is assigned to an argument inside a module block, any references to the corresponding input variable within the child module will use that unknown value.\n\nIf an unknown value is used in the value argument of an output value, any references to that output value in the parent module will use that unknown value.\n\nTerraform will attempt to validate that unknown values are of suitable types where possible, but incorrect use of such values may not be detected until the apply phase, causing the apply to fail.\n\nUnknown values appear in the terraform plan output as (known after apply).\n\nEdit this page on GitHub\n\nOn this page:\n\nReferences to Named Values\nTypes of Named Values\nNamed Values and Dependencies\nReferences to Resource Attributes\nTheme\nDark\nLight\nSystem\nCertifications\nSystem Status\nCookie Manager\nTerms of Use\nSecurity\nPrivacy\nTrademark Policy\nTrade Controls\nGive Feedback\n(opens in new tab)\nWe use cookies & other similar technology to collect data to improve your experience on our site, as described in our Privacy Policy and Cookie Policy.\nManage PreferencesACCEPT"
  },
  {
    "title": "Functions - Configuration Language | Terraform | HashiCorp Developer",
    "url": "https://developer.hashicorp.com/terraform/language/functions",
    "html": "Skip to main content\nNews\nHashiCorp to join IBM to accelerate multi-cloud automation.\nRead the blog →\nDismiss alert\nTerraform\nInstall\nTutorials\nDocumentation\nRegistry\n(opens in new tab)\nTry Cloud\n(opens in new tab)\nSearch\nCommand or control key\nK key\nTerraform Home\nConfiguration Language\nConfiguration Language\nStyle Guide\nFiles and Directories\nSyntax\nResources\nData Sources\nProviders\nVariables and Outputs\nModules\nMoved block\nChecks\nImport\nExpressions\nFunctions\nOverview\nNumeric Functions\nString Functions\nCollection Functions\nEncoding Functions\nFilesystem Functions\nDate and Time Functions\nHash and Crypto Functions\nIP Network Functions\nType Conversion Functions\nTerraform-specific Functions\nTerraform Settings\nState\nTests\nUpgrading to Terraform v1.7\nv1.x Compatibility Promises\nTerraform Internals\nResources\nTutorial Library\nCertifications\nCommunity Forum\n(opens in new tab)\nSupport\n(opens in new tab)\nGitHub\n(opens in new tab)\nTerraform Registry\n(opens in new tab)\nDeveloper\nTerraform\nConfiguration Language\nFunctions\nv1.8.x (latest)\nBuilt-in Functions\n\nHands-on: Try the Perform Dynamic Operations with Functions tutorial.\n\nThe Terraform language includes a number of built-in functions that you can call from within expressions to transform and combine values. The general syntax for function calls is a function name followed by comma-separated arguments in parentheses:\n\nmax(5, 12, 9)\n\nCopy\n\nFor more details on syntax, see Function Calls in the Expressions section.\n\nThe Terraform language does not support user-defined functions, and so only the functions built in to the language are available for use. The documentation includes a page for all of the available built-in functions.\n\nYou can experiment with the behavior of Terraform's built-in functions from the Terraform expression console, by running the terraform console command:\n\n> max(5, 12, 9)\n\n12\n\nCopy\n\nThe examples in the documentation for each function use console output to illustrate the result of calling the function with different parameters.\n\nEdit this page on GitHub\nTheme\nDark\nLight\nSystem\nCertifications\nSystem Status\nCookie Manager\nTerms of Use\nSecurity\nPrivacy\nTrademark Policy\nTrade Controls\nGive Feedback\n(opens in new tab)\nWe use cookies & other similar technology to collect data to improve your experience on our site, as described in our Privacy Policy and Cookie Policy.\nManage PreferencesACCEPT"
  },
  {
    "title": "Terraform Settings - Configuration Language | Terraform | HashiCorp Developer",
    "url": "https://developer.hashicorp.com/terraform/language/settings",
    "html": "Skip to main content\nNews\nHashiCorp to join IBM to accelerate multi-cloud automation.\nRead the blog →\nDismiss alert\nTerraform\nInstall\nTutorials\nDocumentation\nRegistry\n(opens in new tab)\nTry Cloud\n(opens in new tab)\nSearch\nCommand or control key\nK key\nTerraform Home\nConfiguration Language\nConfiguration Language\nStyle Guide\nFiles and Directories\nSyntax\nResources\nData Sources\nProviders\nVariables and Outputs\nModules\nMoved block\nChecks\nImport\nExpressions\nFunctions\nTerraform Settings\nOverview\nHCP Terraform\nBackends\nState\nTests\nUpgrading to Terraform v1.7\nv1.x Compatibility Promises\nTerraform Internals\nResources\nTutorial Library\nCertifications\nCommunity Forum\n(opens in new tab)\nSupport\n(opens in new tab)\nGitHub\n(opens in new tab)\nTerraform Registry\n(opens in new tab)\nDeveloper\nTerraform\nConfiguration Language\nTerraform Settings\nv1.8.x (latest)\nTerraform Settings\n\nThe special terraform configuration block type is used to configure some behaviors of Terraform itself, such as requiring a minimum Terraform version to apply your configuration.\n\nTerraform Block Syntax\n\nTerraform settings are gathered together into terraform blocks:\n\nterraform {\n\n  # ...\n\n}\n\nCopy\n\nEach terraform block can contain a number of settings related to Terraform's behavior. Within a terraform block, only constant values can be used; arguments may not refer to named objects such as resources, input variables, etc, and may not use any of the Terraform language built-in functions.\n\nThe various options supported within a terraform block are described in the following sections.\n\nConfiguring HCP Terraform\n\nThe nested cloud block configures HCP Terraform for enabling its CLI-driven run workflow.\n\nRefer to HCP Terraform Configuration for a summary of the cloud block's syntax.\n\nRefer to Using HCP Terraform in the Terraform CLI documentation for complete details about how to initialize and configure the HCP Terraform CLI integration.\n\nConfiguring a Terraform Backend\n\nThe nested backend block configures which state backend Terraform should use.\n\nThe syntax and behavior of the backend block is described in Backend Configuration.\n\nSpecifying a Required Terraform Version\n\nHands-on: Try the Manage Terraform Versions or Manage Terraform Versions in HCP Terraform tutorials.\n\nThe required_version setting accepts a version constraint string, which specifies which versions of Terraform can be used with your configuration.\n\nIf the running version of Terraform doesn't match the constraints specified, Terraform will produce an error and exit without taking any further actions.\n\nWhen you use child modules, each module can specify its own version requirements. The requirements of all modules in the tree must be satisfied.\n\nUse Terraform version constraints in a collaborative environment to ensure that everyone is using a specific Terraform version, or using at least a minimum Terraform version that has behavior expected by the configuration.\n\nThe required_version setting applies only to the version of Terraform CLI. Terraform's resource types are implemented by provider plugins, whose release cycles are independent of Terraform CLI and of each other. Use the required_providers block to manage the expected versions for each provider you use.\n\nSpecifying Provider Requirements\n\nThe required_providers block specifies all of the providers required by the current module, mapping each local provider name to a source address and a version constraint.\n\nterraform {\n\n  required_providers {\n\n    aws = {\n\n      version = \">= 2.7.0\"\n\n      source = \"hashicorp/aws\"\n\n    }\n\n  }\n\n}\n\nCopy\n\nFor more information, see Provider Requirements.\n\nExperimental Language Features\n\nThe Terraform team will sometimes introduce new language features initially via an opt-in experiment, so that the community can try the new feature and give feedback on it prior to it becoming a backward-compatibility constraint.\n\nIn releases where experimental features are available, you can enable them on a per-module basis by setting the experiments argument inside a terraform block:\n\nterraform {\n\n  experiments = [example]\n\n}\n\nCopy\n\nThe above would opt in to an experiment named example, assuming such an experiment were available in the current Terraform version.\n\nExperiments are subject to arbitrary changes in later releases and, depending on the outcome of the experiment, may change drastically before final release or may not be released in stable form at all. Such breaking changes may appear even in minor and patch releases. We do not recommend using experimental features in Terraform modules intended for production use.\n\nIn order to make that explicit and to avoid module callers inadvertently depending on an experimental feature, any module with experiments enabled will generate a warning on every terraform plan or terraform apply. If you want to try experimental features in a shared module, we recommend enabling the experiment only in alpha or beta releases of the module.\n\nThe introduction and completion of experiments is reported in Terraform's changelog, so you can watch the release notes there to discover which experiment keywords, if any, are available in a particular Terraform release.\n\nPassing Metadata to Providers\n\nThe terraform block can have a nested provider_meta block for each provider a module is using, if the provider defines a schema for it. This allows the provider to receive module-specific information, and is primarily intended for modules distributed by the same vendor as the associated provider.\n\nFor more information, see Provider Metadata.\n\nEdit this page on GitHub\n\nOn this page:\n\nTerraform Settings\nTerraform Block Syntax\nConfiguring HCP Terraform\nConfiguring a Terraform Backend\nSpecifying a Required Terraform Version\nSpecifying Provider Requirements\nExperimental Language Features\nPassing Metadata to Providers\nTheme\nDark\nLight\nSystem\nCertifications\nSystem Status\nCookie Manager\nTerms of Use\nSecurity\nPrivacy\nTrademark Policy\nTrade Controls\nGive Feedback\n(opens in new tab)\nWe use cookies & other similar technology to collect data to improve your experience on our site, as described in our Privacy Policy and Cookie Policy.\nManage PreferencesACCEPT"
  },
  {
    "title": "Modules - Configuration Language | Terraform | HashiCorp Developer",
    "url": "https://developer.hashicorp.com/terraform/language/modules/syntax",
    "html": "Skip to main content\nNews\nHashiCorp to join IBM to accelerate multi-cloud automation.\nRead the blog →\nDismiss alert\nTerraform\nInstall\nTutorials\nDocumentation\nRegistry\n(opens in new tab)\nTry Cloud\n(opens in new tab)\nSearch\nCommand or control key\nK key\nTerraform Home\nConfiguration Language\nConfiguration Language\nStyle Guide\nFiles and Directories\nSyntax\nResources\nData Sources\nProviders\nVariables and Outputs\nModules\nOverview\nModule Blocks\nModule Sources\nMeta-Arguments\nModule Development\nMoved block\nChecks\nImport\nExpressions\nFunctions\nTerraform Settings\nState\nTests\nUpgrading to Terraform v1.7\nv1.x Compatibility Promises\nTerraform Internals\nResources\nTutorial Library\nCertifications\nCommunity Forum\n(opens in new tab)\nSupport\n(opens in new tab)\nGitHub\n(opens in new tab)\nTerraform Registry\n(opens in new tab)\nDeveloper\nTerraform\nConfiguration Language\nModules\nModule Blocks\nv1.8.x (latest)\nModule Blocks\n\nHands-on: Try the Reuse Configuration with Modules tutorials.\n\nA module is a container for multiple resources that are used together.\n\nEvery Terraform configuration has at least one module, known as its root module, which consists of the resources defined in the .tf files in the main working directory.\n\nA module can call other modules, which lets you include the child module's resources into the configuration in a concise way. Modules can also be called multiple times, either within the same configuration or in separate configurations, allowing resource configurations to be packaged and re-used.\n\nThis page describes how to call one module from another. For more information about creating re-usable child modules, see Module Development.\n\nCalling a Child Module\n\nTo call a module means to include the contents of that module into the configuration with specific values for its input variables. Modules are called from within other modules using module blocks:\n\nmodule \"servers\" {\n\n  source = \"./app-cluster\"\n\n\n\n  servers = 5\n\n}\n\nCopy\n\nA module that includes a module block like this is the calling module of the child module.\n\nThe label immediately after the module keyword is a local name, which the calling module can use to refer to this instance of the module.\n\nWithin the block body (between { and }) are the arguments for the module. Module calls use the following kinds of arguments:\n\nThe source argument is mandatory for all modules.\n\nThe version argument is recommended for modules from a registry.\n\nMost other arguments correspond to input variables defined by the module. (The servers argument in the example above is one of these.)\n\nTerraform defines a few other meta-arguments that can be used with all modules, including for_each and depends_on.\n\nSource\n\nAll modules require a source argument, which is a meta-argument defined by Terraform. Its value is either the path to a local directory containing the module's configuration files, or a remote module source that Terraform should download and use. This value must be a literal string with no template sequences; arbitrary expressions are not allowed. For more information on possible values for this argument, see Module Sources.\n\nThe same source address can be specified in multiple module blocks to create multiple copies of the resources defined within, possibly with different variable values.\n\nAfter adding, removing, or modifying module blocks, you must re-run terraform init to allow Terraform the opportunity to adjust the installed modules. By default this command will not upgrade an already-installed module; use the -upgrade option to instead upgrade to the newest available version.\n\nVersion\n\nWhen using modules installed from a module registry, we recommend explicitly constraining the acceptable version numbers to avoid unexpected or unwanted changes.\n\nUse the version argument in the module block to specify versions:\n\nmodule \"consul\" {\n\n  source  = \"hashicorp/consul/aws\"\n\n  version = \"0.0.5\"\n\n\n\n  servers = 3\n\n}\n\nCopy\n\nThe version argument accepts a version constraint string. Terraform will use the newest installed version of the module that meets the constraint; if no acceptable versions are installed, it will download the newest version that meets the constraint.\n\nVersion constraints are supported only for modules installed from a module registry, such as the public Terraform Registry or HCP Terraform's private module registry. Other module sources can provide their own versioning mechanisms within the source string itself, or might not support versions at all. In particular, modules sourced from local file paths do not support version; since they're loaded from the same source repository, they always share the same version as their caller.\n\nMeta-arguments\n\nAlong with source and version, Terraform defines a few more optional meta-arguments that have special meaning across all modules, described in more detail in the following pages:\n\ncount - Creates multiple instances of a module from a single module block. See the count page for details.\n\nfor_each - Creates multiple instances of a module from a single module block. See the for_each page for details.\n\nproviders - Passes provider configurations to a child module. See the providers page for details. If not specified, the child module inherits all of the default (un-aliased) provider configurations from the calling module.\n\ndepends_on - Creates explicit dependencies between the entire module and the listed targets. See the depends_on page for details.\n\nTerraform does not use the lifecycle argument. However, the lifecycle block is reserved for future versions.\n\nAccessing Module Output Values\n\nThe resources defined in a module are encapsulated, so the calling module cannot access their attributes directly. However, the child module can declare output values to selectively export certain values to be accessed by the calling module.\n\nFor example, if the ./app-cluster module referenced in the example above exported an output value named instance_ids then the calling module can reference that result using the expression module.servers.instance_ids:\n\nresource \"aws_elb\" \"example\" {\n\n  # ...\n\n\n\n  instances = module.servers.instance_ids\n\n}\n\nCopy\n\nFor more information about referring to named values, see Expressions.\n\nTransferring Resource State Into Modules\n\nMoving resource blocks from one module into several child modules causes Terraform to see the new location as an entirely different resource. As a result, Terraform plans to destroy all resource instances at the old address and create new instances at the new address.\n\nTo preserve existing objects, you can use refactoring blocks to record the old and new addresses for each resource instance. This directs Terraform to treat existing objects at the old addresses as if they had originally been created at the corresponding new addresses.\n\nReplacing resources within a module\n\nYou may have an object that needs to be replaced with a new object for a reason that isn't automatically visible to Terraform, such as if a particular virtual machine is running on degraded underlying hardware. In this case, you can use the -replace=... planning option to force Terraform to propose replacing that object.\n\nIf the object belongs to a resource within a nested module, specify the full path to that resource including all of the nested module steps leading to it. For example:\n\n$ terraform plan -replace=module.example.aws_instance.example\n\nCopy\n\nThe above selects a resource \"aws_instance\" \"example\" declared inside a module \"example\" child module declared inside your root module.\n\nBecause replacing is a very disruptive action, Terraform only allows selecting individual resource instances. There is no syntax to force replacing all resource instances belonging to a particular module.\n\nRemoving Modules\n\nNote: The removed block is available in Terraform v1.7 and later. For earlier Terraform versions, you can use the terraform state rm CLI command as a separate step.\n\nTo remove a module from Terraform, simply delete the module call from your Terraform configuration.\n\nBy default, after you remove the module block, Terraform will plan to destroy any resources it is managing that were declared in that module. This is because when you remove the module call, that module's configuration is no longer included in your Terraform configuration.\n\nSometimes you may wish to remove a module from your Terraform configuration without destroying the real infrastructure objects it manages. In this case, the resources will be removed from the Terraform state, but the real infrastructure objects will not be destroyed.\n\nTo declare that a module was removed from Terraform configuration but that its managed objects should not be destroyed, remove the module block from your configuration and replace it with a removed block:\n\nremoved {\n\n  from = module.example\n\n\n\n  lifecycle {\n\n    destroy = false\n\n  }\n\n}\n\nCopy\n\nThe from argument is the address of the module you want to remove, without any instance keys (such as \"module.example[1]\").\n\nThe lifecycle block is required. The destroy argument determines whether Terraform will attempt to destroy the objects managed by the module or not. A value of false means that Terraform will remove the resources from state without destroying them.\n\nEdit this page on GitHub\n\nOn this page:\n\nModule Blocks\nCalling a Child Module\nAccessing Module Output Values\nTransferring Resource State Into Modules\nReplacing resources within a module\nRemoving Modules\nTheme\nDark\nLight\nSystem\nCertifications\nSystem Status\nCookie Manager\nTerms of Use\nSecurity\nPrivacy\nTrademark Policy\nTrade Controls\nGive Feedback\n(opens in new tab)\nWe use cookies & other similar technology to collect data to improve your experience on our site, as described in our Privacy Policy and Cookie Policy.\nManage PreferencesACCEPT"
  },
  {
    "title": "Output Values - Configuration Language | Terraform | HashiCorp Developer",
    "url": "https://developer.hashicorp.com/terraform/language/values/outputs#declaring-an-output-value",
    "html": "Skip to main content\nNews\nHashiCorp to join IBM to accelerate multi-cloud automation.\nRead the blog →\nDismiss alert\nTerraform\nInstall\nTutorials\nDocumentation\nRegistry\n(opens in new tab)\nTry Cloud\n(opens in new tab)\nSearch\nCommand or control key\nK key\nTerraform Home\nConfiguration Language\nConfiguration Language\nStyle Guide\nFiles and Directories\nSyntax\nResources\nData Sources\nProviders\nVariables and Outputs\nOverview\nInput Variables\nOutput Values\nLocal Values\nModules\nMoved block\nChecks\nImport\nExpressions\nFunctions\nTerraform Settings\nState\nTests\nUpgrading to Terraform v1.7\nv1.x Compatibility Promises\nTerraform Internals\nResources\nTutorial Library\nCertifications\nCommunity Forum\n(opens in new tab)\nSupport\n(opens in new tab)\nGitHub\n(opens in new tab)\nTerraform Registry\n(opens in new tab)\nDeveloper\nTerraform\nConfiguration Language\nVariables and Outputs\nOutput Values\nv1.8.x (latest)\nOutput Values\n\nOutput values make information about your infrastructure available on the command line, and can expose information for other Terraform configurations to use. Output values are similar to return values in programming languages.\n\nHands-on: Try the Output Data From Terraform tutorial.\n\nOutput values have several uses:\n\nA child module can use outputs to expose a subset of its resource attributes to a parent module.\nA root module can use outputs to print certain values in the CLI output after running terraform apply.\nWhen using remote state, root module outputs can be accessed by other configurations via a terraform_remote_state data source.\n\nResource instances managed by Terraform each export attributes whose values can be used elsewhere in configuration. Output values are a way to expose some of that information to the user of your module.\n\nNote: For brevity, output values are often referred to as just \"outputs\" when the meaning is clear from context.\n\nDeclaring an Output Value\n\nEach output value exported by a module must be declared using an output block:\n\noutput \"instance_ip_addr\" {\n\n  value = aws_instance.server.private_ip\n\n}\n\nCopy\n\nThe label immediately after the output keyword is the name, which must be a valid identifier. In a root module, this name is displayed to the user; in a child module, it can be used to access the output's value.\n\nThe value argument takes an expression whose result is to be returned to the user. In this example, the expression refers to the private_ip attribute exposed by an aws_instance resource defined elsewhere in this module (not shown). Any valid expression is allowed as an output value.\n\nNote: Outputs are only rendered when Terraform applies your plan. Running terraform plan will not render outputs.\n\nAccessing Child Module Outputs\n\nIn a parent module, outputs of child modules are available in expressions as module.<MODULE NAME>.<OUTPUT NAME>. For example, if a child module named web_server declared an output named instance_ip_addr, you could access that value as module.web_server.instance_ip_addr.\n\nCustom Condition Checks\n\nYou can use precondition blocks to specify guarantees about output data. The following examples creates a precondition that checks whether the EC2 instance has an encrypted root volume.\n\noutput \"api_base_url\" {\n\n  value = \"https://${aws_instance.example.private_dns}:8433/\"\n\n\n\n  # The EC2 instance must have an encrypted root volume.\n\n  precondition {\n\n    condition     = data.aws_ebs_volume.example.encrypted\n\n    error_message = \"The server's root volume is not encrypted.\"\n\n  }\n\n}\n\nCopy\n\nCustom conditions can help capture assumptions, helping future maintainers understand the configuration design and intent. They also return useful information about errors earlier and in context, helping consumers more easily diagnose issues in their configurations.\n\nRefer to Custom Condition Checks for more details.\n\nOptional Arguments\n\noutput blocks can optionally include description, sensitive, and depends_on arguments, which are described in the following sections.\n\ndescription — Output Value Documentation\n\nBecause the output values of a module are part of its user interface, you can briefly describe the purpose of each value using the optional description argument:\n\noutput \"instance_ip_addr\" {\n\n  value       = aws_instance.server.private_ip\n\n  description = \"The private IP address of the main server instance.\"\n\n}\n\nCopy\n\nThe description should concisely explain the purpose of the output and what kind of value is expected. This description string might be included in documentation about the module, and so it should be written from the perspective of the user of the module rather than its maintainer. For commentary for module maintainers, use comments.\n\nsensitive — Suppressing Values in CLI Output\n\nAn output can be marked as containing sensitive material using the optional sensitive argument:\n\noutput \"db_password\" {\n\n  value       = aws_db_instance.db.password\n\n  description = \"The password for logging in to the database.\"\n\n  sensitive   = true\n\n}\n\nCopy\n\nTerraform will hide values marked as sensitive in the messages from terraform plan and terraform apply. In the following scenario, our root module has an output declared as sensitive and a module call with a sensitive output, which we then use in a resource attribute.\n\n# main.tf\n\n\n\nmodule \"foo\" {\n\n  source = \"./mod\"\n\n}\n\n\n\nresource \"test_instance\" \"x\" {\n\n  some_attribute = module.foo.a # resource attribute references a sensitive output\n\n}\n\n\n\noutput \"out\" {\n\n  value     = \"xyz\"\n\n  sensitive = true\n\n}\n\n\n\n# mod/main.tf, our module containing a sensitive output\n\n\n\noutput \"a\" {\n\n  value     = \"secret\"\n\n  sensitive = true\n\n}\n\nCopy\n\nWhen we run a plan or apply, the sensitive value is redacted from output:\n\nTerraform will perform the following actions:\n\n\n\n  # test_instance.x will be created\n\n  + resource \"test_instance\" \"x\" {\n\n      + some_attribute    = (sensitive value)\n\n    }\n\n\n\nPlan: 1 to add, 0 to change, 0 to destroy.\n\n\n\nChanges to Outputs:\n\n  + out = (sensitive value)\n\nCopy\n\nNote: In Terraform versions prior to Terraform 0.14, setting an output value in the root module as sensitive would prevent Terraform from showing its value in the list of outputs at the end of terraform apply. However, the value could still display in the CLI output for other reasons, like if the value is referenced in an expression for a resource argument.\n\nTerraform will still record sensitive values in the state, and so anyone who can access the state data will have access to the sensitive values in cleartext. For more information, see Sensitive Data in State.\n\ndepends_on — Explicit Output Dependencies\n\nSince output values are just a means for passing data out of a module, it is usually not necessary to worry about their relationships with other nodes in the dependency graph.\n\nHowever, when a parent module accesses an output value exported by one of its child modules, the dependencies of that output value allow Terraform to correctly determine the dependencies between resources defined in different modules.\n\nJust as with resource dependencies, Terraform analyzes the value expression for an output value and automatically determines a set of dependencies, but in less-common cases there are dependencies that cannot be recognized implicitly. In these rare cases, the depends_on argument can be used to create additional explicit dependencies:\n\noutput \"instance_ip_addr\" {\n\n  value       = aws_instance.server.private_ip\n\n  description = \"The private IP address of the main server instance.\"\n\n\n\n  depends_on = [\n\n    # Security group rule must be created before this IP address could\n\n    # actually be used, otherwise the services will be unreachable.\n\n    aws_security_group_rule.local_access,\n\n  ]\n\n}\n\nCopy\n\nThe depends_on argument should be used only as a last resort. When using it, always include a comment explaining why it is being used, to help future maintainers understand the purpose of the additional dependency.\n\nEdit this page on GitHub\n\nOn this page:\n\nOutput Values\nDeclaring an Output Value\nAccessing Child Module Outputs\nCustom Condition Checks\nOptional Arguments\nTheme\nDark\nLight\nSystem\nCertifications\nSystem Status\nCookie Manager\nTerms of Use\nSecurity\nPrivacy\nTrademark Policy\nTrade Controls\nGive Feedback\n(opens in new tab)\nWe use cookies & other similar technology to collect data to improve your experience on our site, as described in our Privacy Policy and Cookie Policy.\nManage PreferencesACCEPT"
  },
  {
    "title": "Terraform v1.x Compatibility Promises | Terraform | HashiCorp Developer",
    "url": "https://developer.hashicorp.com/terraform/language/v1.1.x/v1-compatibility-promises",
    "html": "Skip to main content\nNews\nHashiCorp to join IBM to accelerate multi-cloud automation.\nRead the blog →\nDismiss alert\nTerraform\nInstall\nTutorials\nDocumentation\nRegistry\n(opens in new tab)\nTry Cloud\n(opens in new tab)\nSearch\nCommand or control key\nK key\nTerraform Home\nConfiguration Language\nConfiguration Language\nTerraform Language\nOverview\nFiles and Directories\nSyntax\nResources\nData Sources\nProviders\nVariables and Outputs\nModules\nExpressions\nFunctions\nTerraform Settings\nState\nUpgrade Guides\nOverview\nUpgrading to Terraform v1.1\nUpgrading to Terraform v1.0\nv1.x Compatibility Promises\nUpgrading to Terraform v0.15\nUpgrading to Terraform v0.14\nUpgrading to Terraform v0.13\nUpgrading to Terraform v0.12\nUpgrading to Terraform v0.11\nUpgrading to Terraform v0.10\nUpgrading to Terraform v0.9\nUpgrading to Terraform v0.8\nUpgrading to Terraform v0.7\nHistorical docs: 0.11 and Older\nResources\nTutorial Library\nCertifications\nCommunity Forum\n(opens in new tab)\nSupport\n(opens in new tab)\nGitHub\n(opens in new tab)\nTerraform Registry\n(opens in new tab)\n\nYou are viewing documentation for version v1.1 and earlier. View latest version.\n\nDeveloper\nTerraform\nConfiguration Language v1.1.x\nv1-compatibility-promises\nv1.1 and earlier\nTerraform v1.x Compatibility Promises\n\nThe release of Terraform v1.0 represents an important milestone in the development of the Terraform language and workflow. Terraform v1.0 is a stable platform for describing and managing infrastructure.\n\nIn this release we're defining a number of Terraform behaviors that we intend to remain compatible with throughout the 1.x releases:\n\nA large subset of Terraform language features.\nA more conservative subset of the Terraform CLI workflow commands.\nThe wire protocol for communication between Terraform Core and Terraform providers.\nThe wire protocols for installation of Terraform providers and external Terraform modules.\n\nOur intention is that Terraform modules written for Terraform v1.0 will continue to plan and apply successfully, without required changes, throughout the v1.x releases.\n\nWe also intend that automation built around the workflow subset described in this document will work without changes in all future v1.x releases.\n\nFinally, we intend that providers built against the currently-documented provider wire protocol will be compatible with all future Terraform v1.x releases targeting the same operating system and architecture, without the need for source code modification or binary recompilation.\n\nIn short, we aim to make upgrades between v1.x releases straightforward, requiring no changes to your configuration, no extra commands to run upgrade steps, and no changes to any automation you've set up around Terraform.\n\nThe Terraform v1.x series will be actively maintained for at least 18 months after v1.0.\n\nThe following sections include some specific guidance on what we will promise throughout the v1.x series, for those who would like full details. At a higher level though, we don't intend to make any changes that would cause existing modules or automation to require changes when upgrading to a new v1.x release. We will generally treat compatibility problems in new Terraform CLI releases as bugs to be fixed unless there was a very significant justification for the change, such as in addressing a critical security problem or matching with a breaking change to a remote dependency that isn't directly under our control.\n\nThe Terraform Language\n\nThe main Terraform Language includes the language syntax, the top-level structures such as resource, module, and provider blocks, the \"meta-arguments\" in those blocks, and the documented semantics and behaviors for the operators and built-in functions available for use in expressions.\n\nThere is not a single formal specification for the Terraform language, but the Configuration section of the documentation on the Terraform website serves as a description of the language features and their intended behaviors.\n\nThe following top-level blocks and their defined \"meta-arguments\" (that is, arguments defined by Terraform Core rather than by external plugins such as providers) will retain their current functionality:\n\nresource and data blocks to declare resources, including their nested block types lifecycle, connection, and provisioner, and their meta-argument provider.\nmodule blocks to call other modules, and its meta-argument providers.\nThe count, for_each, and depends_on meta-arguments in resource, data, and module blocks.\nprovider blocks to configure providers, and the alias meta-argument.\nvariable, output, and locals blocks for declaring the various kinds of named values in a module.\nterraform blocks, including the nested required_version and required_providers arguments, and nested backend blocks for backend configuration.\n\nWe also intend to keep compatibility with all expression operators and built-in functions, with the exception of references to terraform.workspace, whose behavior may change as part of future changes to the workspace model.\n\nWe intend to retain broad compatibility with Terraform language features, with a few specific caveats:\n\nWe consider a configuration to be valid if Terraform can create and apply a plan for it without reporting any errors.\n\nA configuration that currently produces errors might generate different errors or exhibit other non-error behaviors in a future version of Terraform. A configuration that generates errors during the apply phase might generate similar errors at an earlier phase in future, because we generally consider it better to detect errors in as early a phase as possible.\n\nGenerally-speaking, the compatibility promises described in this document apply only to valid configurations. Handling of invalid configurations is always subject to change in future Terraform releases.\n\nIf the actual behavior of a feature differs from what we explicitly documented as the feature's behavior, we will usually treat that as a bug and change the feature to match the documentation, although we will avoid making such changes if they seem likely to cause broad compatibility problems. We cannot promise to always remain \"bug-compatible\" with previous releases, but we will consider such fixes carefully to minimize their impact.\n\nAny experimental features may change or may be removed entirely from future releases. Terraform always produces a warning when an experimental language feature is active, to make you aware of that risk. We don't recommend using experimental features in production modules.\n\nWe will introduce new language features, and if you start using them then your configuration won't work with prior Terraform versions that didn't support those features yet.\n\nTerraform Providers are separate plugins which can change independently of Terraform Core and are therefore not subject to these compatibility promises. If you upgrade any of the providers you are using then you might need to change provider or resource configurations related to those providers.\n\nA small number of features remain deprecated with explicit warnings in Terraform v1.0. Those deprecation cycles will end in a future v1.x release, at which point we will remove the corresponding features.\n\nWorkflow\n\nThere is a set of often used Terraform workflows, which we are calling protected workflows. We will not remove these commands, subcommands, and flags or make backward-incompatible changes to protected workflow functionality. If we accidentally change these, we will consider backwards-incompatible changes to core workflows as bugs to be fixed. For a list of the command and option combinations that are part of protected workflows, see Protected Workflow Commands. There is another set of commands that we are explicitly not making compatibility promises about, because we expect their functionality to change in v1.x releases: see Commands That Might Change.\n\nThe supported ways for external software to interact with Terraform are via the JSON output modes offered by some commands and via exit status codes. We may extend certain JSON formats with new object properties but we will not remove or make breaking changes to the definitions of existing properties.\n\nNatural language command output or log output is not a stable interface and may change in any new version. If you write software that parses this output then it may need to be updated when you upgrade Terraform. If you need access to data that is not currently available via one of the machine-readable JSON interfaces, we suggest opening a feature request to discuss your use-case.\n\nUpgrading and Downgrading\n\nThroughout the v1.x series of releases, we intend that you should be able to switch to a newer Terraform version and use it just as before, without any special upgrade steps.\n\nYou should be able to upgrade from any v1.x release to any later v1.x release. You might also be able to downgrade to an earlier v1.x release, but that isn't guaranteed: later releases may introduce new features that earlier versions cannot understand, including new storage formats for Terraform state snapshots.\n\nIf you make use of features introduced in a later v1.x release, your configuration won't be compatible with releases that predate that feature. For example, if a language feature is added in v1.3 and you start using it, your Terraform configuration will no longer be compatible with Terraform v1.2.\n\nProviders\n\nTerraform providers are separate plugins which communicate with Terraform using a documented protocol. Therefore these compatibility promises can only cover the \"client\" side of this protocol as implemented by Terraform Core; the behaviors of individual providers, including which resource types they support and which arguments they expect, are decided by the provider development teams and can change independently of Terraform Core releases.\n\nIf you upgrade to a new version of a provider then you might need to change the parts of your configuration which are interpreted by that provider, even if you are still using a Terraform v1.x release.\n\nProvider Installation Methods\n\nTerraform normally installs providers from a provider registry implementing the Provider Registry Protocol, version 1. All Terraform v1.x releases will remain compatible with that protocol, and so correctly-implemented provider registries will stay compatible.\n\nTerraform also supports installation of providers from local filesystem directories (filesystem mirrors) and from network mirrors (implementing the Provider Mirror Protocol. All Terraform v1.x releases will remain compatible with those installation methods, including the Implied Local Mirror Directories.\n\nSpecific provider registries or network mirrors are run independently from Terraform itself and so their own behaviors are not subject to these compatibility promises.\n\nProvider Protocol Versions\n\nThe current major version of the provider plugin protocol as of Terraform v1.0 is version 5, which is defined by a combination of a Protocol Buffers schema describing the physical wire formats and by additional prose documentation describing the expected provider behaviors.\n\nWe will support protocol version 5 throughout the Terraform v1.x releases. If we make new minor revisions to protocol version 5 in later releases then we will design them such that existing plugins will continue to work, as long as they correctly implemented the protocol.\n\nWe may introduce new major versions of the protocol during the v1.x series. If so, we will continue to support protocol version 5 alongside those new versions. Individual provider teams might decide to remove support for protocol version 5 in later releases, in which case those new provider releases will not be compatible with all of the Terraform v1.x releases.\n\nExternal Modules\n\nTerraform modules are reusable infrastructure components written in the Terraform language. Some modules are \"external\" in the sense that Terraform automatically installs them from a location other than the current configuration directory, in which case their contents could change independently of changes to your local modules, of the providers you use, and of Terraform itself.\n\nModule Installation Methods\n\nTerraform supports installing child modules from a number of different module source types. We will continue to support all of the existing source types throughout the v1.x releases.\n\nOne of the supported source types is a module registry implementing the Module Registry Protocol version 1. All Terraform v1.x releases will remain compatible with correct implementations of that protocol.\n\nSome module source types work directly with services or protocols defined and run by third parties. Although we will not remove Terraform's own client-side support for those, we cannot guarantee that their owners will keep those services running or that they will remain compatible with Terraform's client implementations.\n\nExternal Module Compatibility\n\nIf your configuration depends on external modules, newer versions of those modules may include breaking changes. External modules are not part of Terraform and are therefore not subject to these compatibility promises.\n\nProvisioners\n\nWe will maintain compatibility for the file, local-exec, and remote-exec provisioner types through all v1.x releases.\n\nSome additional vendor-specific provisioners were available in earlier Terraform versions but were deprecated in Terraform v0.13 and removed in Terraform v0.15.\n\nTerraform supports loading additional provisioners as plugins from certain local filesystem directories. We'll continue to support that throughout the Terraform v1.x releases, but since such plugins are separate from Terraform Core itself their own behaviors cannot be subject to these compatibility promises. However, we will continue to support the plugin wire protocol as defined in Terraform v1.0 throughout the v1.x releases, and so correctly-implemented provisioner plugins should remain compatible with future Terraform releases.\n\nState Storage Backends\n\nWhen you use remote state, Terraform interacts with remote services over the network in order to store and manage locks for Terraform state.\n\nFor historical reasons, all supported state storage backends are included as part of Terraform CLI but not all are supported directly by the Terraform Team. Only the following backends maintained by the Terraform team are subject to compatibility promises:\n\nlocal (the default, when you are not using remote state)\nhttp\n\nThe other state storage backends are maintained by external teams via contributions to the Terraform CLI codebase, and so their expected configuration arguments or behaviors might change even in v1.x releases, although we will aim to still ensure a good migration path in such cases, where possible.\n\nWe are considering allowing external state storage backend implementations via plugins, similar to provider plugins. If we introduce such a mechanism during the v1.x releases then you may need to make configuration changes in order to use those plugins, and state storage backends other than those listed above may be removed from later versions of Terraform CLI once equivalent plugins are available.\n\nThe remote Backend and Terraform Cloud\n\nThe remote backend is maintained by the Terraform Cloud team and so its behavior may change along with ongoing changes to Terraform Cloud.\n\nThere will be a supported mechanism to use Terraform CLI with Terraform Cloud throughout the v1.x releases, but the exact details may change. Terraform Cloud evolves independently of Terraform CLI and is therefore not subject to these compatibility promises.\n\nCommunity-maintained State Storage Backends\n\nThe azurerm, consul, s3, and kubernetes backends are maintained by other teams at HashiCorp. Those teams intend to continue basic maintenence at the level of bug fixes through the v1.x releases, unless we implement a plugin protocol for backends at which point development of these backends is likely to continue in the external plugins only, which may require configuration changes to switch to the plugin equivalents.\n\nThe cos, oss, pg, gcs, and etcdv3 backends are maintained by outside contributors and are not subject to these compatibility promises.\n\nUnmaintained State Storage Backends\n\nThe artifactory, etcdv2, manta, and swift state storage backends do not currently have any maintainers and thus remain in Terraform CLI releases on a best-effort basis. They may be removed in later v1.x releases, and will not be updated in case of any breaking changes to the services they integrate with.\n\nSupported Platforms\n\nThroughout the v1.x series we will continue to produce official releases for the following platforms, and make changes as necessary to support new releases of these operating systems:\n\nmacOS on x64 CPUs (darwin_amd64)\nWindows on x64 CPUs (windows_amd64)\nLinux on x64, 32-bit ARMv6, and 64-bit ARMv8 (linux_amd64, linux_arm, and linux_arm64 respectively)\n\nOver time we may require newer versions of these operating systems. For example, subsequent Terraform releases in the v1.x series might end support for earlier versions of macOS or Windows, or earlier Linux kernel releases.\n\nWe have historically produced official releases for a number of other platforms as a convenience to users of those platforms, and we have no current plans to stop publishing them but we cannot promise ongoing releases or bug fixes for the other platforms throughout the v1.x series. We do not routinely test Terraform on any platforms other than those listed above.\n\nWe might add support for new platforms in later v1.x releases. If so, earlier Terraform releases prior to that support will not be available on those platforms.\n\nAll Terraform plugins, including provider plugins, are separate programs that have their own policies for which platforms they support. We cannot guarantee that all providers currently support or will continue to support the platforms listed above, even though Terraform CLI itself will support them.\n\nLater Revisions to These Promises\n\nWe may extend or refine these promises throughout the v1.x series in order to describe promises related to new features or to clarify existing promises if we find via feedback that our earlier statements had been unclear.\n\nPromises for new features will be additive in the sense that they will add further promises without retracting any existing ones. For promises that only apply to later v1.x releases we will mention the earliest version(s) those promises apply to.\n\nEven if we don't add an explicit statement to this document, we intend that any non-experimental features added in later v1.x releases will remain compatible at least through the remainder of the v1.x series, unless otherwise stated.\n\nAppendices\nProtected Workflow Commands\n\nThe following is the list of Terraform CLI subcommands and options that are subject to these compatibility promises. If you build automation around these commands then it should be compatible with all later v1.x releases.\n\nAs noted above, compatibility with external software is limited to explicitly-machine-readable output (-json and -raw modes) and exit codes. Any natural-language output from these commands might change in later releases.\n\ninit\n-backend=false\n-backend-config=FILE\n-backend-config=\"KEY=VALUE\"\n-force-copy\n-get=false\n-input=false\n-migrate-state\n-no-color\n-plugin-dir=DIR\n-reconfigure\n-upgrade\nvalidate\n-json\n-no-color\nplan\n-compact-warnings\n-destroy\n-detailed-exitcode\n-lock=false\n-lock-timeout=DURATION\n-input=false\n-json\n-no-color\n-out=FILE\n-parallelism=N\n-refresh=false\n-refresh-only\n-replace=ADDRESS\n-target=ADDRESS\n-var 'NAME=VALUE'\n-var-file=FILE\napply\n-auto-approve\n-compact-warnings\n-lock=false\n-lock-timeout=DURATION\n-input=false\n-json\n-no-color\n-parallelism=N\n-refresh=false\n-refresh-only\n-replace=ADDRESS\n-target=ADDRESS\n-var 'NAME=VALUE'\n-var-file=FILE\nshow\n-no-color\n-json\n(both with and without a plan file)\nproviders (with no subcommand)\nproviders lock\n-fs-mirror=PATH\n-net-mirror=URL\n-platform=OS_ARCH\nproviders mirror\n-platform=OS_ARCH\nproviders schema\n-json\nfmt\n-list=false\n-write=false\n-diff\n-recursive\n-check\nversion\n-json\noutput\n-no-color\n-json\n-raw\ntaint\n-allow-missing\n-lock=false\n-lock-timeout=DURATION\n-ignore-remote-version\nuntaint\n-allow-missing\n-lock=false\n-lock-timeout=DURATION\n-ignore-remote-version\nforce-unlock\n-force\nstate list\n-id=ID\nstate pull\nstate push\n-force\n-lock=false\n-lock-timeout=DURATION\nstate show\n-ignore-remote-version\nlogin\n\nFor commands or options not in the above list, we will still avoid breaking changes where possible, but can't promise full compatibility throughout the v1.x series. If you are building automation around Terraform, use only the commands above to avoid the need for changes when upgrading.\n\nPlease note that although Terraform's internal logs (via the TF_LOG environment variable) are available in a JSON format, the particular syntax or structure of those log lines is not a supported integration interface. The logs are available as JSON only to help with ad-hoc filtering and processing of logs by Terraform developers.\n\nCommands That Might Change\n\nAll of the following commands and their subcommands/options are not subject to compatibility promises, either because we have existing plans to improve them during the v1.x series or because we are aware of shortcomings in their design that might require breaking changes for ongoing maintenence.\n\nWhile you can freely use these commands when running Terraform interactively as long as they remain supported, we don't recommend using them as part of any automation unless you are willing to potentially update that automation when upgrading to a later v1.x release.\n\ndestroy (consider terraform apply -destroy instead)\nconsole\nget (consider terraform init instead)\ngraph\nimport\npush\nrefresh (consider terraform apply -refresh-only instead)\nstate mv\nstate replace-provider\nstate rm\nall subcommands of workspace (and its deprecated alias env)\n\nWhile we do intend to retain support for the main use-cases associated with these commands in future releases, we cannot promise to retain the exact command names or options used to meet those use-cases.\n\nHow We Will Keep These Promises\nAutomated Regression Testing\n\nThe Terraform codebase includes various unit and integration tests intended to help us to notice accidental behavior regressions before they ship in a stable version.\n\nHowever, Terraform is a relatively complex system with many different features that can interact in interesting ways. In the past we've seen reports of behavior differences that appeared only when combining two or more features in a way we hadn't previously anticipated or written automated tests for.\n\nIn each case we have both implemented a change to resolve the compatibility problem and added one or more integration tests representing the behavior of that combination of features. We intend to continue this approach, so we can improve Terraform's test coverage over time.\n\nPrerelease Versions\n\nWe intend that most accidental changes in behavior covered by these promises will be caught by existing tests. However, we also accept that our test suite can never have perfect coverage of all possible feature interactions or other edge cases, and so we aim for each significant change to be included in both alpha and beta releases before eventual inclusion in a final release.\n\nFor minor releases we will typically also issue at least one release candidate prior to the final release. A release candidate represents that planned development is concluded and that we've fixed any regressions reported based on the alpha and beta releases, and thus the final release that follows should typically match exactly or almost exactly its most recent release candidate.\n\nRegressions in Final Releases\n\nFor more obscure combinations of features it is possible that a regression could be undetected during prerelease the prerelease periods and thus included in a final release.\n\nIf someone finds and reports such a regression soon after its release then we will treat it as a bug and fix it to restore the previous behavior in future releases, unless there is a very significant justification such as a security advisory. In these cases, we'll typically recommend anyone affected by the regression remain on the previous version until the problem is fixed and then skip forward directly to the new release containing that fix.\n\nYou can minimize the risk of being affected by missed regressions in final releases by proactively testing modules against alpha, beta, and release candidate packages. We recommend doing so only in isolated development or staging environments rather than against your production infrastructure. If you find a change in behavior in a prerelease build that seems contrary to the promises in this document, please open an issue in Terraform's GitHub repository to discuss it.\n\nLate-reported Regressions\n\nIn the most extreme case, there may be a regression with a combination of features that is so rare that it remains undetected for a long time.\n\nAfter a change has been included in more releases it becomes increasingly likely that other users will have depended on the newer behavior and thus we will need to make a tradeoff to decide whether restoring the behavior would have a greater negative impact than retaining the new behavior. We will always make this decision with due consideration to the implications of each unique situation.\n\nYou can minimize the risk of your modules being affected by late-reported regressions by upgrading promptly to new minor and patch releases of Terraform and reporting any compatibility problems you encounter in Terraform's GitHub repository.\n\nPragmatic Exceptions\n\nWe are making the promises above in good faith, with the intent that your investment in writing Terraform modules or automation will not be invalidated by future changes to Terraform. However, broad promises like the above can't possibly cover all nuances of practical problems that might arise as we continue to develop Terraform.\n\nFor that reason, there are some situations where we may still need to make changes that may impact existing modules or automation:\n\nSecurity problems: We may become aware of a design problem that has an important security impact. Depending on our determination of the associated risk, we may choose to break compatibility to achieve a more secure system.\nExternal Dependencies: Terraform's behavior depends on interfaces provided by external codebases, including your chosen operating system and including some remote network services for situations such as module and provider installation. These external systems can change outside of our control, including potentially removing or changing features that Terraform's own features depend on. In that case, if there is no suitable replacement mechanism then we may need to change Terraform's design to work within the new constraints.\nOpt-in Compatibility Breaks: The design of a language new feature may require changing the behavior or configuration representation of an existing feature. If so, we will typically make the new feature opt-in only in order to avoid breaking existing modules, but if you change your module to opt in to the new feature then you may also then be required to change other parts of your configuration to work with the new language design.\nBugs in New Features: If we introduce a new feature to Terraform and the initial implementation has problems that cause it to not match the documented design intent at release, we may make a follow-up release that corrects the implementation to match the documented design, even if that represents a minor compatibility regression compared to the initial implementation. However, once a feature is well-established and in common use we will usually defer to the implemented behavior and instead change the documentation to reflect it.\nRegressions in Existing Features: If we learn that a new Terraform release includes a regression for an existing feature that wasn't detected during the development and prerelease periods, and that learning comes promptly after the new release, we will typically restore the previous behavior at the expense of technically therefore breaking compatibility with the behavior of the new release, under the assumption that more users will have systems affected by the regression than will have systems depending on the newly-introduced behavior.\nLate-reported regressions: As described in the previous section, if we learn that there was an unintentional regression of a rarely-used feature or combination of features in a much earlier release then restoring the previous behavior may appear as a regression to later adopters. If we believe that fixing the regression would affect more users than the regression itself affects then we may choose to accept the regression as the new promised behavior.\nSituations we cannot anticipate: Although we've made an effort to consider various specific exceptional situations here, Terraform and its development process are not isolated from broader context, and so we must consider that there may be situations that we cannot possibly anticipate that would affect the future of Terraform. In those situations, we will always do our best to find a course of action that will minimize as much as possible the impact to existing modules and automation.\n\nOur intent with these pragmatic exceptions is only to acknowledge that there will always be situations that general compatibility promises cannot address. We will use these exceptions only with due consideration and as a last resort.\n\nOn this page:\n\nTerraform v1.x Compatibility Promises\nThe Terraform Language\nWorkflow\nUpgrading and Downgrading\nProviders\nExternal Modules\nProvisioners\nState Storage Backends\nCommunity-maintained State Storage Backends\nSupported Platforms\nLater Revisions to These Promises\nAppendices\nHow We Will Keep These Promises\nTheme\nDark\nLight\nSystem\nCertifications\nSystem Status\nCookie Manager\nTerms of Use\nSecurity\nPrivacy\nTrademark Policy\nTrade Controls\nGive Feedback\n(opens in new tab)\nWe use cookies & other similar technology to collect data to improve your experience on our site, as described in our Privacy Policy and Cookie Policy.\nManage PreferencesACCEPT"
  },
  {
    "title": "Terraform v1.x Compatibility Promises | Terraform | HashiCorp Developer",
    "url": "https://developer.hashicorp.com/terraform/language/v1.2.x/v1-compatibility-promises",
    "html": "Skip to main content\nNews\nHashiCorp to join IBM to accelerate multi-cloud automation.\nRead the blog →\nDismiss alert\nTerraform\nInstall\nTutorials\nDocumentation\nRegistry\n(opens in new tab)\nTry Cloud\n(opens in new tab)\nSearch\nCommand or control key\nK key\nTerraform Home\nConfiguration Language\nConfiguration Language\nTerraform Language\nOverview\nStyle Guide\nFiles and Directories\nSyntax\nResources\nData Sources\nProviders\nVariables and Outputs\nModules\nExpressions\nFunctions\nTerraform Settings\nState\nUpgrading to Terraform v1.2\nv1.x Compatibility Promises\nTerraform Internals\nResources\nTutorial Library\nCertifications\nCommunity Forum\n(opens in new tab)\nSupport\n(opens in new tab)\nGitHub\n(opens in new tab)\nTerraform Registry\n(opens in new tab)\n\nYou are viewing documentation for version v1.2.x. View latest version.\n\nDeveloper\nTerraform\nConfiguration Language v1.2.x\nv1.x Compatibility Promises\nv1.2.x\nTerraform v1.x Compatibility Promises\n\nThe release of Terraform v1.0 represents an important milestone in the development of the Terraform language and workflow. Terraform v1.0 is a stable platform for describing and managing infrastructure.\n\nIn this release we're defining a number of Terraform behaviors that we intend to remain compatible with throughout the 1.x releases:\n\nA large subset of Terraform language features.\nA more conservative subset of the Terraform CLI workflow commands.\nThe wire protocol for communication between Terraform Core and Terraform providers.\nThe wire protocols for installation of Terraform providers and external Terraform modules.\n\nOur intention is that Terraform modules written for Terraform v1.0 will continue to plan and apply successfully, without required changes, throughout the v1.x releases.\n\nWe also intend that automation built around the workflow subset described in this document will work without changes in all future v1.x releases.\n\nFinally, we intend that providers built against the currently-documented provider wire protocol will be compatible with all future Terraform v1.x releases targeting the same operating system and architecture, without the need for source code modification or binary recompilation.\n\nIn short, we aim to make upgrades between v1.x releases straightforward, requiring no changes to your configuration, no extra commands to run upgrade steps, and no changes to any automation you've set up around Terraform.\n\nThe Terraform v1.x series will be actively maintained for at least 18 months after v1.0.\n\nThe following sections include some specific guidance on what we will promise throughout the v1.x series, for those who would like full details. At a higher level though, we don't intend to make any changes that would cause existing modules or automation to require changes when upgrading to a new v1.x release. We will generally treat compatibility problems in new Terraform CLI releases as bugs to be fixed unless there was a very significant justification for the change, such as in addressing a critical security problem or matching with a breaking change to a remote dependency that isn't directly under our control.\n\nThe Terraform Language\n\nThe main Terraform Language includes the language syntax, the top-level structures such as resource, module, and provider blocks, the \"meta-arguments\" in those blocks, and the documented semantics and behaviors for the operators and built-in functions available for use in expressions.\n\nThere is not a single formal specification for the Terraform language, but the Configuration section of the documentation on the Terraform website serves as a description of the language features and their intended behaviors.\n\nThe following top-level blocks and their defined \"meta-arguments\" (that is, arguments defined by Terraform Core rather than by external plugins such as providers) will retain their current functionality:\n\nresource and data blocks to declare resources, including their nested block types lifecycle, connection, and provisioner, and their meta-argument provider.\nmodule blocks to call other modules, and its meta-argument providers.\nThe count, for_each, and depends_on meta-arguments in resource, data, and module blocks.\nprovider blocks to configure providers, and the alias meta-argument.\nvariable, output, and locals blocks for declaring the various kinds of named values in a module.\nterraform blocks, including the nested required_version and required_providers arguments, and nested backend blocks for backend configuration.\n\nWe also intend to keep compatibility with all expression operators and built-in functions, with the exception of references to terraform.workspace, whose behavior may change as part of future changes to the workspace model.\n\nWe intend to retain broad compatibility with Terraform language features, with a few specific caveats:\n\nWe consider a configuration to be valid if Terraform can create and apply a plan for it without reporting any errors.\n\nA configuration that currently produces errors might generate different errors or exhibit other non-error behaviors in a future version of Terraform. A configuration that generates errors during the apply phase might generate similar errors at an earlier phase in future, because we generally consider it better to detect errors in as early a phase as possible.\n\nGenerally-speaking, the compatibility promises described in this document apply only to valid configurations. Handling of invalid configurations is always subject to change in future Terraform releases.\n\nIf the actual behavior of a feature differs from what we explicitly documented as the feature's behavior, we will usually treat that as a bug and change the feature to match the documentation, although we will avoid making such changes if they seem likely to cause broad compatibility problems. We cannot promise to always remain \"bug-compatible\" with previous releases, but we will consider such fixes carefully to minimize their impact.\n\nAny experimental features may change or may be removed entirely from future releases. Terraform always produces a warning when an experimental language feature is active, to make you aware of that risk. We don't recommend using experimental features in production modules.\n\nWe will introduce new language features, and if you start using them then your configuration won't work with prior Terraform versions that didn't support those features yet.\n\nTerraform Providers are separate plugins which can change independently of Terraform Core and are therefore not subject to these compatibility promises. If you upgrade any of the providers you are using then you might need to change provider or resource configurations related to those providers.\n\nA small number of features remain deprecated with explicit warnings in Terraform v1.0. Those deprecation cycles will end in a future v1.x release, at which point we will remove the corresponding features.\n\nWorkflow\n\nThere is a set of often used Terraform workflows, which we are calling protected workflows. We will not remove these commands, subcommands, and flags or make backward-incompatible changes to protected workflow functionality. If we accidentally change these, we will consider backwards-incompatible changes to core workflows as bugs to be fixed. For a list of the command and option combinations that are part of protected workflows, see Protected Workflow Commands. There is another set of commands that we are explicitly not making compatibility promises about, because we expect their functionality to change in v1.x releases: see Commands That Might Change.\n\nThe supported ways for external software to interact with Terraform are via the JSON output modes offered by some commands and via exit status codes. We may extend certain JSON formats with new object properties but we will not remove or make breaking changes to the definitions of existing properties.\n\nNatural language command output or log output is not a stable interface and may change in any new version. If you write software that parses this output then it may need to be updated when you upgrade Terraform. If you need access to data that is not currently available via one of the machine-readable JSON interfaces, we suggest opening a feature request to discuss your use-case.\n\nUpgrading and Downgrading\n\nThroughout the v1.x series of releases, we intend that you should be able to switch to a newer Terraform version and use it just as before, without any special upgrade steps.\n\nYou should be able to upgrade from any v1.x release to any later v1.x release. You might also be able to downgrade to an earlier v1.x release, but that isn't guaranteed: later releases may introduce new features that earlier versions cannot understand, including new storage formats for Terraform state snapshots.\n\nIf you make use of features introduced in a later v1.x release, your configuration won't be compatible with releases that predate that feature. For example, if a language feature is added in v1.3 and you start using it, your Terraform configuration will no longer be compatible with Terraform v1.2.\n\nProviders\n\nTerraform providers are separate plugins which communicate with Terraform using a documented protocol. Therefore these compatibility promises can only cover the \"client\" side of this protocol as implemented by Terraform Core; the behaviors of individual providers, including which resource types they support and which arguments they expect, are decided by the provider development teams and can change independently of Terraform Core releases.\n\nIf you upgrade to a new version of a provider then you might need to change the parts of your configuration which are interpreted by that provider, even if you are still using a Terraform v1.x release.\n\nProvider Installation Methods\n\nTerraform normally installs providers from a provider registry implementing the Provider Registry Protocol, version 1. All Terraform v1.x releases will remain compatible with that protocol, and so correctly-implemented provider registries will stay compatible.\n\nTerraform also supports installation of providers from local filesystem directories (filesystem mirrors) and from network mirrors (implementing the Provider Mirror Protocol. All Terraform v1.x releases will remain compatible with those installation methods, including the Implied Local Mirror Directories.\n\nSpecific provider registries or network mirrors are run independently from Terraform itself and so their own behaviors are not subject to these compatibility promises.\n\nProvider Protocol Versions\n\nThe current major version of the provider plugin protocol as of Terraform v1.0 is version 5, which is defined by a combination of a Protocol Buffers schema describing the physical wire formats and by additional prose documentation describing the expected provider behaviors.\n\nWe will support protocol version 5 throughout the Terraform v1.x releases. If we make new minor revisions to protocol version 5 in later releases then we will design them such that existing plugins will continue to work, as long as they correctly implemented the protocol.\n\nWe may introduce new major versions of the protocol during the v1.x series. If so, we will continue to support protocol version 5 alongside those new versions. Individual provider teams might decide to remove support for protocol version 5 in later releases, in which case those new provider releases will not be compatible with all of the Terraform v1.x releases.\n\nExternal Modules\n\nTerraform modules are reusable infrastructure components written in the Terraform language. Some modules are \"external\" in the sense that Terraform automatically installs them from a location other than the current configuration directory, in which case their contents could change independently of changes to your local modules, of the providers you use, and of Terraform itself.\n\nModule Installation Methods\n\nTerraform supports installing child modules from a number of different module source types. We will continue to support all of the existing source types throughout the v1.x releases.\n\nOne of the supported source types is a module registry implementing the Module Registry Protocol version 1. All Terraform v1.x releases will remain compatible with correct implementations of that protocol.\n\nSome module source types work directly with services or protocols defined and run by third parties. Although we will not remove Terraform's own client-side support for those, we cannot guarantee that their owners will keep those services running or that they will remain compatible with Terraform's client implementations.\n\nExternal Module Compatibility\n\nIf your configuration depends on external modules, newer versions of those modules may include breaking changes. External modules are not part of Terraform and are therefore not subject to these compatibility promises.\n\nProvisioners\n\nWe will maintain compatibility for the file, local-exec, and remote-exec provisioner types through all v1.x releases.\n\nSome additional vendor-specific provisioners were available in earlier Terraform versions but were deprecated in Terraform v0.13 and removed in Terraform v0.15.\n\nTerraform supports loading additional provisioners as plugins from certain local filesystem directories. We'll continue to support that throughout the Terraform v1.x releases, but since such plugins are separate from Terraform Core itself their own behaviors cannot be subject to these compatibility promises. However, we will continue to support the plugin wire protocol as defined in Terraform v1.0 throughout the v1.x releases, and so correctly-implemented provisioner plugins should remain compatible with future Terraform releases.\n\nState Storage Backends\n\nWhen you use remote state, Terraform interacts with remote services over the network in order to store and manage locks for Terraform state.\n\nFor historical reasons, all supported state storage backends are included as part of Terraform CLI but not all are supported directly by the Terraform Team. Only the following backends maintained by the Terraform team are subject to compatibility promises:\n\nlocal (the default, when you are not using remote state)\nhttp\n\nThe other state storage backends are maintained by external teams via contributions to the Terraform CLI codebase, and so their expected configuration arguments or behaviors might change even in v1.x releases, although we will aim to still ensure a good migration path in such cases, where possible.\n\nWe are considering allowing external state storage backend implementations via plugins, similar to provider plugins. If we introduce such a mechanism during the v1.x releases then you may need to make configuration changes in order to use those plugins, and state storage backends other than those listed above may be removed from later versions of Terraform CLI once equivalent plugins are available.\n\nThe remote Backend and Terraform Cloud\n\nThe remote backend is maintained by the Terraform Cloud team and so its behavior may change along with ongoing changes to Terraform Cloud.\n\nThere will be a supported mechanism to use Terraform CLI with Terraform Cloud throughout the v1.x releases, but the exact details may change. Terraform Cloud evolves independently of Terraform CLI and is therefore not subject to these compatibility promises.\n\nCommunity-maintained State Storage Backends\n\nThe azurerm, consul, s3, and kubernetes backends are maintained by other teams at HashiCorp. Those teams intend to continue basic maintenence at the level of bug fixes through the v1.x releases, unless we implement a plugin protocol for backends at which point development of these backends is likely to continue in the external plugins only, which may require configuration changes to switch to the plugin equivalents.\n\nThe cos, oss, pg, gcs, and etcdv3 backends are maintained by outside contributors and are not subject to these compatibility promises.\n\nUnmaintained State Storage Backends\n\nThe artifactory, etcdv2, manta, and swift state storage backends do not currently have any maintainers and thus remain in Terraform CLI releases on a best-effort basis. They may be removed in later v1.x releases, and will not be updated in case of any breaking changes to the services they integrate with.\n\nSupported Platforms\n\nThroughout the v1.x series we will continue to produce official releases for the following platforms, and make changes as necessary to support new releases of these operating systems:\n\nmacOS on x64 CPUs (darwin_amd64)\nWindows on x64 CPUs (windows_amd64)\nLinux on x64, 32-bit ARMv6, and 64-bit ARMv8 (linux_amd64, linux_arm, and linux_arm64 respectively)\n\nOver time we may require newer versions of these operating systems. For example, subsequent Terraform releases in the v1.x series might end support for earlier versions of macOS or Windows, or earlier Linux kernel releases.\n\nWe have historically produced official releases for a number of other platforms as a convenience to users of those platforms, and we have no current plans to stop publishing them but we cannot promise ongoing releases or bug fixes for the other platforms throughout the v1.x series. We do not routinely test Terraform on any platforms other than those listed above.\n\nWe might add support for new platforms in later v1.x releases. If so, earlier Terraform releases prior to that support will not be available on those platforms.\n\nAll Terraform plugins, including provider plugins, are separate programs that have their own policies for which platforms they support. We cannot guarantee that all providers currently support or will continue to support the platforms listed above, even though Terraform CLI itself will support them.\n\nLater Revisions to These Promises\n\nWe may extend or refine these promises throughout the v1.x series in order to describe promises related to new features or to clarify existing promises if we find via feedback that our earlier statements had been unclear.\n\nPromises for new features will be additive in the sense that they will add further promises without retracting any existing ones. For promises that only apply to later v1.x releases we will mention the earliest version(s) those promises apply to.\n\nEven if we don't add an explicit statement to this document, we intend that any non-experimental features added in later v1.x releases will remain compatible at least through the remainder of the v1.x series, unless otherwise stated.\n\nAppendices\nProtected Workflow Commands\n\nThe following is the list of Terraform CLI subcommands and options that are subject to these compatibility promises. If you build automation around these commands then it should be compatible with all later v1.x releases.\n\nAs noted above, compatibility with external software is limited to explicitly-machine-readable output (-json and -raw modes) and exit codes. Any natural-language output from these commands might change in later releases.\n\ninit\n-backend=false\n-backend-config=FILE\n-backend-config=\"KEY=VALUE\"\n-force-copy\n-get=false\n-input=false\n-migrate-state\n-no-color\n-plugin-dir=DIR\n-reconfigure\n-upgrade\nvalidate\n-json\n-no-color\nplan\n-compact-warnings\n-destroy\n-detailed-exitcode\n-lock=false\n-lock-timeout=DURATION\n-input=false\n-json\n-no-color\n-out=FILE\n-parallelism=N\n-refresh=false\n-refresh-only\n-replace=ADDRESS\n-target=ADDRESS\n-var 'NAME=VALUE'\n-var-file=FILE\napply\n-auto-approve\n-compact-warnings\n-lock=false\n-lock-timeout=DURATION\n-input=false\n-json\n-no-color\n-parallelism=N\n-refresh=false\n-refresh-only\n-replace=ADDRESS\n-target=ADDRESS\n-var 'NAME=VALUE'\n-var-file=FILE\nshow\n-no-color\n-json\n(both with and without a plan file)\nproviders (with no subcommand)\nproviders lock\n-fs-mirror=PATH\n-net-mirror=URL\n-platform=OS_ARCH\nproviders mirror\n-platform=OS_ARCH\nproviders schema\n-json\nfmt\n-list=false\n-write=false\n-diff\n-recursive\n-check\nversion\n-json\noutput\n-no-color\n-json\n-raw\ntaint\n-allow-missing\n-lock=false\n-lock-timeout=DURATION\n-ignore-remote-version\nuntaint\n-allow-missing\n-lock=false\n-lock-timeout=DURATION\n-ignore-remote-version\nforce-unlock\n-force\nstate list\n-id=ID\nstate pull\nstate push\n-force\n-lock=false\n-lock-timeout=DURATION\nstate show\n-ignore-remote-version\nlogin\n\nFor commands or options not in the above list, we will still avoid breaking changes where possible, but can't promise full compatibility throughout the v1.x series. If you are building automation around Terraform, use only the commands above to avoid the need for changes when upgrading.\n\nPlease note that although Terraform's internal logs (via the TF_LOG environment variable) are available in a JSON format, the particular syntax or structure of those log lines is not a supported integration interface. The logs are available as JSON only to help with ad-hoc filtering and processing of logs by Terraform developers.\n\nCommands That Might Change\n\nAll of the following commands and their subcommands/options are not subject to compatibility promises, either because we have existing plans to improve them during the v1.x series or because we are aware of shortcomings in their design that might require breaking changes for ongoing maintenence.\n\nWhile you can freely use these commands when running Terraform interactively as long as they remain supported, we don't recommend using them as part of any automation unless you are willing to potentially update that automation when upgrading to a later v1.x release.\n\ndestroy (consider terraform apply -destroy instead)\nconsole\nget (consider terraform init instead)\ngraph\nimport\npush\nrefresh (consider terraform apply -refresh-only instead)\nstate mv\nstate replace-provider\nstate rm\nall subcommands of workspace (and its deprecated alias env)\n\nWhile we do intend to retain support for the main use-cases associated with these commands in future releases, we cannot promise to retain the exact command names or options used to meet those use-cases.\n\nHow We Will Keep These Promises\nAutomated Regression Testing\n\nThe Terraform codebase includes various unit and integration tests intended to help us to notice accidental behavior regressions before they ship in a stable version.\n\nHowever, Terraform is a relatively complex system with many different features that can interact in interesting ways. In the past we've seen reports of behavior differences that appeared only when combining two or more features in a way we hadn't previously anticipated or written automated tests for.\n\nIn each case we have both implemented a change to resolve the compatibility problem and added one or more integration tests representing the behavior of that combination of features. We intend to continue this approach, so we can improve Terraform's test coverage over time.\n\nPrerelease Versions\n\nWe intend that most accidental changes in behavior covered by these promises will be caught by existing tests. However, we also accept that our test suite can never have perfect coverage of all possible feature interactions or other edge cases, and so we aim for each significant change to be included in both alpha and beta releases before eventual inclusion in a final release.\n\nFor minor releases we will typically also issue at least one release candidate prior to the final release. A release candidate represents that planned development is concluded and that we've fixed any regressions reported based on the alpha and beta releases, and thus the final release that follows should typically match exactly or almost exactly its most recent release candidate.\n\nRegressions in Final Releases\n\nFor more obscure combinations of features it is possible that a regression could be undetected during prerelease the prerelease periods and thus included in a final release.\n\nIf someone finds and reports such a regression soon after its release then we will treat it as a bug and fix it to restore the previous behavior in future releases, unless there is a very significant justification such as a security advisory. In these cases, we'll typically recommend anyone affected by the regression remain on the previous version until the problem is fixed and then skip forward directly to the new release containing that fix.\n\nYou can minimize the risk of being affected by missed regressions in final releases by proactively testing modules against alpha, beta, and release candidate packages. We recommend doing so only in isolated development or staging environments rather than against your production infrastructure. If you find a change in behavior in a prerelease build that seems contrary to the promises in this document, please open an issue in Terraform's GitHub repository to discuss it.\n\nLate-reported Regressions\n\nIn the most extreme case, there may be a regression with a combination of features that is so rare that it remains undetected for a long time.\n\nAfter a change has been included in more releases it becomes increasingly likely that other users will have depended on the newer behavior and thus we will need to make a tradeoff to decide whether restoring the behavior would have a greater negative impact than retaining the new behavior. We will always make this decision with due consideration to the implications of each unique situation.\n\nYou can minimize the risk of your modules being affected by late-reported regressions by upgrading promptly to new minor and patch releases of Terraform and reporting any compatibility problems you encounter in Terraform's GitHub repository.\n\nPragmatic Exceptions\n\nWe are making the promises above in good faith, with the intent that your investment in writing Terraform modules or automation will not be invalidated by future changes to Terraform. However, broad promises like the above can't possibly cover all nuances of practical problems that might arise as we continue to develop Terraform.\n\nFor that reason, there are some situations where we may still need to make changes that may impact existing modules or automation:\n\nSecurity problems: We may become aware of a design problem that has an important security impact. Depending on our determination of the associated risk, we may choose to break compatibility to achieve a more secure system.\nExternal Dependencies: Terraform's behavior depends on interfaces provided by external codebases, including your chosen operating system and including some remote network services for situations such as module and provider installation. These external systems can change outside of our control, including potentially removing or changing features that Terraform's own features depend on. In that case, if there is no suitable replacement mechanism then we may need to change Terraform's design to work within the new constraints.\nOpt-in Compatibility Breaks: The design of a language new feature may require changing the behavior or configuration representation of an existing feature. If so, we will typically make the new feature opt-in only in order to avoid breaking existing modules, but if you change your module to opt in to the new feature then you may also then be required to change other parts of your configuration to work with the new language design.\nBugs in New Features: If we introduce a new feature to Terraform and the initial implementation has problems that cause it to not match the documented design intent at release, we may make a follow-up release that corrects the implementation to match the documented design, even if that represents a minor compatibility regression compared to the initial implementation. However, once a feature is well-established and in common use we will usually defer to the implemented behavior and instead change the documentation to reflect it.\nRegressions in Existing Features: If we learn that a new Terraform release includes a regression for an existing feature that wasn't detected during the development and prerelease periods, and that learning comes promptly after the new release, we will typically restore the previous behavior at the expense of technically therefore breaking compatibility with the behavior of the new release, under the assumption that more users will have systems affected by the regression than will have systems depending on the newly-introduced behavior.\nLate-reported regressions: As described in the previous section, if we learn that there was an unintentional regression of a rarely-used feature or combination of features in a much earlier release then restoring the previous behavior may appear as a regression to later adopters. If we believe that fixing the regression would affect more users than the regression itself affects then we may choose to accept the regression as the new promised behavior.\nSituations we cannot anticipate: Although we've made an effort to consider various specific exceptional situations here, Terraform and its development process are not isolated from broader context, and so we must consider that there may be situations that we cannot possibly anticipate that would affect the future of Terraform. In those situations, we will always do our best to find a course of action that will minimize as much as possible the impact to existing modules and automation.\n\nOur intent with these pragmatic exceptions is only to acknowledge that there will always be situations that general compatibility promises cannot address. We will use these exceptions only with due consideration and as a last resort.\n\nOn this page:\n\nTerraform v1.x Compatibility Promises\nThe Terraform Language\nWorkflow\nUpgrading and Downgrading\nProviders\nExternal Modules\nProvisioners\nState Storage Backends\nCommunity-maintained State Storage Backends\nSupported Platforms\nLater Revisions to These Promises\nAppendices\nHow We Will Keep These Promises\nTheme\nDark\nLight\nSystem\nCertifications\nSystem Status\nCookie Manager\nTerms of Use\nSecurity\nPrivacy\nTrademark Policy\nTrade Controls\nGive Feedback\n(opens in new tab)\nWe use cookies & other similar technology to collect data to improve your experience on our site, as described in our Privacy Policy and Cookie Policy.\nManage PreferencesACCEPT"
  },
  {
    "title": "Terraform v1.x Compatibility Promises | Terraform | HashiCorp Developer",
    "url": "https://developer.hashicorp.com/terraform/language/v1.3.x/v1-compatibility-promises",
    "html": "Skip to main content\nNews\nHashiCorp to join IBM to accelerate multi-cloud automation.\nRead the blog →\nDismiss alert\nTerraform\nInstall\nTutorials\nDocumentation\nRegistry\n(opens in new tab)\nTry Cloud\n(opens in new tab)\nSearch\nCommand or control key\nK key\nTerraform Home\nConfiguration Language\nConfiguration Language\nTerraform Language\nOverview\nStyle Guide\nFiles and Directories\nSyntax\nResources\nData Sources\nProviders\nVariables and Outputs\nModules\nMoved block\nExpressions\nFunctions\nTerraform Settings\nState\nUpgrading to Terraform v1.3\nv1.x Compatibility Promises\nTerraform Internals\nResources\nTutorial Library\nCertifications\nCommunity Forum\n(opens in new tab)\nSupport\n(opens in new tab)\nGitHub\n(opens in new tab)\nTerraform Registry\n(opens in new tab)\n\nYou are viewing documentation for version v1.3.x. View latest version.\n\nDeveloper\nTerraform\nConfiguration Language v1.3.x\nv1.x Compatibility Promises\nv1.3.x\nTerraform v1.x Compatibility Promises\n\nThe release of Terraform v1.0 represents an important milestone in the development of the Terraform language and workflow. Terraform v1.0 is a stable platform for describing and managing infrastructure.\n\nIn this release we're defining a number of Terraform behaviors that we intend to remain compatible with throughout the 1.x releases:\n\nA large subset of Terraform language features.\nA more conservative subset of the Terraform CLI workflow commands.\nThe wire protocol for communication between Terraform Core and Terraform providers.\nThe wire protocols for installation of Terraform providers and external Terraform modules.\n\nOur intention is that Terraform modules written for Terraform v1.0 will continue to plan and apply successfully, without required changes, throughout the v1.x releases.\n\nWe also intend that automation built around the workflow subset described in this document will work without changes in all future v1.x releases.\n\nFinally, we intend that providers built against the currently-documented provider wire protocol will be compatible with all future Terraform v1.x releases targeting the same operating system and architecture, without the need for source code modification or binary recompilation.\n\nIn short, we aim to make upgrades between v1.x releases straightforward, requiring no changes to your configuration, no extra commands to run upgrade steps, and no changes to any automation you've set up around Terraform.\n\nThe Terraform v1.x series will be actively maintained for at least 18 months after v1.0.\n\nThe following sections include some specific guidance on what we will promise throughout the v1.x series, for those who would like full details. At a higher level though, we don't intend to make any changes that would cause existing modules or automation to require changes when upgrading to a new v1.x release. We will generally treat compatibility problems in new Terraform CLI releases as bugs to be fixed unless there was a very significant justification for the change, such as in addressing a critical security problem or matching with a breaking change to a remote dependency that isn't directly under our control.\n\nThe Terraform Language\n\nThe main Terraform Language includes the language syntax, the top-level structures such as resource, module, and provider blocks, the \"meta-arguments\" in those blocks, and the documented semantics and behaviors for the operators and built-in functions available for use in expressions.\n\nThere is not a single formal specification for the Terraform language, but the Configuration section of the documentation on the Terraform website serves as a description of the language features and their intended behaviors.\n\nThe following top-level blocks and their defined \"meta-arguments\" (that is, arguments defined by Terraform Core rather than by external plugins such as providers) will retain their current functionality:\n\nresource and data blocks to declare resources, including their nested block types lifecycle, connection, and provisioner, and their meta-argument provider.\nmodule blocks to call other modules, and its meta-argument providers.\nThe count, for_each, and depends_on meta-arguments in resource, data, and module blocks.\nprovider blocks to configure providers, and the alias meta-argument.\nvariable, output, and locals blocks for declaring the various kinds of named values in a module.\nterraform blocks, including the nested required_version and required_providers arguments, and nested backend blocks for backend configuration.\n\nWe also intend to keep compatibility with all expression operators and built-in functions, with the exception of references to terraform.workspace, whose behavior may change as part of future changes to the workspace model.\n\nWe intend to retain broad compatibility with Terraform language features, with a few specific caveats:\n\nWe consider a configuration to be valid if Terraform can create and apply a plan for it without reporting any errors.\n\nA configuration that currently produces errors might generate different errors or exhibit other non-error behaviors in a future version of Terraform. A configuration that generates errors during the apply phase might generate similar errors at an earlier phase in future, because we generally consider it better to detect errors in as early a phase as possible.\n\nGenerally-speaking, the compatibility promises described in this document apply only to valid configurations. Handling of invalid configurations is always subject to change in future Terraform releases.\n\nIf the actual behavior of a feature differs from what we explicitly documented as the feature's behavior, we will usually treat that as a bug and change the feature to match the documentation, although we will avoid making such changes if they seem likely to cause broad compatibility problems. We cannot promise to always remain \"bug-compatible\" with previous releases, but we will consider such fixes carefully to minimize their impact.\n\nAny experimental features may change or may be removed entirely from future releases. Terraform always produces a warning when an experimental language feature is active, to make you aware of that risk. We don't recommend using experimental features in production modules.\n\nWe will introduce new language features, and if you start using them then your configuration won't work with prior Terraform versions that didn't support those features yet.\n\nTerraform Providers are separate plugins which can change independently of Terraform Core and are therefore not subject to these compatibility promises. If you upgrade any of the providers you are using then you might need to change provider or resource configurations related to those providers.\n\nA small number of features remain deprecated with explicit warnings in Terraform v1.0. Those deprecation cycles will end in a future v1.x release, at which point we will remove the corresponding features.\n\nWorkflow\n\nThere is a set of often used Terraform workflows, which we are calling protected workflows. We will not remove these commands, subcommands, and flags or make backward-incompatible changes to protected workflow functionality. If we accidentally change these, we will consider backwards-incompatible changes to core workflows as bugs to be fixed. For a list of the command and option combinations that are part of protected workflows, see Protected Workflow Commands. There is another set of commands that we are explicitly not making compatibility promises about, because we expect their functionality to change in v1.x releases: see Commands That Might Change.\n\nThe supported ways for external software to interact with Terraform are via the JSON output modes offered by some commands and via exit status codes. We may extend certain JSON formats with new object properties but we will not remove or make breaking changes to the definitions of existing properties.\n\nNatural language command output or log output is not a stable interface and may change in any new version. If you write software that parses this output then it may need to be updated when you upgrade Terraform. If you need access to data that is not currently available via one of the machine-readable JSON interfaces, we suggest opening a feature request to discuss your use-case.\n\nUpgrading and Downgrading\n\nThroughout the v1.x series of releases, we intend that you should be able to switch to a newer Terraform version and use it just as before, without any special upgrade steps.\n\nYou should be able to upgrade from any v1.x release to any later v1.x release. You might also be able to downgrade to an earlier v1.x release, but that isn't guaranteed: later releases may introduce new features that earlier versions cannot understand, including new storage formats for Terraform state snapshots.\n\nIf you make use of features introduced in a later v1.x release, your configuration won't be compatible with releases that predate that feature. For example, if a language feature is added in v1.3 and you start using it, your Terraform configuration will no longer be compatible with Terraform v1.2.\n\nProviders\n\nTerraform providers are separate plugins which communicate with Terraform using a documented protocol. Therefore these compatibility promises can only cover the \"client\" side of this protocol as implemented by Terraform Core; the behaviors of individual providers, including which resource types they support and which arguments they expect, are decided by the provider development teams and can change independently of Terraform Core releases.\n\nIf you upgrade to a new version of a provider then you might need to change the parts of your configuration which are interpreted by that provider, even if you are still using a Terraform v1.x release.\n\nProvider Installation Methods\n\nTerraform normally installs providers from a provider registry implementing the Provider Registry Protocol, version 1. All Terraform v1.x releases will remain compatible with that protocol, and so correctly-implemented provider registries will stay compatible.\n\nTerraform also supports installation of providers from local filesystem directories (filesystem mirrors) and from network mirrors (implementing the Provider Mirror Protocol. All Terraform v1.x releases will remain compatible with those installation methods, including the Implied Local Mirror Directories.\n\nSpecific provider registries or network mirrors are run independently from Terraform itself and so their own behaviors are not subject to these compatibility promises.\n\nProvider Protocol Versions\n\nThe current major version of the provider plugin protocol as of Terraform v1.0 is version 5, which is defined by a combination of a Protocol Buffers schema describing the physical wire formats and by additional prose documentation describing the expected provider behaviors.\n\nWe will support protocol version 5 throughout the Terraform v1.x releases. If we make new minor revisions to protocol version 5 in later releases then we will design them such that existing plugins will continue to work, as long as they correctly implemented the protocol.\n\nWe may introduce new major versions of the protocol during the v1.x series. If so, we will continue to support protocol version 5 alongside those new versions. Individual provider teams might decide to remove support for protocol version 5 in later releases, in which case those new provider releases will not be compatible with all of the Terraform v1.x releases.\n\nExternal Modules\n\nTerraform modules are reusable infrastructure components written in the Terraform language. Some modules are \"external\" in the sense that Terraform automatically installs them from a location other than the current configuration directory, in which case their contents could change independently of changes to your local modules, of the providers you use, and of Terraform itself.\n\nModule Installation Methods\n\nTerraform supports installing child modules from a number of different module source types. We will continue to support all of the existing source types throughout the v1.x releases.\n\nOne of the supported source types is a module registry implementing the Module Registry Protocol version 1. All Terraform v1.x releases will remain compatible with correct implementations of that protocol.\n\nSome module source types work directly with services or protocols defined and run by third parties. Although we will not remove Terraform's own client-side support for those, we cannot guarantee that their owners will keep those services running or that they will remain compatible with Terraform's client implementations.\n\nExternal Module Compatibility\n\nIf your configuration depends on external modules, newer versions of those modules may include breaking changes. External modules are not part of Terraform and are therefore not subject to these compatibility promises.\n\nProvisioners\n\nWe will maintain compatibility for the file, local-exec, and remote-exec provisioner types through all v1.x releases.\n\nSome additional vendor-specific provisioners were available in earlier Terraform versions but were deprecated in Terraform v0.13 and removed in Terraform v0.15.\n\nTerraform supports loading additional provisioners as plugins from certain local filesystem directories. We'll continue to support that throughout the Terraform v1.x releases, but since such plugins are separate from Terraform Core itself their own behaviors cannot be subject to these compatibility promises. However, we will continue to support the plugin wire protocol as defined in Terraform v1.0 throughout the v1.x releases, and so correctly-implemented provisioner plugins should remain compatible with future Terraform releases.\n\nState Storage Backends\n\nWhen you use remote state, Terraform interacts with remote services over the network in order to store and manage locks for Terraform state.\n\nFor historical reasons, all supported state storage backends are included as part of Terraform CLI but not all are supported directly by the Terraform Team. Only the following backends maintained by the Terraform team are subject to compatibility promises:\n\nlocal (the default, when you are not using remote state)\nhttp\n\nThe other state storage backends are maintained by external teams via contributions to the Terraform CLI codebase, and so their expected configuration arguments or behaviors might change even in v1.x releases, although we will aim to still ensure a good migration path in such cases, where possible.\n\nWe are considering allowing external state storage backend implementations via plugins, similar to provider plugins. If we introduce such a mechanism during the v1.x releases then you may need to make configuration changes in order to use those plugins, and state storage backends other than those listed above may be removed from later versions of Terraform CLI once equivalent plugins are available.\n\nThe remote Backend and Terraform Cloud\n\nThe remote backend is maintained by the Terraform Cloud team and so its behavior may change along with ongoing changes to Terraform Cloud.\n\nThere will be a supported mechanism to use Terraform CLI with Terraform Cloud throughout the v1.x releases, but the exact details may change. Terraform Cloud evolves independently of Terraform CLI and is therefore not subject to these compatibility promises.\n\nCommunity-maintained State Storage Backends\n\nThe azurerm, consul, s3, and kubernetes backends are maintained by other teams at HashiCorp. Those teams intend to continue basic maintenence at the level of bug fixes through the v1.x releases, unless we implement a plugin protocol for backends at which point development of these backends is likely to continue in the external plugins only, which may require configuration changes to switch to the plugin equivalents.\n\nThe cos, oss, pg, gcs, and etcdv3 backends are maintained by outside contributors and are not subject to these compatibility promises.\n\nUnmaintained State Storage Backends\n\nThe artifactory, etcdv2, manta, and swift state storage backends do not currently have any maintainers and thus remain in Terraform CLI releases on a best-effort basis. They may be removed in later v1.x releases, and will not be updated in case of any breaking changes to the services they integrate with.\n\nSupported Platforms\n\nThroughout the v1.x series we will continue to produce official releases for the following platforms, and make changes as necessary to support new releases of these operating systems:\n\nmacOS on x64 CPUs (darwin_amd64)\nWindows on x64 CPUs (windows_amd64)\nLinux on x64, 32-bit ARMv6, and 64-bit ARMv8 (linux_amd64, linux_arm, and linux_arm64 respectively)\n\nOver time we may require newer versions of these operating systems. For example, subsequent Terraform releases in the v1.x series might end support for earlier versions of macOS or Windows, or earlier Linux kernel releases.\n\nWe have historically produced official releases for a number of other platforms as a convenience to users of those platforms, and we have no current plans to stop publishing them but we cannot promise ongoing releases or bug fixes for the other platforms throughout the v1.x series. We do not routinely test Terraform on any platforms other than those listed above.\n\nWe might add support for new platforms in later v1.x releases. If so, earlier Terraform releases prior to that support will not be available on those platforms.\n\nAll Terraform plugins, including provider plugins, are separate programs that have their own policies for which platforms they support. We cannot guarantee that all providers currently support or will continue to support the platforms listed above, even though Terraform CLI itself will support them.\n\nLater Revisions to These Promises\n\nWe may extend or refine these promises throughout the v1.x series in order to describe promises related to new features or to clarify existing promises if we find via feedback that our earlier statements had been unclear.\n\nPromises for new features will be additive in the sense that they will add further promises without retracting any existing ones. For promises that only apply to later v1.x releases we will mention the earliest version(s) those promises apply to.\n\nEven if we don't add an explicit statement to this document, we intend that any non-experimental features added in later v1.x releases will remain compatible at least through the remainder of the v1.x series, unless otherwise stated.\n\nAppendices\nProtected Workflow Commands\n\nThe following is the list of Terraform CLI subcommands and options that are subject to these compatibility promises. If you build automation around these commands then it should be compatible with all later v1.x releases.\n\nAs noted above, compatibility with external software is limited to explicitly-machine-readable output (-json and -raw modes) and exit codes. Any natural-language output from these commands might change in later releases.\n\ninit\n-backend=false\n-backend-config=FILE\n-backend-config=\"KEY=VALUE\"\n-force-copy\n-get=false\n-input=false\n-migrate-state\n-no-color\n-plugin-dir=DIR\n-reconfigure\n-upgrade\nvalidate\n-json\n-no-color\nplan\n-compact-warnings\n-destroy\n-detailed-exitcode\n-lock=false\n-lock-timeout=DURATION\n-input=false\n-json\n-no-color\n-out=FILE\n-parallelism=N\n-refresh=false\n-refresh-only\n-replace=ADDRESS\n-target=ADDRESS\n-var 'NAME=VALUE'\n-var-file=FILE\napply\n-auto-approve\n-compact-warnings\n-lock=false\n-lock-timeout=DURATION\n-input=false\n-json\n-no-color\n-parallelism=N\n-refresh=false\n-refresh-only\n-replace=ADDRESS\n-target=ADDRESS\n-var 'NAME=VALUE'\n-var-file=FILE\nshow\n-no-color\n-json\n(both with and without a plan file)\nproviders (with no subcommand)\nproviders lock\n-fs-mirror=PATH\n-net-mirror=URL\n-platform=OS_ARCH\nproviders mirror\n-platform=OS_ARCH\nproviders schema\n-json\nfmt\n-list=false\n-write=false\n-diff\n-recursive\n-check\nversion\n-json\noutput\n-no-color\n-json\n-raw\ntaint\n-allow-missing\n-lock=false\n-lock-timeout=DURATION\n-ignore-remote-version\nuntaint\n-allow-missing\n-lock=false\n-lock-timeout=DURATION\n-ignore-remote-version\nforce-unlock\n-force\nstate list\n-id=ID\nstate pull\nstate push\n-force\n-lock=false\n-lock-timeout=DURATION\nstate show\n-ignore-remote-version\nlogin\n\nFor commands or options not in the above list, we will still avoid breaking changes where possible, but can't promise full compatibility throughout the v1.x series. If you are building automation around Terraform, use only the commands above to avoid the need for changes when upgrading.\n\nPlease note that although Terraform's internal logs (via the TF_LOG environment variable) are available in a JSON format, the particular syntax or structure of those log lines is not a supported integration interface. The logs are available as JSON only to help with ad-hoc filtering and processing of logs by Terraform developers.\n\nCommands That Might Change\n\nAll of the following commands and their subcommands/options are not subject to compatibility promises, either because we have existing plans to improve them during the v1.x series or because we are aware of shortcomings in their design that might require breaking changes for ongoing maintenence.\n\nWhile you can freely use these commands when running Terraform interactively as long as they remain supported, we don't recommend using them as part of any automation unless you are willing to potentially update that automation when upgrading to a later v1.x release.\n\ndestroy (consider terraform apply -destroy instead)\nconsole\nget (consider terraform init instead)\ngraph\nimport\npush\nrefresh (consider terraform apply -refresh-only instead)\nstate mv\nstate replace-provider\nstate rm\nall subcommands of workspace (and its deprecated alias env)\n\nWhile we do intend to retain support for the main use-cases associated with these commands in future releases, we cannot promise to retain the exact command names or options used to meet those use-cases.\n\nHow We Will Keep These Promises\nAutomated Regression Testing\n\nThe Terraform codebase includes various unit and integration tests intended to help us to notice accidental behavior regressions before they ship in a stable version.\n\nHowever, Terraform is a relatively complex system with many different features that can interact in interesting ways. In the past we've seen reports of behavior differences that appeared only when combining two or more features in a way we hadn't previously anticipated or written automated tests for.\n\nIn each case we have both implemented a change to resolve the compatibility problem and added one or more integration tests representing the behavior of that combination of features. We intend to continue this approach, so we can improve Terraform's test coverage over time.\n\nPrerelease Versions\n\nWe intend that most accidental changes in behavior covered by these promises will be caught by existing tests. However, we also accept that our test suite can never have perfect coverage of all possible feature interactions or other edge cases, and so we aim for each significant change to be included in both alpha and beta releases before eventual inclusion in a final release.\n\nFor minor releases we will typically also issue at least one release candidate prior to the final release. A release candidate represents that planned development is concluded and that we've fixed any regressions reported based on the alpha and beta releases, and thus the final release that follows should typically match exactly or almost exactly its most recent release candidate.\n\nRegressions in Final Releases\n\nFor more obscure combinations of features it is possible that a regression could be undetected during prerelease the prerelease periods and thus included in a final release.\n\nIf someone finds and reports such a regression soon after its release then we will treat it as a bug and fix it to restore the previous behavior in future releases, unless there is a very significant justification such as a security advisory. In these cases, we'll typically recommend anyone affected by the regression remain on the previous version until the problem is fixed and then skip forward directly to the new release containing that fix.\n\nYou can minimize the risk of being affected by missed regressions in final releases by proactively testing modules against alpha, beta, and release candidate packages. We recommend doing so only in isolated development or staging environments rather than against your production infrastructure. If you find a change in behavior in a prerelease build that seems contrary to the promises in this document, please open an issue in Terraform's GitHub repository to discuss it.\n\nLate-reported Regressions\n\nIn the most extreme case, there may be a regression with a combination of features that is so rare that it remains undetected for a long time.\n\nAfter a change has been included in more releases it becomes increasingly likely that other users will have depended on the newer behavior and thus we will need to make a tradeoff to decide whether restoring the behavior would have a greater negative impact than retaining the new behavior. We will always make this decision with due consideration to the implications of each unique situation.\n\nYou can minimize the risk of your modules being affected by late-reported regressions by upgrading promptly to new minor and patch releases of Terraform and reporting any compatibility problems you encounter in Terraform's GitHub repository.\n\nPragmatic Exceptions\n\nWe are making the promises above in good faith, with the intent that your investment in writing Terraform modules or automation will not be invalidated by future changes to Terraform. However, broad promises like the above can't possibly cover all nuances of practical problems that might arise as we continue to develop Terraform.\n\nFor that reason, there are some situations where we may still need to make changes that may impact existing modules or automation:\n\nSecurity problems: We may become aware of a design problem that has an important security impact. Depending on our determination of the associated risk, we may choose to break compatibility to achieve a more secure system.\nExternal Dependencies: Terraform's behavior depends on interfaces provided by external codebases, including your chosen operating system and including some remote network services for situations such as module and provider installation. These external systems can change outside of our control, including potentially removing or changing features that Terraform's own features depend on. In that case, if there is no suitable replacement mechanism then we may need to change Terraform's design to work within the new constraints.\nOpt-in Compatibility Breaks: The design of a language new feature may require changing the behavior or configuration representation of an existing feature. If so, we will typically make the new feature opt-in only in order to avoid breaking existing modules, but if you change your module to opt in to the new feature then you may also then be required to change other parts of your configuration to work with the new language design.\nBugs in New Features: If we introduce a new feature to Terraform and the initial implementation has problems that cause it to not match the documented design intent at release, we may make a follow-up release that corrects the implementation to match the documented design, even if that represents a minor compatibility regression compared to the initial implementation. However, once a feature is well-established and in common use we will usually defer to the implemented behavior and instead change the documentation to reflect it.\nRegressions in Existing Features: If we learn that a new Terraform release includes a regression for an existing feature that wasn't detected during the development and prerelease periods, and that learning comes promptly after the new release, we will typically restore the previous behavior at the expense of technically therefore breaking compatibility with the behavior of the new release, under the assumption that more users will have systems affected by the regression than will have systems depending on the newly-introduced behavior.\nLate-reported regressions: As described in the previous section, if we learn that there was an unintentional regression of a rarely-used feature or combination of features in a much earlier release then restoring the previous behavior may appear as a regression to later adopters. If we believe that fixing the regression would affect more users than the regression itself affects then we may choose to accept the regression as the new promised behavior.\nSituations we cannot anticipate: Although we've made an effort to consider various specific exceptional situations here, Terraform and its development process are not isolated from broader context, and so we must consider that there may be situations that we cannot possibly anticipate that would affect the future of Terraform. In those situations, we will always do our best to find a course of action that will minimize as much as possible the impact to existing modules and automation.\n\nOur intent with these pragmatic exceptions is only to acknowledge that there will always be situations that general compatibility promises cannot address. We will use these exceptions only with due consideration and as a last resort.\n\nOn this page:\n\nTerraform v1.x Compatibility Promises\nThe Terraform Language\nWorkflow\nUpgrading and Downgrading\nProviders\nExternal Modules\nProvisioners\nState Storage Backends\nCommunity-maintained State Storage Backends\nSupported Platforms\nLater Revisions to These Promises\nAppendices\nHow We Will Keep These Promises\nTheme\nDark\nLight\nSystem\nCertifications\nSystem Status\nCookie Manager\nTerms of Use\nSecurity\nPrivacy\nTrademark Policy\nTrade Controls\nGive Feedback\n(opens in new tab)\nWe use cookies & other similar technology to collect data to improve your experience on our site, as described in our Privacy Policy and Cookie Policy.\nManage PreferencesACCEPT"
  },
  {
    "title": "Terraform v1.x Compatibility Promises | Terraform | HashiCorp Developer",
    "url": "https://developer.hashicorp.com/terraform/language/v1.4.x/v1-compatibility-promises",
    "html": "Skip to main content\nNews\nHashiCorp to join IBM to accelerate multi-cloud automation.\nRead the blog →\nDismiss alert\nTerraform\nInstall\nTutorials\nDocumentation\nRegistry\n(opens in new tab)\nTry Cloud\n(opens in new tab)\nSearch\nCommand or control key\nK key\nTerraform Home\nConfiguration Language\nConfiguration Language\nTerraform Language\nOverview\nStyle Guide\nFiles and Directories\nSyntax\nResources\nData Sources\nProviders\nVariables and Outputs\nModules\nExpressions\nFunctions\nTerraform Settings\nState\nUpgrading to Terraform v1.4\nv1.x Compatibility Promises\nTerraform Internals\nResources\nTutorial Library\nCertifications\nCommunity Forum\n(opens in new tab)\nSupport\n(opens in new tab)\nGitHub\n(opens in new tab)\nTerraform Registry\n(opens in new tab)\n\nYou are viewing documentation for version v1.4.x. View latest version.\n\nDeveloper\nTerraform\nConfiguration Language v1.4.x\nv1.x Compatibility Promises\nv1.4.x\nTerraform v1.x Compatibility Promises\n\nThe release of Terraform v1.0 represents an important milestone in the development of the Terraform language and workflow. Terraform v1.0 is a stable platform for describing and managing infrastructure.\n\nIn this release we're defining a number of Terraform behaviors that we intend to remain compatible with throughout the 1.x releases:\n\nA large subset of Terraform language features.\nA more conservative subset of the Terraform CLI workflow commands.\nThe wire protocol for communication between Terraform Core and Terraform providers.\nThe wire protocols for installation of Terraform providers and external Terraform modules.\n\nOur intention is that Terraform modules written for Terraform v1.0 will continue to plan and apply successfully, without required changes, throughout the v1.x releases.\n\nWe also intend that automation built around the workflow subset described in this document will work without changes in all future v1.x releases.\n\nFinally, we intend that providers built against the currently-documented provider wire protocol will be compatible with all future Terraform v1.x releases targeting the same operating system and architecture, without the need for source code modification or binary recompilation.\n\nIn short, we aim to make upgrades between v1.x releases straightforward, requiring no changes to your configuration, no extra commands to run upgrade steps, and no changes to any automation you've set up around Terraform.\n\nThe Terraform v1.x series will be actively maintained for at least 18 months after v1.0.\n\nThe following sections include some specific guidance on what we will promise throughout the v1.x series, for those who would like full details. At a higher level though, we don't intend to make any changes that would cause existing modules or automation to require changes when upgrading to a new v1.x release. We will generally treat compatibility problems in new Terraform CLI releases as bugs to be fixed unless there was a very significant justification for the change, such as in addressing a critical security problem or matching with a breaking change to a remote dependency that isn't directly under our control.\n\nThe Terraform Language\n\nThe main Terraform Language includes the language syntax, the top-level structures such as resource, module, and provider blocks, the \"meta-arguments\" in those blocks, and the documented semantics and behaviors for the operators and built-in functions available for use in expressions.\n\nThere is not a single formal specification for the Terraform language, but the Configuration section of the documentation on the Terraform website serves as a description of the language features and their intended behaviors.\n\nThe following top-level blocks and their defined \"meta-arguments\" (that is, arguments defined by Terraform Core rather than by external plugins such as providers) will retain their current functionality:\n\nresource and data blocks to declare resources, including their nested block types lifecycle, connection, and provisioner, and their meta-argument provider.\nmodule blocks to call other modules, and its meta-argument providers.\nThe count, for_each, and depends_on meta-arguments in resource, data, and module blocks.\nprovider blocks to configure providers, and the alias meta-argument.\nvariable, output, and locals blocks for declaring the various kinds of named values in a module.\nterraform blocks, including the nested required_version and required_providers arguments, and nested backend blocks for backend configuration.\n\nWe also intend to keep compatibility with all expression operators and built-in functions, with the exception of references to terraform.workspace, whose behavior may change as part of future changes to the workspace model.\n\nWe intend to retain broad compatibility with Terraform language features, with a few specific caveats:\n\nWe consider a configuration to be valid if Terraform can create and apply a plan for it without reporting any errors.\n\nA configuration that currently produces errors might generate different errors or exhibit other non-error behaviors in a future version of Terraform. A configuration that generates errors during the apply phase might generate similar errors at an earlier phase in future, because we generally consider it better to detect errors in as early a phase as possible.\n\nGenerally-speaking, the compatibility promises described in this document apply only to valid configurations. Handling of invalid configurations is always subject to change in future Terraform releases.\n\nIf the actual behavior of a feature differs from what we explicitly documented as the feature's behavior, we will usually treat that as a bug and change the feature to match the documentation, although we will avoid making such changes if they seem likely to cause broad compatibility problems. We cannot promise to always remain \"bug-compatible\" with previous releases, but we will consider such fixes carefully to minimize their impact.\n\nAny experimental features may change or may be removed entirely from future releases. Terraform always produces a warning when an experimental language feature is active, to make you aware of that risk. We don't recommend using experimental features in production modules.\n\nWe will introduce new language features, and if you start using them then your configuration won't work with prior Terraform versions that didn't support those features yet.\n\nTerraform Providers are separate plugins which can change independently of Terraform Core and are therefore not subject to these compatibility promises. If you upgrade any of the providers you are using then you might need to change provider or resource configurations related to those providers.\n\nA small number of features remain deprecated with explicit warnings in Terraform v1.0. Those deprecation cycles will end in a future v1.x release, at which point we will remove the corresponding features.\n\nWorkflow\n\nThere is a set of often used Terraform workflows, which we are calling protected workflows. We will not remove these commands, subcommands, and flags or make backward-incompatible changes to protected workflow functionality. If we accidentally change these, we will consider backwards-incompatible changes to core workflows as bugs to be fixed. For a list of the command and option combinations that are part of protected workflows, see Protected Workflow Commands. There is another set of commands that we are explicitly not making compatibility promises about, because we expect their functionality to change in v1.x releases: see Commands That Might Change.\n\nThe supported ways for external software to interact with Terraform are via the JSON output modes offered by some commands and via exit status codes. We may extend certain JSON formats with new object properties but we will not remove or make breaking changes to the definitions of existing properties.\n\nNatural language command output or log output is not a stable interface and may change in any new version. If you write software that parses this output then it may need to be updated when you upgrade Terraform. If you need access to data that is not currently available via one of the machine-readable JSON interfaces, we suggest opening a feature request to discuss your use-case.\n\nUpgrading and Downgrading\n\nThroughout the v1.x series of releases, we intend that you should be able to switch to a newer Terraform version and use it just as before, without any special upgrade steps.\n\nYou should be able to upgrade from any v1.x release to any later v1.x release. You might also be able to downgrade to an earlier v1.x release, but that isn't guaranteed: later releases may introduce new features that earlier versions cannot understand, including new storage formats for Terraform state snapshots.\n\nIf you make use of features introduced in a later v1.x release, your configuration won't be compatible with releases that predate that feature. For example, if a language feature is added in v1.3 and you start using it, your Terraform configuration will no longer be compatible with Terraform v1.2.\n\nProviders\n\nTerraform providers are separate plugins which communicate with Terraform using a documented protocol. Therefore these compatibility promises can only cover the \"client\" side of this protocol as implemented by Terraform Core; the behaviors of individual providers, including which resource types they support and which arguments they expect, are decided by the provider development teams and can change independently of Terraform Core releases.\n\nIf you upgrade to a new version of a provider then you might need to change the parts of your configuration which are interpreted by that provider, even if you are still using a Terraform v1.x release.\n\nProvider Installation Methods\n\nTerraform normally installs providers from a provider registry implementing the Provider Registry Protocol, version 1. All Terraform v1.x releases will remain compatible with that protocol, and so correctly-implemented provider registries will stay compatible.\n\nTerraform also supports installation of providers from local filesystem directories (filesystem mirrors) and from network mirrors (implementing the Provider Mirror Protocol. All Terraform v1.x releases will remain compatible with those installation methods, including the Implied Local Mirror Directories.\n\nSpecific provider registries or network mirrors are run independently from Terraform itself and so their own behaviors are not subject to these compatibility promises.\n\nProvider Protocol Versions\n\nThe current major version of the provider plugin protocol as of Terraform v1.0 is version 5, which is defined by a combination of a Protocol Buffers schema describing the physical wire formats and by additional prose documentation describing the expected provider behaviors.\n\nWe will support protocol version 5 throughout the Terraform v1.x releases. If we make new minor revisions to protocol version 5 in later releases then we will design them such that existing plugins will continue to work, as long as they correctly implemented the protocol.\n\nWe may introduce new major versions of the protocol during the v1.x series. If so, we will continue to support protocol version 5 alongside those new versions. Individual provider teams might decide to remove support for protocol version 5 in later releases, in which case those new provider releases will not be compatible with all of the Terraform v1.x releases.\n\nExternal Modules\n\nTerraform modules are reusable infrastructure components written in the Terraform language. Some modules are \"external\" in the sense that Terraform automatically installs them from a location other than the current configuration directory, in which case their contents could change independently of changes to your local modules, of the providers you use, and of Terraform itself.\n\nModule Installation Methods\n\nTerraform supports installing child modules from a number of different module source types. We will continue to support all of the existing source types throughout the v1.x releases.\n\nOne of the supported source types is a module registry implementing the Module Registry Protocol version 1. All Terraform v1.x releases will remain compatible with correct implementations of that protocol.\n\nSome module source types work directly with services or protocols defined and run by third parties. Although we will not remove Terraform's own client-side support for those, we cannot guarantee that their owners will keep those services running or that they will remain compatible with Terraform's client implementations.\n\nExternal Module Compatibility\n\nIf your configuration depends on external modules, newer versions of those modules may include breaking changes. External modules are not part of Terraform and are therefore not subject to these compatibility promises.\n\nProvisioners\n\nWe will maintain compatibility for the file, local-exec, and remote-exec provisioner types through all v1.x releases.\n\nSome additional vendor-specific provisioners were available in earlier Terraform versions but were deprecated in Terraform v0.13 and removed in Terraform v0.15.\n\nTerraform supports loading additional provisioners as plugins from certain local filesystem directories. We'll continue to support that throughout the Terraform v1.x releases, but since such plugins are separate from Terraform Core itself their own behaviors cannot be subject to these compatibility promises. However, we will continue to support the plugin wire protocol as defined in Terraform v1.0 throughout the v1.x releases, and so correctly-implemented provisioner plugins should remain compatible with future Terraform releases.\n\nState Storage Backends\n\nWhen you use remote state, Terraform interacts with remote services over the network in order to store and manage locks for Terraform state.\n\nFor historical reasons, all supported state storage backends are included as part of Terraform CLI but not all are supported directly by the Terraform Team. Only the following backends maintained by the Terraform team are subject to compatibility promises:\n\nlocal (the default, when you are not using remote state)\nhttp\n\nThe other state storage backends are maintained by external teams via contributions to the Terraform CLI codebase, and so their expected configuration arguments or behaviors might change even in v1.x releases, although we will aim to still ensure a good migration path in such cases, where possible.\n\nWe are considering allowing external state storage backend implementations via plugins, similar to provider plugins. If we introduce such a mechanism during the v1.x releases then you may need to make configuration changes in order to use those plugins, and state storage backends other than those listed above may be removed from later versions of Terraform CLI once equivalent plugins are available.\n\nThe remote Backend and Terraform Cloud\n\nThe remote backend is maintained by the Terraform Cloud team and so its behavior may change along with ongoing changes to Terraform Cloud.\n\nThere will be a supported mechanism to use Terraform CLI with Terraform Cloud throughout the v1.x releases, but the exact details may change. Terraform Cloud evolves independently of Terraform CLI and is therefore not subject to these compatibility promises.\n\nCommunity-maintained State Storage Backends\n\nThe azurerm, consul, s3, and kubernetes backends are maintained by other teams at HashiCorp. Those teams intend to continue basic maintenence at the level of bug fixes through the v1.x releases, unless we implement a plugin protocol for backends at which point development of these backends is likely to continue in the external plugins only, which may require configuration changes to switch to the plugin equivalents.\n\nThe cos, oss, pg, gcs, and etcdv3 backends are maintained by outside contributors and are not subject to these compatibility promises.\n\nUnmaintained State Storage Backends\n\nThe artifactory, etcdv2, manta, and swift state storage backends do not currently have any maintainers and thus remain in Terraform CLI releases on a best-effort basis. They may be removed in later v1.x releases, and will not be updated in case of any breaking changes to the services they integrate with.\n\nSupported Platforms\n\nThroughout the v1.x series we will continue to produce official releases for the following platforms, and make changes as necessary to support new releases of these operating systems:\n\nmacOS on x64 CPUs (darwin_amd64)\nWindows on x64 CPUs (windows_amd64)\nLinux on x64, 32-bit ARMv6, and 64-bit ARMv8 (linux_amd64, linux_arm, and linux_arm64 respectively)\n\nOver time we may require newer versions of these operating systems. For example, subsequent Terraform releases in the v1.x series might end support for earlier versions of macOS or Windows, or earlier Linux kernel releases.\n\nWe have historically produced official releases for a number of other platforms as a convenience to users of those platforms, and we have no current plans to stop publishing them but we cannot promise ongoing releases or bug fixes for the other platforms throughout the v1.x series. We do not routinely test Terraform on any platforms other than those listed above.\n\nWe might add support for new platforms in later v1.x releases. If so, earlier Terraform releases prior to that support will not be available on those platforms.\n\nAll Terraform plugins, including provider plugins, are separate programs that have their own policies for which platforms they support. We cannot guarantee that all providers currently support or will continue to support the platforms listed above, even though Terraform CLI itself will support them.\n\nLater Revisions to These Promises\n\nWe may extend or refine these promises throughout the v1.x series in order to describe promises related to new features or to clarify existing promises if we find via feedback that our earlier statements had been unclear.\n\nPromises for new features will be additive in the sense that they will add further promises without retracting any existing ones. For promises that only apply to later v1.x releases we will mention the earliest version(s) those promises apply to.\n\nEven if we don't add an explicit statement to this document, we intend that any non-experimental features added in later v1.x releases will remain compatible at least through the remainder of the v1.x series, unless otherwise stated.\n\nAppendices\nProtected Workflow Commands\n\nThe following is the list of Terraform CLI subcommands and options that are subject to these compatibility promises. If you build automation around these commands then it should be compatible with all later v1.x releases.\n\nAs noted above, compatibility with external software is limited to explicitly-machine-readable output (-json and -raw modes) and exit codes. Any natural-language output from these commands might change in later releases.\n\ninit\n-backend=false\n-backend-config=FILE\n-backend-config=\"KEY=VALUE\"\n-force-copy\n-get=false\n-input=false\n-migrate-state\n-no-color\n-plugin-dir=DIR\n-reconfigure\n-upgrade\nvalidate\n-json\n-no-color\nplan\n-compact-warnings\n-destroy\n-detailed-exitcode\n-lock=false\n-lock-timeout=DURATION\n-input=false\n-json\n-no-color\n-out=FILE\n-parallelism=N\n-refresh=false\n-refresh-only\n-replace=ADDRESS\n-target=ADDRESS\n-var 'NAME=VALUE'\n-var-file=FILE\napply\n-auto-approve\n-compact-warnings\n-lock=false\n-lock-timeout=DURATION\n-input=false\n-json\n-no-color\n-parallelism=N\n-refresh=false\n-refresh-only\n-replace=ADDRESS\n-target=ADDRESS\n-var 'NAME=VALUE'\n-var-file=FILE\nshow\n-no-color\n-json\n(both with and without a plan file)\nproviders (with no subcommand)\nproviders lock\n-fs-mirror=PATH\n-net-mirror=URL\n-platform=OS_ARCH\nproviders mirror\n-platform=OS_ARCH\nproviders schema\n-json\nfmt\n-list=false\n-write=false\n-diff\n-recursive\n-check\nversion\n-json\noutput\n-no-color\n-json\n-raw\ntaint\n-allow-missing\n-lock=false\n-lock-timeout=DURATION\n-ignore-remote-version\nuntaint\n-allow-missing\n-lock=false\n-lock-timeout=DURATION\n-ignore-remote-version\nforce-unlock\n-force\nstate list\n-id=ID\nstate pull\nstate push\n-force\n-lock=false\n-lock-timeout=DURATION\nstate show\n-ignore-remote-version\nlogin\n\nFor commands or options not in the above list, we will still avoid breaking changes where possible, but can't promise full compatibility throughout the v1.x series. If you are building automation around Terraform, use only the commands above to avoid the need for changes when upgrading.\n\nPlease note that although Terraform's internal logs (via the TF_LOG environment variable) are available in a JSON format, the particular syntax or structure of those log lines is not a supported integration interface. The logs are available as JSON only to help with ad-hoc filtering and processing of logs by Terraform developers.\n\nCommands That Might Change\n\nAll of the following commands and their subcommands/options are not subject to compatibility promises, either because we have existing plans to improve them during the v1.x series or because we are aware of shortcomings in their design that might require breaking changes for ongoing maintenence.\n\nWhile you can freely use these commands when running Terraform interactively as long as they remain supported, we don't recommend using them as part of any automation unless you are willing to potentially update that automation when upgrading to a later v1.x release.\n\ndestroy (consider terraform apply -destroy instead)\nconsole\nget (consider terraform init instead)\ngraph\nimport\npush\nrefresh (consider terraform apply -refresh-only instead)\nstate mv\nstate replace-provider\nstate rm\nall subcommands of workspace (and its deprecated alias env)\n\nWhile we do intend to retain support for the main use-cases associated with these commands in future releases, we cannot promise to retain the exact command names or options used to meet those use-cases.\n\nHow We Will Keep These Promises\nAutomated Regression Testing\n\nThe Terraform codebase includes various unit and integration tests intended to help us to notice accidental behavior regressions before they ship in a stable version.\n\nHowever, Terraform is a relatively complex system with many different features that can interact in interesting ways. In the past we've seen reports of behavior differences that appeared only when combining two or more features in a way we hadn't previously anticipated or written automated tests for.\n\nIn each case we have both implemented a change to resolve the compatibility problem and added one or more integration tests representing the behavior of that combination of features. We intend to continue this approach, so we can improve Terraform's test coverage over time.\n\nPrerelease Versions\n\nWe intend that most accidental changes in behavior covered by these promises will be caught by existing tests. However, we also accept that our test suite can never have perfect coverage of all possible feature interactions or other edge cases, and so we aim for each significant change to be included in both alpha and beta releases before eventual inclusion in a final release.\n\nFor minor releases we will typically also issue at least one release candidate prior to the final release. A release candidate represents that planned development is concluded and that we've fixed any regressions reported based on the alpha and beta releases, and thus the final release that follows should typically match exactly or almost exactly its most recent release candidate.\n\nRegressions in Final Releases\n\nFor more obscure combinations of features it is possible that a regression could be undetected during prerelease the prerelease periods and thus included in a final release.\n\nIf someone finds and reports such a regression soon after its release then we will treat it as a bug and fix it to restore the previous behavior in future releases, unless there is a very significant justification such as a security advisory. In these cases, we'll typically recommend anyone affected by the regression remain on the previous version until the problem is fixed and then skip forward directly to the new release containing that fix.\n\nYou can minimize the risk of being affected by missed regressions in final releases by proactively testing modules against alpha, beta, and release candidate packages. We recommend doing so only in isolated development or staging environments rather than against your production infrastructure. If you find a change in behavior in a prerelease build that seems contrary to the promises in this document, please open an issue in Terraform's GitHub repository to discuss it.\n\nLate-reported Regressions\n\nIn the most extreme case, there may be a regression with a combination of features that is so rare that it remains undetected for a long time.\n\nAfter a change has been included in more releases it becomes increasingly likely that other users will have depended on the newer behavior and thus we will need to make a tradeoff to decide whether restoring the behavior would have a greater negative impact than retaining the new behavior. We will always make this decision with due consideration to the implications of each unique situation.\n\nYou can minimize the risk of your modules being affected by late-reported regressions by upgrading promptly to new minor and patch releases of Terraform and reporting any compatibility problems you encounter in Terraform's GitHub repository.\n\nPragmatic Exceptions\n\nWe are making the promises above in good faith, with the intent that your investment in writing Terraform modules or automation will not be invalidated by future changes to Terraform. However, broad promises like the above can't possibly cover all nuances of practical problems that might arise as we continue to develop Terraform.\n\nFor that reason, there are some situations where we may still need to make changes that may impact existing modules or automation:\n\nSecurity problems: We may become aware of a design problem that has an important security impact. Depending on our determination of the associated risk, we may choose to break compatibility to achieve a more secure system.\nExternal Dependencies: Terraform's behavior depends on interfaces provided by external codebases, including your chosen operating system and including some remote network services for situations such as module and provider installation. These external systems can change outside of our control, including potentially removing or changing features that Terraform's own features depend on. In that case, if there is no suitable replacement mechanism then we may need to change Terraform's design to work within the new constraints.\nOpt-in Compatibility Breaks: The design of a language new feature may require changing the behavior or configuration representation of an existing feature. If so, we will typically make the new feature opt-in only in order to avoid breaking existing modules, but if you change your module to opt in to the new feature then you may also then be required to change other parts of your configuration to work with the new language design.\nBugs in New Features: If we introduce a new feature to Terraform and the initial implementation has problems that cause it to not match the documented design intent at release, we may make a follow-up release that corrects the implementation to match the documented design, even if that represents a minor compatibility regression compared to the initial implementation. However, once a feature is well-established and in common use we will usually defer to the implemented behavior and instead change the documentation to reflect it.\nRegressions in Existing Features: If we learn that a new Terraform release includes a regression for an existing feature that wasn't detected during the development and prerelease periods, and that learning comes promptly after the new release, we will typically restore the previous behavior at the expense of technically therefore breaking compatibility with the behavior of the new release, under the assumption that more users will have systems affected by the regression than will have systems depending on the newly-introduced behavior.\nLate-reported regressions: As described in the previous section, if we learn that there was an unintentional regression of a rarely-used feature or combination of features in a much earlier release then restoring the previous behavior may appear as a regression to later adopters. If we believe that fixing the regression would affect more users than the regression itself affects then we may choose to accept the regression as the new promised behavior.\nSituations we cannot anticipate: Although we've made an effort to consider various specific exceptional situations here, Terraform and its development process are not isolated from broader context, and so we must consider that there may be situations that we cannot possibly anticipate that would affect the future of Terraform. In those situations, we will always do our best to find a course of action that will minimize as much as possible the impact to existing modules and automation.\n\nOur intent with these pragmatic exceptions is only to acknowledge that there will always be situations that general compatibility promises cannot address. We will use these exceptions only with due consideration and as a last resort.\n\nOn this page:\n\nTerraform v1.x Compatibility Promises\nThe Terraform Language\nWorkflow\nUpgrading and Downgrading\nProviders\nExternal Modules\nProvisioners\nState Storage Backends\nCommunity-maintained State Storage Backends\nSupported Platforms\nLater Revisions to These Promises\nAppendices\nHow We Will Keep These Promises\nTheme\nDark\nLight\nSystem\nCertifications\nSystem Status\nCookie Manager\nTerms of Use\nSecurity\nPrivacy\nTrademark Policy\nTrade Controls\nGive Feedback\n(opens in new tab)\nWe use cookies & other similar technology to collect data to improve your experience on our site, as described in our Privacy Policy and Cookie Policy.\nManage PreferencesACCEPT"
  },
  {
    "title": "Terraform v1.x Compatibility Promises | Terraform | HashiCorp Developer",
    "url": "https://developer.hashicorp.com/terraform/language/v1.9.x/v1-compatibility-promises",
    "html": "Skip to main content\nNews\nHashiCorp to join IBM to accelerate multi-cloud automation.\nRead the blog →\nDismiss alert\nTerraform\nInstall\nTutorials\nDocumentation\nRegistry\n(opens in new tab)\nTry Cloud\n(opens in new tab)\nSearch\nCommand or control key\nK key\nTerraform Home\nConfiguration Language\nConfiguration Language\nTerraform Language\nOverview\nStyle Guide\nFiles and Directories\nSyntax\nResources\nData Sources\nProviders\nVariables and Outputs\nModules\nChecks\nImport\nExpressions\nFunctions\nTerraform Settings\nState\nTests\nUpgrading to Terraform v1.7\nv1.x Compatibility Promises\nTerraform Internals\nResources\nTutorial Library\nCertifications\nCommunity Forum\n(opens in new tab)\nSupport\n(opens in new tab)\nGitHub\n(opens in new tab)\nTerraform Registry\n(opens in new tab)\n\nYou are viewing documentation for pre-release version v1.9.0 (alpha). View latest version.\n\nDeveloper\nTerraform\nConfiguration Language v1.9.x\nv1.x Compatibility Promises\nv1.9.0 (alpha)\nTerraform v1.x Compatibility Promises\n\nThe release of Terraform v1.0 represents an important milestone in the development of the Terraform language and workflow. Terraform v1.0 is a stable platform for describing and managing infrastructure.\n\nIn this release we're defining a number of Terraform behaviors that we intend to remain compatible with throughout the 1.x releases:\n\nA large subset of Terraform language features.\nA more conservative subset of the Terraform CLI workflow commands.\nThe wire protocol for communication between Terraform Core and Terraform providers.\nThe wire protocols for installation of Terraform providers and external Terraform modules.\n\nOur intention is that Terraform modules written for Terraform v1.0 will continue to plan and apply successfully, without required changes, throughout the v1.x releases.\n\nWe also intend that automation built around the workflow subset described in this document will work without changes in all future v1.x releases.\n\nFinally, we intend that providers built against the currently-documented provider wire protocol will be compatible with all future Terraform v1.x releases targeting the same operating system and architecture, without the need for source code modification or binary recompilation.\n\nIn short, we aim to make upgrades between v1.x releases straightforward, requiring no changes to your configuration, no extra commands to run upgrade steps, and no changes to any automation you've set up around Terraform.\n\nThe Terraform v1.x series will be actively maintained for at least 18 months after v1.0.\n\nThe following sections include some specific guidance on what we will promise throughout the v1.x series, for those who would like full details. At a higher level though, we don't intend to make any changes that would cause existing modules or automation to require changes when upgrading to a new v1.x release. We will generally treat compatibility problems in new Terraform CLI releases as bugs to be fixed unless there was a very significant justification for the change, such as in addressing a critical security problem or matching with a breaking change to a remote dependency that isn't directly under our control.\n\nThe Terraform Language\n\nThe main Terraform Language includes the language syntax, the top-level structures such as resource, module, and provider blocks, the \"meta-arguments\" in those blocks, and the documented semantics and behaviors for the operators and built-in functions available for use in expressions.\n\nThere is not a single formal specification for the Terraform language, but the Configuration section of the documentation on the Terraform website serves as a description of the language features and their intended behaviors.\n\nThe following top-level blocks and their defined \"meta-arguments\" (that is, arguments defined by Terraform Core rather than by external plugins such as providers) will retain their current functionality:\n\nresource and data blocks to declare resources, including their nested block types lifecycle, connection, and provisioner, and their meta-argument provider.\nmodule blocks to call other modules, and its meta-argument providers.\nThe count, for_each, and depends_on meta-arguments in resource, data, and module blocks.\nprovider blocks to configure providers, and the alias meta-argument.\nvariable, output, and locals blocks for declaring the various kinds of named values in a module.\nterraform blocks, including the nested required_version and required_providers arguments, and nested backend blocks for backend configuration.\n\nWe also intend to keep compatibility with all expression operators and built-in functions, with the exception of references to terraform.workspace, whose behavior may change as part of future changes to the workspace model.\n\nWe intend to retain broad compatibility with Terraform language features, with a few specific caveats:\n\nWe consider a configuration to be valid if Terraform can create and apply a plan for it without reporting any errors.\n\nA configuration that currently produces errors might generate different errors or exhibit other non-error behaviors in a future version of Terraform. A configuration that generates errors during the apply phase might generate similar errors at an earlier phase in future, because we generally consider it better to detect errors in as early a phase as possible.\n\nGenerally-speaking, the compatibility promises described in this document apply only to valid configurations. Handling of invalid configurations is always subject to change in future Terraform releases.\n\nIf the actual behavior of a feature differs from what we explicitly documented as the feature's behavior, we will usually treat that as a bug and change the feature to match the documentation, although we will avoid making such changes if they seem likely to cause broad compatibility problems. We cannot promise to always remain \"bug-compatible\" with previous releases, but we will consider such fixes carefully to minimize their impact.\n\nAny experimental features may change or may be removed entirely from future releases. Terraform always produces a warning when an experimental language feature is active, to make you aware of that risk. We don't recommend using experimental features in production modules.\n\nWe will introduce new language features, and if you start using them then your configuration won't work with prior Terraform versions that didn't support those features yet.\n\nTerraform Providers are separate plugins which can change independently of Terraform Core and are therefore not subject to these compatibility promises. If you upgrade any of the providers you are using then you might need to change provider or resource configurations related to those providers.\n\nA small number of features remain deprecated with explicit warnings in Terraform v1.0. Those deprecation cycles will end in a future v1.x release, at which point we will remove the corresponding features.\n\nWorkflow\n\nThere is a set of often used Terraform workflows, which we are calling protected workflows. We will not remove these commands, subcommands, and flags or make backward-incompatible changes to protected workflow functionality. If we accidentally change these, we will consider backwards-incompatible changes to core workflows as bugs to be fixed. For a list of the command and option combinations that are part of protected workflows, see Protected Workflow Commands. There is another set of commands that we are explicitly not making compatibility promises about, because we expect their functionality to change in v1.x releases: see Commands That Might Change.\n\nThe supported ways for external software to interact with Terraform are via the JSON output modes offered by some commands and via exit status codes. We may extend certain JSON formats with new object properties but we will not remove or make breaking changes to the definitions of existing properties.\n\nNatural language command output or log output is not a stable interface and may change in any new version. If you write software that parses this output then it may need to be updated when you upgrade Terraform. If you need access to data that is not currently available via one of the machine-readable JSON interfaces, we suggest opening a feature request to discuss your use-case.\n\nUpgrading and Downgrading\n\nThroughout the v1.x series of releases, we intend that you should be able to switch to a newer Terraform version and use it just as before, without any special upgrade steps.\n\nYou should be able to upgrade from any v1.x release to any later v1.x release. You might also be able to downgrade to an earlier v1.x release, but that isn't guaranteed: later releases may introduce new features that earlier versions cannot understand, including new storage formats for Terraform state snapshots.\n\nIf you make use of features introduced in a later v1.x release, your configuration won't be compatible with releases that predate that feature. For example, if a language feature is added in v1.3 and you start using it, your Terraform configuration will no longer be compatible with Terraform v1.2.\n\nProviders\n\nTerraform providers are separate plugins which communicate with Terraform using a documented protocol. Therefore these compatibility promises can only cover the \"client\" side of this protocol as implemented by Terraform Core; the behaviors of individual providers, including which resource types they support and which arguments they expect, are decided by the provider development teams and can change independently of Terraform Core releases.\n\nIf you upgrade to a new version of a provider then you might need to change the parts of your configuration which are interpreted by that provider, even if you are still using a Terraform v1.x release.\n\nProvider Installation Methods\n\nTerraform normally installs providers from a provider registry implementing the Provider Registry Protocol, version 1. All Terraform v1.x releases will remain compatible with that protocol, and so correctly-implemented provider registries will stay compatible.\n\nTerraform also supports installation of providers from local filesystem directories (filesystem mirrors) and from network mirrors (implementing the Provider Mirror Protocol. All Terraform v1.x releases will remain compatible with those installation methods, including the Implied Local Mirror Directories.\n\nSpecific provider registries or network mirrors are run independently from Terraform itself and so their own behaviors are not subject to these compatibility promises.\n\nProvider Protocol Versions\n\nThe current major version of the provider plugin protocol as of Terraform v1.0 is version 5, which is defined by a combination of a Protocol Buffers schema describing the physical wire formats and by additional prose documentation describing the expected provider behaviors.\n\nWe will support protocol version 5 throughout the Terraform v1.x releases. If we make new minor revisions to protocol version 5 in later releases then we will design them such that existing plugins will continue to work, as long as they correctly implemented the protocol.\n\nWe may introduce new major versions of the protocol during the v1.x series. If so, we will continue to support protocol version 5 alongside those new versions. Individual provider teams might decide to remove support for protocol version 5 in later releases, in which case those new provider releases will not be compatible with all of the Terraform v1.x releases.\n\nExternal Modules\n\nTerraform modules are reusable infrastructure components written in the Terraform language. Some modules are \"external\" in the sense that Terraform automatically installs them from a location other than the current configuration directory, in which case their contents could change independently of changes to your local modules, of the providers you use, and of Terraform itself.\n\nModule Installation Methods\n\nTerraform supports installing child modules from a number of different module source types. We will continue to support all of the existing source types throughout the v1.x releases.\n\nOne of the supported source types is a module registry implementing the Module Registry Protocol version 1. All Terraform v1.x releases will remain compatible with correct implementations of that protocol.\n\nSome module source types work directly with services or protocols defined and run by third parties. Although we will not remove Terraform's own client-side support for those, we cannot guarantee that their owners will keep those services running or that they will remain compatible with Terraform's client implementations.\n\nExternal Module Compatibility\n\nIf your configuration depends on external modules, newer versions of those modules may include breaking changes. External modules are not part of Terraform and are therefore not subject to these compatibility promises.\n\nProvisioners\n\nWe will maintain compatibility for the file, local-exec, and remote-exec provisioner types through all v1.x releases.\n\nSome additional vendor-specific provisioners were available in earlier Terraform versions but were deprecated in Terraform v0.13 and removed in Terraform v0.15.\n\nTerraform supports loading additional provisioners as plugins from certain local filesystem directories. We'll continue to support that throughout the Terraform v1.x releases, but since such plugins are separate from Terraform Core itself their own behaviors cannot be subject to these compatibility promises. However, we will continue to support the plugin wire protocol as defined in Terraform v1.0 throughout the v1.x releases, and so correctly-implemented provisioner plugins should remain compatible with future Terraform releases.\n\nState Storage Backends\n\nWhen you use remote state, Terraform interacts with remote services over the network in order to store and manage locks for Terraform state.\n\nFor historical reasons, all supported state storage backends are included as part of Terraform CLI but not all are supported directly by the Terraform Team. Only the following backends maintained by the Terraform team are subject to compatibility promises:\n\nlocal (the default, when you are not using remote state)\nhttp\n\nThe other state storage backends are maintained by external teams via contributions to the Terraform CLI codebase, and so their expected configuration arguments or behaviors might change even in v1.x releases, although we will aim to still ensure a good migration path in such cases, where possible.\n\nWe are considering allowing external state storage backend implementations via plugins, similar to provider plugins. If we introduce such a mechanism during the v1.x releases then you may need to make configuration changes in order to use those plugins, and state storage backends other than those listed above may be removed from later versions of Terraform CLI once equivalent plugins are available.\n\nThe remote Backend and HCP Terraform\n\nThe remote backend is maintained by the HCP Terraform team and so its behavior may change along with ongoing changes to HCP Terraform.\n\nThere will be a supported mechanism to use Terraform CLI with HCP Terraform throughout the v1.x releases, but the exact details may change. HCP Terraform evolves independently of Terraform CLI and is therefore not subject to these compatibility promises.\n\nCommunity-maintained State Storage Backends\n\nThe azurerm, consul, s3, and kubernetes backends are maintained by other teams at HashiCorp. Those teams intend to continue basic maintenence at the level of bug fixes through the v1.x releases, unless we implement a plugin protocol for backends at which point development of these backends is likely to continue in the external plugins only, which may require configuration changes to switch to the plugin equivalents.\n\nThe cos, oss, pg, gcs, and etcdv3 backends are maintained by outside contributors and are not subject to these compatibility promises.\n\nUnmaintained State Storage Backends\n\nThe artifactory, etcdv2, manta, and swift state storage backends do not currently have any maintainers and thus remain in Terraform CLI releases on a best-effort basis. They may be removed in later v1.x releases, and will not be updated in case of any breaking changes to the services they integrate with.\n\nSupported Platforms\n\nThroughout the v1.x series we will continue to produce official releases for the following platforms, and make changes as necessary to support new releases of these operating systems:\n\nmacOS on x64 CPUs (darwin_amd64)\nWindows on x64 CPUs (windows_amd64)\nLinux on x64, 32-bit ARMv6, and 64-bit ARMv8 (linux_amd64, linux_arm, and linux_arm64 respectively)\n\nOver time we may require newer versions of these operating systems. For example, subsequent Terraform releases in the v1.x series might end support for earlier versions of macOS or Windows, or earlier Linux kernel releases.\n\nWe have historically produced official releases for a number of other platforms as a convenience to users of those platforms, and we have no current plans to stop publishing them but we cannot promise ongoing releases or bug fixes for the other platforms throughout the v1.x series. We do not routinely test Terraform on any platforms other than those listed above.\n\nWe might add support for new platforms in later v1.x releases. If so, earlier Terraform releases prior to that support will not be available on those platforms.\n\nAll Terraform plugins, including provider plugins, are separate programs that have their own policies for which platforms they support. We cannot guarantee that all providers currently support or will continue to support the platforms listed above, even though Terraform CLI itself will support them.\n\nLater Revisions to These Promises\n\nWe may extend or refine these promises throughout the v1.x series in order to describe promises related to new features or to clarify existing promises if we find via feedback that our earlier statements had been unclear.\n\nPromises for new features will be additive in the sense that they will add further promises without retracting any existing ones. For promises that only apply to later v1.x releases we will mention the earliest version(s) those promises apply to.\n\nEven if we don't add an explicit statement to this document, we intend that any non-experimental features added in later v1.x releases will remain compatible at least through the remainder of the v1.x series, unless otherwise stated.\n\nAppendices\nProtected Workflow Commands\n\nThe following is the list of Terraform CLI subcommands and options that are subject to these compatibility promises. If you build automation around these commands then it should be compatible with all later v1.x releases.\n\nAs noted above, compatibility with external software is limited to explicitly-machine-readable output (-json and -raw modes) and exit codes. Any natural-language output from these commands might change in later releases.\n\ninit\n-backend=false\n-backend-config=FILE\n-backend-config=\"KEY=VALUE\"\n-force-copy\n-get=false\n-input=false\n-migrate-state\n-no-color\n-plugin-dir=DIR\n-reconfigure\n-upgrade\nvalidate\n-json\n-no-color\nplan\n-compact-warnings\n-destroy\n-detailed-exitcode\n-lock=false\n-lock-timeout=DURATION\n-input=false\n-json\n-no-color\n-out=FILE\n-parallelism=N\n-refresh=false\n-refresh-only\n-replace=ADDRESS\n-target=ADDRESS\n-var 'NAME=VALUE'\n-var-file=FILE\napply\n-auto-approve\n-compact-warnings\n-lock=false\n-lock-timeout=DURATION\n-input=false\n-json\n-no-color\n-parallelism=N\n-refresh=false\n-refresh-only\n-replace=ADDRESS\n-target=ADDRESS\n-var 'NAME=VALUE'\n-var-file=FILE\nshow\n-no-color\n-json\n(both with and without a plan file)\nproviders (with no subcommand)\nproviders lock\n-fs-mirror=PATH\n-net-mirror=URL\n-platform=OS_ARCH\nproviders mirror\n-platform=OS_ARCH\nproviders schema\n-json\nfmt\n-list=false\n-write=false\n-diff\n-recursive\n-check\nversion\n-json\noutput\n-no-color\n-json\n-raw\ntaint\n-allow-missing\n-lock=false\n-lock-timeout=DURATION\n-ignore-remote-version\nuntaint\n-allow-missing\n-lock=false\n-lock-timeout=DURATION\n-ignore-remote-version\nforce-unlock\n-force\nstate list\n-id=ID\nstate pull\nstate push\n-force\n-lock=false\n-lock-timeout=DURATION\nstate show\n-ignore-remote-version\nlogin\n\nFor commands or options not in the above list, we will still avoid breaking changes where possible, but can't promise full compatibility throughout the v1.x series. If you are building automation around Terraform, use only the commands above to avoid the need for changes when upgrading.\n\nPlease note that although Terraform's internal logs (via the TF_LOG environment variable) are available in a JSON format, the particular syntax or structure of those log lines is not a supported integration interface. The logs are available as JSON only to help with ad-hoc filtering and processing of logs by Terraform developers.\n\nCommands That Might Change\n\nAll of the following commands and their subcommands/options are not subject to compatibility promises, either because we have existing plans to improve them during the v1.x series or because we are aware of shortcomings in their design that might require breaking changes for ongoing maintenence.\n\nWhile you can freely use these commands when running Terraform interactively as long as they remain supported, we don't recommend using them as part of any automation unless you are willing to potentially update that automation when upgrading to a later v1.x release.\n\ndestroy (consider terraform apply -destroy instead)\nconsole\nget (consider terraform init instead)\ngraph\nimport\npush\nrefresh (consider terraform apply -refresh-only instead)\nstate mv\nstate replace-provider\nstate rm\nall subcommands of workspace (and its deprecated alias env)\n\nWhile we do intend to retain support for the main use-cases associated with these commands in future releases, we cannot promise to retain the exact command names or options used to meet those use-cases.\n\nHow We Will Keep These Promises\nAutomated Regression Testing\n\nThe Terraform codebase includes various unit and integration tests intended to help us to notice accidental behavior regressions before they ship in a stable version.\n\nHowever, Terraform is a relatively complex system with many different features that can interact in interesting ways. In the past we've seen reports of behavior differences that appeared only when combining two or more features in a way we hadn't previously anticipated or written automated tests for.\n\nIn each case we have both implemented a change to resolve the compatibility problem and added one or more integration tests representing the behavior of that combination of features. We intend to continue this approach, so we can improve Terraform's test coverage over time.\n\nPrerelease Versions\n\nWe intend that most accidental changes in behavior covered by these promises will be caught by existing tests. However, we also accept that our test suite can never have perfect coverage of all possible feature interactions or other edge cases, and so we aim for each significant change to be included in both alpha and beta releases before eventual inclusion in a final release.\n\nFor minor releases we will typically also issue at least one release candidate prior to the final release. A release candidate represents that planned development is concluded and that we've fixed any regressions reported based on the alpha and beta releases, and thus the final release that follows should typically match exactly or almost exactly its most recent release candidate.\n\nRegressions in Final Releases\n\nFor more obscure combinations of features it is possible that a regression could be undetected during prerelease the prerelease periods and thus included in a final release.\n\nIf someone finds and reports such a regression soon after its release then we will treat it as a bug and fix it to restore the previous behavior in future releases, unless there is a very significant justification such as a security advisory. In these cases, we'll typically recommend anyone affected by the regression remain on the previous version until the problem is fixed and then skip forward directly to the new release containing that fix.\n\nYou can minimize the risk of being affected by missed regressions in final releases by proactively testing modules against alpha, beta, and release candidate packages. We recommend doing so only in isolated development or staging environments rather than against your production infrastructure. If you find a change in behavior in a prerelease build that seems contrary to the promises in this document, please open an issue in Terraform's GitHub repository to discuss it.\n\nLate-reported Regressions\n\nIn the most extreme case, there may be a regression with a combination of features that is so rare that it remains undetected for a long time.\n\nAfter a change has been included in more releases it becomes increasingly likely that other users will have depended on the newer behavior and thus we will need to make a tradeoff to decide whether restoring the behavior would have a greater negative impact than retaining the new behavior. We will always make this decision with due consideration to the implications of each unique situation.\n\nYou can minimize the risk of your modules being affected by late-reported regressions by upgrading promptly to new minor and patch releases of Terraform and reporting any compatibility problems you encounter in Terraform's GitHub repository.\n\nPragmatic Exceptions\n\nWe are making the promises above in good faith, with the intent that your investment in writing Terraform modules or automation will not be invalidated by future changes to Terraform. However, broad promises like the above can't possibly cover all nuances of practical problems that might arise as we continue to develop Terraform.\n\nFor that reason, there are some situations where we may still need to make changes that may impact existing modules or automation:\n\nSecurity problems: We may become aware of a design problem that has an important security impact. Depending on our determination of the associated risk, we may choose to break compatibility to achieve a more secure system.\nExternal Dependencies: Terraform's behavior depends on interfaces provided by external codebases, including your chosen operating system and including some remote network services for situations such as module and provider installation. These external systems can change outside of our control, including potentially removing or changing features that Terraform's own features depend on. In that case, if there is no suitable replacement mechanism then we may need to change Terraform's design to work within the new constraints.\nOpt-in Compatibility Breaks: The design of a language new feature may require changing the behavior or configuration representation of an existing feature. If so, we will typically make the new feature opt-in only in order to avoid breaking existing modules, but if you change your module to opt in to the new feature then you may also then be required to change other parts of your configuration to work with the new language design.\nBugs in New Features: If we introduce a new feature to Terraform and the initial implementation has problems that cause it to not match the documented design intent at release, we may make a follow-up release that corrects the implementation to match the documented design, even if that represents a minor compatibility regression compared to the initial implementation. However, once a feature is well-established and in common use we will usually defer to the implemented behavior and instead change the documentation to reflect it.\nRegressions in Existing Features: If we learn that a new Terraform release includes a regression for an existing feature that wasn't detected during the development and prerelease periods, and that learning comes promptly after the new release, we will typically restore the previous behavior at the expense of technically therefore breaking compatibility with the behavior of the new release, under the assumption that more users will have systems affected by the regression than will have systems depending on the newly-introduced behavior.\nLate-reported regressions: As described in the previous section, if we learn that there was an unintentional regression of a rarely-used feature or combination of features in a much earlier release then restoring the previous behavior may appear as a regression to later adopters. If we believe that fixing the regression would affect more users than the regression itself affects then we may choose to accept the regression as the new promised behavior.\nSituations we cannot anticipate: Although we've made an effort to consider various specific exceptional situations here, Terraform and its development process are not isolated from broader context, and so we must consider that there may be situations that we cannot possibly anticipate that would affect the future of Terraform. In those situations, we will always do our best to find a course of action that will minimize as much as possible the impact to existing modules and automation.\n\nOur intent with these pragmatic exceptions is only to acknowledge that there will always be situations that general compatibility promises cannot address. We will use these exceptions only with due consideration and as a last resort.\n\nOn this page:\n\nTerraform v1.x Compatibility Promises\nThe Terraform Language\nWorkflow\nUpgrading and Downgrading\nProviders\nExternal Modules\nProvisioners\nState Storage Backends\nCommunity-maintained State Storage Backends\nSupported Platforms\nLater Revisions to These Promises\nAppendices\nHow We Will Keep These Promises\nTheme\nDark\nLight\nSystem\nCertifications\nSystem Status\nCookie Manager\nTerms of Use\nSecurity\nPrivacy\nTrademark Policy\nTrade Controls\nGive Feedback\n(opens in new tab)\nWe use cookies & other similar technology to collect data to improve your experience on our site, as described in our Privacy Policy and Cookie Policy.\nManage PreferencesACCEPT"
  },
  {
    "title": "Terraform v1.x Compatibility Promises | Terraform | HashiCorp Developer",
    "url": "https://developer.hashicorp.com/terraform/language/v1.6.x/v1-compatibility-promises",
    "html": "Skip to main content\nNews\nHashiCorp to join IBM to accelerate multi-cloud automation.\nRead the blog →\nDismiss alert\nTerraform\nInstall\nTutorials\nDocumentation\nRegistry\n(opens in new tab)\nTry Cloud\n(opens in new tab)\nSearch\nCommand or control key\nK key\nTerraform Home\nConfiguration Language\nConfiguration Language\nTerraform Language\nOverview\nStyle Guide\nFiles and Directories\nSyntax\nResources\nData Sources\nProviders\nVariables and Outputs\nModules\nMoved block\nChecks\nImport\nExpressions\nFunctions\nTerraform Settings\nState\nTests\nUpgrading to Terraform v1.6\nv1.x Compatibility Promises\nTerraform Internals\nResources\nTutorial Library\nCertifications\nCommunity Forum\n(opens in new tab)\nSupport\n(opens in new tab)\nGitHub\n(opens in new tab)\nTerraform Registry\n(opens in new tab)\n\nYou are viewing documentation for version v1.6.x. View latest version.\n\nDeveloper\nTerraform\nConfiguration Language v1.6.x\nv1.x Compatibility Promises\nv1.6.x\nTerraform v1.x Compatibility Promises\n\nThe release of Terraform v1.0 represents an important milestone in the development of the Terraform language and workflow. Terraform v1.0 is a stable platform for describing and managing infrastructure.\n\nIn this release we're defining a number of Terraform behaviors that we intend to remain compatible with throughout the 1.x releases:\n\nA large subset of Terraform language features.\nA more conservative subset of the Terraform CLI workflow commands.\nThe wire protocol for communication between Terraform Core and Terraform providers.\nThe wire protocols for installation of Terraform providers and external Terraform modules.\n\nOur intention is that Terraform modules written for Terraform v1.0 will continue to plan and apply successfully, without required changes, throughout the v1.x releases.\n\nWe also intend that automation built around the workflow subset described in this document will work without changes in all future v1.x releases.\n\nFinally, we intend that providers built against the currently-documented provider wire protocol will be compatible with all future Terraform v1.x releases targeting the same operating system and architecture, without the need for source code modification or binary recompilation.\n\nIn short, we aim to make upgrades between v1.x releases straightforward, requiring no changes to your configuration, no extra commands to run upgrade steps, and no changes to any automation you've set up around Terraform.\n\nThe Terraform v1.x series will be actively maintained for at least 18 months after v1.0.\n\nThe following sections include some specific guidance on what we will promise throughout the v1.x series, for those who would like full details. At a higher level though, we don't intend to make any changes that would cause existing modules or automation to require changes when upgrading to a new v1.x release. We will generally treat compatibility problems in new Terraform CLI releases as bugs to be fixed unless there was a very significant justification for the change, such as in addressing a critical security problem or matching with a breaking change to a remote dependency that isn't directly under our control.\n\nThe Terraform Language\n\nThe main Terraform Language includes the language syntax, the top-level structures such as resource, module, and provider blocks, the \"meta-arguments\" in those blocks, and the documented semantics and behaviors for the operators and built-in functions available for use in expressions.\n\nThere is not a single formal specification for the Terraform language, but the Configuration section of the documentation on the Terraform website serves as a description of the language features and their intended behaviors.\n\nThe following top-level blocks and their defined \"meta-arguments\" (that is, arguments defined by Terraform Core rather than by external plugins such as providers) will retain their current functionality:\n\nresource and data blocks to declare resources, including their nested block types lifecycle, connection, and provisioner, and their meta-argument provider.\nmodule blocks to call other modules, and its meta-argument providers.\nThe count, for_each, and depends_on meta-arguments in resource, data, and module blocks.\nprovider blocks to configure providers, and the alias meta-argument.\nvariable, output, and locals blocks for declaring the various kinds of named values in a module.\nterraform blocks, including the nested required_version and required_providers arguments, and nested backend blocks for backend configuration.\n\nWe also intend to keep compatibility with all expression operators and built-in functions, with the exception of references to terraform.workspace, whose behavior may change as part of future changes to the workspace model.\n\nWe intend to retain broad compatibility with Terraform language features, with a few specific caveats:\n\nWe consider a configuration to be valid if Terraform can create and apply a plan for it without reporting any errors.\n\nA configuration that currently produces errors might generate different errors or exhibit other non-error behaviors in a future version of Terraform. A configuration that generates errors during the apply phase might generate similar errors at an earlier phase in future, because we generally consider it better to detect errors in as early a phase as possible.\n\nGenerally-speaking, the compatibility promises described in this document apply only to valid configurations. Handling of invalid configurations is always subject to change in future Terraform releases.\n\nIf the actual behavior of a feature differs from what we explicitly documented as the feature's behavior, we will usually treat that as a bug and change the feature to match the documentation, although we will avoid making such changes if they seem likely to cause broad compatibility problems. We cannot promise to always remain \"bug-compatible\" with previous releases, but we will consider such fixes carefully to minimize their impact.\n\nAny experimental features may change or may be removed entirely from future releases. Terraform always produces a warning when an experimental language feature is active, to make you aware of that risk. We don't recommend using experimental features in production modules.\n\nWe will introduce new language features, and if you start using them then your configuration won't work with prior Terraform versions that didn't support those features yet.\n\nTerraform Providers are separate plugins which can change independently of Terraform Core and are therefore not subject to these compatibility promises. If you upgrade any of the providers you are using then you might need to change provider or resource configurations related to those providers.\n\nA small number of features remain deprecated with explicit warnings in Terraform v1.0. Those deprecation cycles will end in a future v1.x release, at which point we will remove the corresponding features.\n\nWorkflow\n\nThere is a set of often used Terraform workflows, which we are calling protected workflows. We will not remove these commands, subcommands, and flags or make backward-incompatible changes to protected workflow functionality. If we accidentally change these, we will consider backwards-incompatible changes to core workflows as bugs to be fixed. For a list of the command and option combinations that are part of protected workflows, see Protected Workflow Commands. There is another set of commands that we are explicitly not making compatibility promises about, because we expect their functionality to change in v1.x releases: see Commands That Might Change.\n\nThe supported ways for external software to interact with Terraform are via the JSON output modes offered by some commands and via exit status codes. We may extend certain JSON formats with new object properties but we will not remove or make breaking changes to the definitions of existing properties.\n\nNatural language command output or log output is not a stable interface and may change in any new version. If you write software that parses this output then it may need to be updated when you upgrade Terraform. If you need access to data that is not currently available via one of the machine-readable JSON interfaces, we suggest opening a feature request to discuss your use-case.\n\nUpgrading and Downgrading\n\nThroughout the v1.x series of releases, we intend that you should be able to switch to a newer Terraform version and use it just as before, without any special upgrade steps.\n\nYou should be able to upgrade from any v1.x release to any later v1.x release. You might also be able to downgrade to an earlier v1.x release, but that isn't guaranteed: later releases may introduce new features that earlier versions cannot understand, including new storage formats for Terraform state snapshots.\n\nIf you make use of features introduced in a later v1.x release, your configuration won't be compatible with releases that predate that feature. For example, if a language feature is added in v1.3 and you start using it, your Terraform configuration will no longer be compatible with Terraform v1.2.\n\nProviders\n\nTerraform providers are separate plugins which communicate with Terraform using a documented protocol. Therefore these compatibility promises can only cover the \"client\" side of this protocol as implemented by Terraform Core; the behaviors of individual providers, including which resource types they support and which arguments they expect, are decided by the provider development teams and can change independently of Terraform Core releases.\n\nIf you upgrade to a new version of a provider then you might need to change the parts of your configuration which are interpreted by that provider, even if you are still using a Terraform v1.x release.\n\nProvider Installation Methods\n\nTerraform normally installs providers from a provider registry implementing the Provider Registry Protocol, version 1. All Terraform v1.x releases will remain compatible with that protocol, and so correctly-implemented provider registries will stay compatible.\n\nTerraform also supports installation of providers from local filesystem directories (filesystem mirrors) and from network mirrors (implementing the Provider Mirror Protocol. All Terraform v1.x releases will remain compatible with those installation methods, including the Implied Local Mirror Directories.\n\nSpecific provider registries or network mirrors are run independently from Terraform itself and so their own behaviors are not subject to these compatibility promises.\n\nProvider Protocol Versions\n\nThe current major version of the provider plugin protocol as of Terraform v1.0 is version 5, which is defined by a combination of a Protocol Buffers schema describing the physical wire formats and by additional prose documentation describing the expected provider behaviors.\n\nWe will support protocol version 5 throughout the Terraform v1.x releases. If we make new minor revisions to protocol version 5 in later releases then we will design them such that existing plugins will continue to work, as long as they correctly implemented the protocol.\n\nWe may introduce new major versions of the protocol during the v1.x series. If so, we will continue to support protocol version 5 alongside those new versions. Individual provider teams might decide to remove support for protocol version 5 in later releases, in which case those new provider releases will not be compatible with all of the Terraform v1.x releases.\n\nExternal Modules\n\nTerraform modules are reusable infrastructure components written in the Terraform language. Some modules are \"external\" in the sense that Terraform automatically installs them from a location other than the current configuration directory, in which case their contents could change independently of changes to your local modules, of the providers you use, and of Terraform itself.\n\nModule Installation Methods\n\nTerraform supports installing child modules from a number of different module source types. We will continue to support all of the existing source types throughout the v1.x releases.\n\nOne of the supported source types is a module registry implementing the Module Registry Protocol version 1. All Terraform v1.x releases will remain compatible with correct implementations of that protocol.\n\nSome module source types work directly with services or protocols defined and run by third parties. Although we will not remove Terraform's own client-side support for those, we cannot guarantee that their owners will keep those services running or that they will remain compatible with Terraform's client implementations.\n\nExternal Module Compatibility\n\nIf your configuration depends on external modules, newer versions of those modules may include breaking changes. External modules are not part of Terraform and are therefore not subject to these compatibility promises.\n\nProvisioners\n\nWe will maintain compatibility for the file, local-exec, and remote-exec provisioner types through all v1.x releases.\n\nSome additional vendor-specific provisioners were available in earlier Terraform versions but were deprecated in Terraform v0.13 and removed in Terraform v0.15.\n\nTerraform supports loading additional provisioners as plugins from certain local filesystem directories. We'll continue to support that throughout the Terraform v1.x releases, but since such plugins are separate from Terraform Core itself their own behaviors cannot be subject to these compatibility promises. However, we will continue to support the plugin wire protocol as defined in Terraform v1.0 throughout the v1.x releases, and so correctly-implemented provisioner plugins should remain compatible with future Terraform releases.\n\nState Storage Backends\n\nWhen you use remote state, Terraform interacts with remote services over the network in order to store and manage locks for Terraform state.\n\nFor historical reasons, all supported state storage backends are included as part of Terraform CLI but not all are supported directly by the Terraform Team. Only the following backends maintained by the Terraform team are subject to compatibility promises:\n\nlocal (the default, when you are not using remote state)\nhttp\n\nThe other state storage backends are maintained by external teams via contributions to the Terraform CLI codebase, and so their expected configuration arguments or behaviors might change even in v1.x releases, although we will aim to still ensure a good migration path in such cases, where possible.\n\nWe are considering allowing external state storage backend implementations via plugins, similar to provider plugins. If we introduce such a mechanism during the v1.x releases then you may need to make configuration changes in order to use those plugins, and state storage backends other than those listed above may be removed from later versions of Terraform CLI once equivalent plugins are available.\n\nThe remote Backend and Terraform Cloud\n\nThe remote backend is maintained by the Terraform Cloud team and so its behavior may change along with ongoing changes to Terraform Cloud.\n\nThere will be a supported mechanism to use Terraform CLI with Terraform Cloud throughout the v1.x releases, but the exact details may change. Terraform Cloud evolves independently of Terraform CLI and is therefore not subject to these compatibility promises.\n\nCommunity-maintained State Storage Backends\n\nThe azurerm, consul, s3, and kubernetes backends are maintained by other teams at HashiCorp. Those teams intend to continue basic maintenence at the level of bug fixes through the v1.x releases, unless we implement a plugin protocol for backends at which point development of these backends is likely to continue in the external plugins only, which may require configuration changes to switch to the plugin equivalents.\n\nThe cos, oss, pg, gcs, and etcdv3 backends are maintained by outside contributors and are not subject to these compatibility promises.\n\nUnmaintained State Storage Backends\n\nThe artifactory, etcdv2, manta, and swift state storage backends do not currently have any maintainers and thus remain in Terraform CLI releases on a best-effort basis. They may be removed in later v1.x releases, and will not be updated in case of any breaking changes to the services they integrate with.\n\nSupported Platforms\n\nThroughout the v1.x series we will continue to produce official releases for the following platforms, and make changes as necessary to support new releases of these operating systems:\n\nmacOS on x64 CPUs (darwin_amd64)\nWindows on x64 CPUs (windows_amd64)\nLinux on x64, 32-bit ARMv6, and 64-bit ARMv8 (linux_amd64, linux_arm, and linux_arm64 respectively)\n\nOver time we may require newer versions of these operating systems. For example, subsequent Terraform releases in the v1.x series might end support for earlier versions of macOS or Windows, or earlier Linux kernel releases.\n\nWe have historically produced official releases for a number of other platforms as a convenience to users of those platforms, and we have no current plans to stop publishing them but we cannot promise ongoing releases or bug fixes for the other platforms throughout the v1.x series. We do not routinely test Terraform on any platforms other than those listed above.\n\nWe might add support for new platforms in later v1.x releases. If so, earlier Terraform releases prior to that support will not be available on those platforms.\n\nAll Terraform plugins, including provider plugins, are separate programs that have their own policies for which platforms they support. We cannot guarantee that all providers currently support or will continue to support the platforms listed above, even though Terraform CLI itself will support them.\n\nLater Revisions to These Promises\n\nWe may extend or refine these promises throughout the v1.x series in order to describe promises related to new features or to clarify existing promises if we find via feedback that our earlier statements had been unclear.\n\nPromises for new features will be additive in the sense that they will add further promises without retracting any existing ones. For promises that only apply to later v1.x releases we will mention the earliest version(s) those promises apply to.\n\nEven if we don't add an explicit statement to this document, we intend that any non-experimental features added in later v1.x releases will remain compatible at least through the remainder of the v1.x series, unless otherwise stated.\n\nAppendices\nProtected Workflow Commands\n\nThe following is the list of Terraform CLI subcommands and options that are subject to these compatibility promises. If you build automation around these commands then it should be compatible with all later v1.x releases.\n\nAs noted above, compatibility with external software is limited to explicitly-machine-readable output (-json and -raw modes) and exit codes. Any natural-language output from these commands might change in later releases.\n\ninit\n-backend=false\n-backend-config=FILE\n-backend-config=\"KEY=VALUE\"\n-force-copy\n-get=false\n-input=false\n-migrate-state\n-no-color\n-plugin-dir=DIR\n-reconfigure\n-upgrade\nvalidate\n-json\n-no-color\nplan\n-compact-warnings\n-destroy\n-detailed-exitcode\n-lock=false\n-lock-timeout=DURATION\n-input=false\n-json\n-no-color\n-out=FILE\n-parallelism=N\n-refresh=false\n-refresh-only\n-replace=ADDRESS\n-target=ADDRESS\n-var 'NAME=VALUE'\n-var-file=FILE\napply\n-auto-approve\n-compact-warnings\n-lock=false\n-lock-timeout=DURATION\n-input=false\n-json\n-no-color\n-parallelism=N\n-refresh=false\n-refresh-only\n-replace=ADDRESS\n-target=ADDRESS\n-var 'NAME=VALUE'\n-var-file=FILE\nshow\n-no-color\n-json\n(both with and without a plan file)\nproviders (with no subcommand)\nproviders lock\n-fs-mirror=PATH\n-net-mirror=URL\n-platform=OS_ARCH\nproviders mirror\n-platform=OS_ARCH\nproviders schema\n-json\nfmt\n-list=false\n-write=false\n-diff\n-recursive\n-check\nversion\n-json\noutput\n-no-color\n-json\n-raw\ntaint\n-allow-missing\n-lock=false\n-lock-timeout=DURATION\n-ignore-remote-version\nuntaint\n-allow-missing\n-lock=false\n-lock-timeout=DURATION\n-ignore-remote-version\nforce-unlock\n-force\nstate list\n-id=ID\nstate pull\nstate push\n-force\n-lock=false\n-lock-timeout=DURATION\nstate show\n-ignore-remote-version\nlogin\n\nFor commands or options not in the above list, we will still avoid breaking changes where possible, but can't promise full compatibility throughout the v1.x series. If you are building automation around Terraform, use only the commands above to avoid the need for changes when upgrading.\n\nPlease note that although Terraform's internal logs (via the TF_LOG environment variable) are available in a JSON format, the particular syntax or structure of those log lines is not a supported integration interface. The logs are available as JSON only to help with ad-hoc filtering and processing of logs by Terraform developers.\n\nCommands That Might Change\n\nAll of the following commands and their subcommands/options are not subject to compatibility promises, either because we have existing plans to improve them during the v1.x series or because we are aware of shortcomings in their design that might require breaking changes for ongoing maintenence.\n\nWhile you can freely use these commands when running Terraform interactively as long as they remain supported, we don't recommend using them as part of any automation unless you are willing to potentially update that automation when upgrading to a later v1.x release.\n\ndestroy (consider terraform apply -destroy instead)\nconsole\nget (consider terraform init instead)\ngraph\nimport\npush\nrefresh (consider terraform apply -refresh-only instead)\nstate mv\nstate replace-provider\nstate rm\nall subcommands of workspace (and its deprecated alias env)\n\nWhile we do intend to retain support for the main use-cases associated with these commands in future releases, we cannot promise to retain the exact command names or options used to meet those use-cases.\n\nHow We Will Keep These Promises\nAutomated Regression Testing\n\nThe Terraform codebase includes various unit and integration tests intended to help us to notice accidental behavior regressions before they ship in a stable version.\n\nHowever, Terraform is a relatively complex system with many different features that can interact in interesting ways. In the past we've seen reports of behavior differences that appeared only when combining two or more features in a way we hadn't previously anticipated or written automated tests for.\n\nIn each case we have both implemented a change to resolve the compatibility problem and added one or more integration tests representing the behavior of that combination of features. We intend to continue this approach, so we can improve Terraform's test coverage over time.\n\nPrerelease Versions\n\nWe intend that most accidental changes in behavior covered by these promises will be caught by existing tests. However, we also accept that our test suite can never have perfect coverage of all possible feature interactions or other edge cases, and so we aim for each significant change to be included in both alpha and beta releases before eventual inclusion in a final release.\n\nFor minor releases we will typically also issue at least one release candidate prior to the final release. A release candidate represents that planned development is concluded and that we've fixed any regressions reported based on the alpha and beta releases, and thus the final release that follows should typically match exactly or almost exactly its most recent release candidate.\n\nRegressions in Final Releases\n\nFor more obscure combinations of features it is possible that a regression could be undetected during prerelease the prerelease periods and thus included in a final release.\n\nIf someone finds and reports such a regression soon after its release then we will treat it as a bug and fix it to restore the previous behavior in future releases, unless there is a very significant justification such as a security advisory. In these cases, we'll typically recommend anyone affected by the regression remain on the previous version until the problem is fixed and then skip forward directly to the new release containing that fix.\n\nYou can minimize the risk of being affected by missed regressions in final releases by proactively testing modules against alpha, beta, and release candidate packages. We recommend doing so only in isolated development or staging environments rather than against your production infrastructure. If you find a change in behavior in a prerelease build that seems contrary to the promises in this document, please open an issue in Terraform's GitHub repository to discuss it.\n\nLate-reported Regressions\n\nIn the most extreme case, there may be a regression with a combination of features that is so rare that it remains undetected for a long time.\n\nAfter a change has been included in more releases it becomes increasingly likely that other users will have depended on the newer behavior and thus we will need to make a tradeoff to decide whether restoring the behavior would have a greater negative impact than retaining the new behavior. We will always make this decision with due consideration to the implications of each unique situation.\n\nYou can minimize the risk of your modules being affected by late-reported regressions by upgrading promptly to new minor and patch releases of Terraform and reporting any compatibility problems you encounter in Terraform's GitHub repository.\n\nPragmatic Exceptions\n\nWe are making the promises above in good faith, with the intent that your investment in writing Terraform modules or automation will not be invalidated by future changes to Terraform. However, broad promises like the above can't possibly cover all nuances of practical problems that might arise as we continue to develop Terraform.\n\nFor that reason, there are some situations where we may still need to make changes that may impact existing modules or automation:\n\nSecurity problems: We may become aware of a design problem that has an important security impact. Depending on our determination of the associated risk, we may choose to break compatibility to achieve a more secure system.\nExternal Dependencies: Terraform's behavior depends on interfaces provided by external codebases, including your chosen operating system and including some remote network services for situations such as module and provider installation. These external systems can change outside of our control, including potentially removing or changing features that Terraform's own features depend on. In that case, if there is no suitable replacement mechanism then we may need to change Terraform's design to work within the new constraints.\nOpt-in Compatibility Breaks: The design of a language new feature may require changing the behavior or configuration representation of an existing feature. If so, we will typically make the new feature opt-in only in order to avoid breaking existing modules, but if you change your module to opt in to the new feature then you may also then be required to change other parts of your configuration to work with the new language design.\nBugs in New Features: If we introduce a new feature to Terraform and the initial implementation has problems that cause it to not match the documented design intent at release, we may make a follow-up release that corrects the implementation to match the documented design, even if that represents a minor compatibility regression compared to the initial implementation. However, once a feature is well-established and in common use we will usually defer to the implemented behavior and instead change the documentation to reflect it.\nRegressions in Existing Features: If we learn that a new Terraform release includes a regression for an existing feature that wasn't detected during the development and prerelease periods, and that learning comes promptly after the new release, we will typically restore the previous behavior at the expense of technically therefore breaking compatibility with the behavior of the new release, under the assumption that more users will have systems affected by the regression than will have systems depending on the newly-introduced behavior.\nLate-reported regressions: As described in the previous section, if we learn that there was an unintentional regression of a rarely-used feature or combination of features in a much earlier release then restoring the previous behavior may appear as a regression to later adopters. If we believe that fixing the regression would affect more users than the regression itself affects then we may choose to accept the regression as the new promised behavior.\nSituations we cannot anticipate: Although we've made an effort to consider various specific exceptional situations here, Terraform and its development process are not isolated from broader context, and so we must consider that there may be situations that we cannot possibly anticipate that would affect the future of Terraform. In those situations, we will always do our best to find a course of action that will minimize as much as possible the impact to existing modules and automation.\n\nOur intent with these pragmatic exceptions is only to acknowledge that there will always be situations that general compatibility promises cannot address. We will use these exceptions only with due consideration and as a last resort.\n\nOn this page:\n\nTerraform v1.x Compatibility Promises\nThe Terraform Language\nWorkflow\nUpgrading and Downgrading\nProviders\nExternal Modules\nProvisioners\nState Storage Backends\nCommunity-maintained State Storage Backends\nSupported Platforms\nLater Revisions to These Promises\nAppendices\nHow We Will Keep These Promises\nTheme\nDark\nLight\nSystem\nCertifications\nSystem Status\nCookie Manager\nTerms of Use\nSecurity\nPrivacy\nTrademark Policy\nTrade Controls\nGive Feedback\n(opens in new tab)\nWe use cookies & other similar technology to collect data to improve your experience on our site, as described in our Privacy Policy and Cookie Policy.\nManage PreferencesACCEPT"
  },
  {
    "title": "Terraform v1.x Compatibility Promises | Terraform | HashiCorp Developer",
    "url": "https://developer.hashicorp.com/terraform/language/v1.7.x/v1-compatibility-promises",
    "html": "Skip to main content\nNews\nHashiCorp to join IBM to accelerate multi-cloud automation.\nRead the blog →\nDismiss alert\nTerraform\nInstall\nTutorials\nDocumentation\nRegistry\n(opens in new tab)\nTry Cloud\n(opens in new tab)\nSearch\nCommand or control key\nK key\nTerraform Home\nConfiguration Language\nConfiguration Language\nTerraform Language\nOverview\nStyle Guide\nFiles and Directories\nSyntax\nResources\nData Sources\nProviders\nVariables and Outputs\nModules\nMoved block\nChecks\nImport\nExpressions\nFunctions\nTerraform Settings\nState\nTests\nUpgrading to Terraform v1.7\nv1.x Compatibility Promises\nTerraform Internals\nResources\nTutorial Library\nCertifications\nCommunity Forum\n(opens in new tab)\nSupport\n(opens in new tab)\nGitHub\n(opens in new tab)\nTerraform Registry\n(opens in new tab)\n\nYou are viewing documentation for version v1.7.x. View latest version.\n\nDeveloper\nTerraform\nConfiguration Language v1.7.x\nv1.x Compatibility Promises\nv1.7.x\nTerraform v1.x Compatibility Promises\n\nThe release of Terraform v1.0 represents an important milestone in the development of the Terraform language and workflow. Terraform v1.0 is a stable platform for describing and managing infrastructure.\n\nIn this release we're defining a number of Terraform behaviors that we intend to remain compatible with throughout the 1.x releases:\n\nA large subset of Terraform language features.\nA more conservative subset of the Terraform CLI workflow commands.\nThe wire protocol for communication between Terraform Core and Terraform providers.\nThe wire protocols for installation of Terraform providers and external Terraform modules.\n\nOur intention is that Terraform modules written for Terraform v1.0 will continue to plan and apply successfully, without required changes, throughout the v1.x releases.\n\nWe also intend that automation built around the workflow subset described in this document will work without changes in all future v1.x releases.\n\nFinally, we intend that providers built against the currently-documented provider wire protocol will be compatible with all future Terraform v1.x releases targeting the same operating system and architecture, without the need for source code modification or binary recompilation.\n\nIn short, we aim to make upgrades between v1.x releases straightforward, requiring no changes to your configuration, no extra commands to run upgrade steps, and no changes to any automation you've set up around Terraform.\n\nThe Terraform v1.x series will be actively maintained for at least 18 months after v1.0.\n\nThe following sections include some specific guidance on what we will promise throughout the v1.x series, for those who would like full details. At a higher level though, we don't intend to make any changes that would cause existing modules or automation to require changes when upgrading to a new v1.x release. We will generally treat compatibility problems in new Terraform CLI releases as bugs to be fixed unless there was a very significant justification for the change, such as in addressing a critical security problem or matching with a breaking change to a remote dependency that isn't directly under our control.\n\nThe Terraform Language\n\nThe main Terraform Language includes the language syntax, the top-level structures such as resource, module, and provider blocks, the \"meta-arguments\" in those blocks, and the documented semantics and behaviors for the operators and built-in functions available for use in expressions.\n\nThere is not a single formal specification for the Terraform language, but the Configuration section of the documentation on the Terraform website serves as a description of the language features and their intended behaviors.\n\nThe following top-level blocks and their defined \"meta-arguments\" (that is, arguments defined by Terraform Core rather than by external plugins such as providers) will retain their current functionality:\n\nresource and data blocks to declare resources, including their nested block types lifecycle, connection, and provisioner, and their meta-argument provider.\nmodule blocks to call other modules, and its meta-argument providers.\nThe count, for_each, and depends_on meta-arguments in resource, data, and module blocks.\nprovider blocks to configure providers, and the alias meta-argument.\nvariable, output, and locals blocks for declaring the various kinds of named values in a module.\nterraform blocks, including the nested required_version and required_providers arguments, and nested backend blocks for backend configuration.\n\nWe also intend to keep compatibility with all expression operators and built-in functions, with the exception of references to terraform.workspace, whose behavior may change as part of future changes to the workspace model.\n\nWe intend to retain broad compatibility with Terraform language features, with a few specific caveats:\n\nWe consider a configuration to be valid if Terraform can create and apply a plan for it without reporting any errors.\n\nA configuration that currently produces errors might generate different errors or exhibit other non-error behaviors in a future version of Terraform. A configuration that generates errors during the apply phase might generate similar errors at an earlier phase in future, because we generally consider it better to detect errors in as early a phase as possible.\n\nGenerally-speaking, the compatibility promises described in this document apply only to valid configurations. Handling of invalid configurations is always subject to change in future Terraform releases.\n\nIf the actual behavior of a feature differs from what we explicitly documented as the feature's behavior, we will usually treat that as a bug and change the feature to match the documentation, although we will avoid making such changes if they seem likely to cause broad compatibility problems. We cannot promise to always remain \"bug-compatible\" with previous releases, but we will consider such fixes carefully to minimize their impact.\n\nAny experimental features may change or may be removed entirely from future releases. Terraform always produces a warning when an experimental language feature is active, to make you aware of that risk. We don't recommend using experimental features in production modules.\n\nWe will introduce new language features, and if you start using them then your configuration won't work with prior Terraform versions that didn't support those features yet.\n\nTerraform Providers are separate plugins which can change independently of Terraform Core and are therefore not subject to these compatibility promises. If you upgrade any of the providers you are using then you might need to change provider or resource configurations related to those providers.\n\nA small number of features remain deprecated with explicit warnings in Terraform v1.0. Those deprecation cycles will end in a future v1.x release, at which point we will remove the corresponding features.\n\nWorkflow\n\nThere is a set of often used Terraform workflows, which we are calling protected workflows. We will not remove these commands, subcommands, and flags or make backward-incompatible changes to protected workflow functionality. If we accidentally change these, we will consider backwards-incompatible changes to core workflows as bugs to be fixed. For a list of the command and option combinations that are part of protected workflows, see Protected Workflow Commands. There is another set of commands that we are explicitly not making compatibility promises about, because we expect their functionality to change in v1.x releases: see Commands That Might Change.\n\nThe supported ways for external software to interact with Terraform are via the JSON output modes offered by some commands and via exit status codes. We may extend certain JSON formats with new object properties but we will not remove or make breaking changes to the definitions of existing properties.\n\nNatural language command output or log output is not a stable interface and may change in any new version. If you write software that parses this output then it may need to be updated when you upgrade Terraform. If you need access to data that is not currently available via one of the machine-readable JSON interfaces, we suggest opening a feature request to discuss your use-case.\n\nUpgrading and Downgrading\n\nThroughout the v1.x series of releases, we intend that you should be able to switch to a newer Terraform version and use it just as before, without any special upgrade steps.\n\nYou should be able to upgrade from any v1.x release to any later v1.x release. You might also be able to downgrade to an earlier v1.x release, but that isn't guaranteed: later releases may introduce new features that earlier versions cannot understand, including new storage formats for Terraform state snapshots.\n\nIf you make use of features introduced in a later v1.x release, your configuration won't be compatible with releases that predate that feature. For example, if a language feature is added in v1.3 and you start using it, your Terraform configuration will no longer be compatible with Terraform v1.2.\n\nProviders\n\nTerraform providers are separate plugins which communicate with Terraform using a documented protocol. Therefore these compatibility promises can only cover the \"client\" side of this protocol as implemented by Terraform Core; the behaviors of individual providers, including which resource types they support and which arguments they expect, are decided by the provider development teams and can change independently of Terraform Core releases.\n\nIf you upgrade to a new version of a provider then you might need to change the parts of your configuration which are interpreted by that provider, even if you are still using a Terraform v1.x release.\n\nProvider Installation Methods\n\nTerraform normally installs providers from a provider registry implementing the Provider Registry Protocol, version 1. All Terraform v1.x releases will remain compatible with that protocol, and so correctly-implemented provider registries will stay compatible.\n\nTerraform also supports installation of providers from local filesystem directories (filesystem mirrors) and from network mirrors (implementing the Provider Mirror Protocol. All Terraform v1.x releases will remain compatible with those installation methods, including the Implied Local Mirror Directories.\n\nSpecific provider registries or network mirrors are run independently from Terraform itself and so their own behaviors are not subject to these compatibility promises.\n\nProvider Protocol Versions\n\nThe current major version of the provider plugin protocol as of Terraform v1.0 is version 5, which is defined by a combination of a Protocol Buffers schema describing the physical wire formats and by additional prose documentation describing the expected provider behaviors.\n\nWe will support protocol version 5 throughout the Terraform v1.x releases. If we make new minor revisions to protocol version 5 in later releases then we will design them such that existing plugins will continue to work, as long as they correctly implemented the protocol.\n\nWe may introduce new major versions of the protocol during the v1.x series. If so, we will continue to support protocol version 5 alongside those new versions. Individual provider teams might decide to remove support for protocol version 5 in later releases, in which case those new provider releases will not be compatible with all of the Terraform v1.x releases.\n\nExternal Modules\n\nTerraform modules are reusable infrastructure components written in the Terraform language. Some modules are \"external\" in the sense that Terraform automatically installs them from a location other than the current configuration directory, in which case their contents could change independently of changes to your local modules, of the providers you use, and of Terraform itself.\n\nModule Installation Methods\n\nTerraform supports installing child modules from a number of different module source types. We will continue to support all of the existing source types throughout the v1.x releases.\n\nOne of the supported source types is a module registry implementing the Module Registry Protocol version 1. All Terraform v1.x releases will remain compatible with correct implementations of that protocol.\n\nSome module source types work directly with services or protocols defined and run by third parties. Although we will not remove Terraform's own client-side support for those, we cannot guarantee that their owners will keep those services running or that they will remain compatible with Terraform's client implementations.\n\nExternal Module Compatibility\n\nIf your configuration depends on external modules, newer versions of those modules may include breaking changes. External modules are not part of Terraform and are therefore not subject to these compatibility promises.\n\nProvisioners\n\nWe will maintain compatibility for the file, local-exec, and remote-exec provisioner types through all v1.x releases.\n\nSome additional vendor-specific provisioners were available in earlier Terraform versions but were deprecated in Terraform v0.13 and removed in Terraform v0.15.\n\nTerraform supports loading additional provisioners as plugins from certain local filesystem directories. We'll continue to support that throughout the Terraform v1.x releases, but since such plugins are separate from Terraform Core itself their own behaviors cannot be subject to these compatibility promises. However, we will continue to support the plugin wire protocol as defined in Terraform v1.0 throughout the v1.x releases, and so correctly-implemented provisioner plugins should remain compatible with future Terraform releases.\n\nState Storage Backends\n\nWhen you use remote state, Terraform interacts with remote services over the network in order to store and manage locks for Terraform state.\n\nFor historical reasons, all supported state storage backends are included as part of Terraform CLI but not all are supported directly by the Terraform Team. Only the following backends maintained by the Terraform team are subject to compatibility promises:\n\nlocal (the default, when you are not using remote state)\nhttp\n\nThe other state storage backends are maintained by external teams via contributions to the Terraform CLI codebase, and so their expected configuration arguments or behaviors might change even in v1.x releases, although we will aim to still ensure a good migration path in such cases, where possible.\n\nWe are considering allowing external state storage backend implementations via plugins, similar to provider plugins. If we introduce such a mechanism during the v1.x releases then you may need to make configuration changes in order to use those plugins, and state storage backends other than those listed above may be removed from later versions of Terraform CLI once equivalent plugins are available.\n\nThe remote Backend and Terraform Cloud\n\nThe remote backend is maintained by the Terraform Cloud team and so its behavior may change along with ongoing changes to Terraform Cloud.\n\nThere will be a supported mechanism to use Terraform CLI with Terraform Cloud throughout the v1.x releases, but the exact details may change. Terraform Cloud evolves independently of Terraform CLI and is therefore not subject to these compatibility promises.\n\nCommunity-maintained State Storage Backends\n\nThe azurerm, consul, s3, and kubernetes backends are maintained by other teams at HashiCorp. Those teams intend to continue basic maintenence at the level of bug fixes through the v1.x releases, unless we implement a plugin protocol for backends at which point development of these backends is likely to continue in the external plugins only, which may require configuration changes to switch to the plugin equivalents.\n\nThe cos, oss, pg, gcs, and etcdv3 backends are maintained by outside contributors and are not subject to these compatibility promises.\n\nUnmaintained State Storage Backends\n\nThe artifactory, etcdv2, manta, and swift state storage backends do not currently have any maintainers and thus remain in Terraform CLI releases on a best-effort basis. They may be removed in later v1.x releases, and will not be updated in case of any breaking changes to the services they integrate with.\n\nSupported Platforms\n\nThroughout the v1.x series we will continue to produce official releases for the following platforms, and make changes as necessary to support new releases of these operating systems:\n\nmacOS on x64 CPUs (darwin_amd64)\nWindows on x64 CPUs (windows_amd64)\nLinux on x64, 32-bit ARMv6, and 64-bit ARMv8 (linux_amd64, linux_arm, and linux_arm64 respectively)\n\nOver time we may require newer versions of these operating systems. For example, subsequent Terraform releases in the v1.x series might end support for earlier versions of macOS or Windows, or earlier Linux kernel releases.\n\nWe have historically produced official releases for a number of other platforms as a convenience to users of those platforms, and we have no current plans to stop publishing them but we cannot promise ongoing releases or bug fixes for the other platforms throughout the v1.x series. We do not routinely test Terraform on any platforms other than those listed above.\n\nWe might add support for new platforms in later v1.x releases. If so, earlier Terraform releases prior to that support will not be available on those platforms.\n\nAll Terraform plugins, including provider plugins, are separate programs that have their own policies for which platforms they support. We cannot guarantee that all providers currently support or will continue to support the platforms listed above, even though Terraform CLI itself will support them.\n\nLater Revisions to These Promises\n\nWe may extend or refine these promises throughout the v1.x series in order to describe promises related to new features or to clarify existing promises if we find via feedback that our earlier statements had been unclear.\n\nPromises for new features will be additive in the sense that they will add further promises without retracting any existing ones. For promises that only apply to later v1.x releases we will mention the earliest version(s) those promises apply to.\n\nEven if we don't add an explicit statement to this document, we intend that any non-experimental features added in later v1.x releases will remain compatible at least through the remainder of the v1.x series, unless otherwise stated.\n\nAppendices\nProtected Workflow Commands\n\nThe following is the list of Terraform CLI subcommands and options that are subject to these compatibility promises. If you build automation around these commands then it should be compatible with all later v1.x releases.\n\nAs noted above, compatibility with external software is limited to explicitly-machine-readable output (-json and -raw modes) and exit codes. Any natural-language output from these commands might change in later releases.\n\ninit\n-backend=false\n-backend-config=FILE\n-backend-config=\"KEY=VALUE\"\n-force-copy\n-get=false\n-input=false\n-migrate-state\n-no-color\n-plugin-dir=DIR\n-reconfigure\n-upgrade\nvalidate\n-json\n-no-color\nplan\n-compact-warnings\n-destroy\n-detailed-exitcode\n-lock=false\n-lock-timeout=DURATION\n-input=false\n-json\n-no-color\n-out=FILE\n-parallelism=N\n-refresh=false\n-refresh-only\n-replace=ADDRESS\n-target=ADDRESS\n-var 'NAME=VALUE'\n-var-file=FILE\napply\n-auto-approve\n-compact-warnings\n-lock=false\n-lock-timeout=DURATION\n-input=false\n-json\n-no-color\n-parallelism=N\n-refresh=false\n-refresh-only\n-replace=ADDRESS\n-target=ADDRESS\n-var 'NAME=VALUE'\n-var-file=FILE\nshow\n-no-color\n-json\n(both with and without a plan file)\nproviders (with no subcommand)\nproviders lock\n-fs-mirror=PATH\n-net-mirror=URL\n-platform=OS_ARCH\nproviders mirror\n-platform=OS_ARCH\nproviders schema\n-json\nfmt\n-list=false\n-write=false\n-diff\n-recursive\n-check\nversion\n-json\noutput\n-no-color\n-json\n-raw\ntaint\n-allow-missing\n-lock=false\n-lock-timeout=DURATION\n-ignore-remote-version\nuntaint\n-allow-missing\n-lock=false\n-lock-timeout=DURATION\n-ignore-remote-version\nforce-unlock\n-force\nstate list\n-id=ID\nstate pull\nstate push\n-force\n-lock=false\n-lock-timeout=DURATION\nstate show\n-ignore-remote-version\nlogin\n\nFor commands or options not in the above list, we will still avoid breaking changes where possible, but can't promise full compatibility throughout the v1.x series. If you are building automation around Terraform, use only the commands above to avoid the need for changes when upgrading.\n\nPlease note that although Terraform's internal logs (via the TF_LOG environment variable) are available in a JSON format, the particular syntax or structure of those log lines is not a supported integration interface. The logs are available as JSON only to help with ad-hoc filtering and processing of logs by Terraform developers.\n\nCommands That Might Change\n\nAll of the following commands and their subcommands/options are not subject to compatibility promises, either because we have existing plans to improve them during the v1.x series or because we are aware of shortcomings in their design that might require breaking changes for ongoing maintenence.\n\nWhile you can freely use these commands when running Terraform interactively as long as they remain supported, we don't recommend using them as part of any automation unless you are willing to potentially update that automation when upgrading to a later v1.x release.\n\ndestroy (consider terraform apply -destroy instead)\nconsole\nget (consider terraform init instead)\ngraph\nimport\npush\nrefresh (consider terraform apply -refresh-only instead)\nstate mv\nstate replace-provider\nstate rm\nall subcommands of workspace (and its deprecated alias env)\n\nWhile we do intend to retain support for the main use-cases associated with these commands in future releases, we cannot promise to retain the exact command names or options used to meet those use-cases.\n\nHow We Will Keep These Promises\nAutomated Regression Testing\n\nThe Terraform codebase includes various unit and integration tests intended to help us to notice accidental behavior regressions before they ship in a stable version.\n\nHowever, Terraform is a relatively complex system with many different features that can interact in interesting ways. In the past we've seen reports of behavior differences that appeared only when combining two or more features in a way we hadn't previously anticipated or written automated tests for.\n\nIn each case we have both implemented a change to resolve the compatibility problem and added one or more integration tests representing the behavior of that combination of features. We intend to continue this approach, so we can improve Terraform's test coverage over time.\n\nPrerelease Versions\n\nWe intend that most accidental changes in behavior covered by these promises will be caught by existing tests. However, we also accept that our test suite can never have perfect coverage of all possible feature interactions or other edge cases, and so we aim for each significant change to be included in both alpha and beta releases before eventual inclusion in a final release.\n\nFor minor releases we will typically also issue at least one release candidate prior to the final release. A release candidate represents that planned development is concluded and that we've fixed any regressions reported based on the alpha and beta releases, and thus the final release that follows should typically match exactly or almost exactly its most recent release candidate.\n\nRegressions in Final Releases\n\nFor more obscure combinations of features it is possible that a regression could be undetected during prerelease the prerelease periods and thus included in a final release.\n\nIf someone finds and reports such a regression soon after its release then we will treat it as a bug and fix it to restore the previous behavior in future releases, unless there is a very significant justification such as a security advisory. In these cases, we'll typically recommend anyone affected by the regression remain on the previous version until the problem is fixed and then skip forward directly to the new release containing that fix.\n\nYou can minimize the risk of being affected by missed regressions in final releases by proactively testing modules against alpha, beta, and release candidate packages. We recommend doing so only in isolated development or staging environments rather than against your production infrastructure. If you find a change in behavior in a prerelease build that seems contrary to the promises in this document, please open an issue in Terraform's GitHub repository to discuss it.\n\nLate-reported Regressions\n\nIn the most extreme case, there may be a regression with a combination of features that is so rare that it remains undetected for a long time.\n\nAfter a change has been included in more releases it becomes increasingly likely that other users will have depended on the newer behavior and thus we will need to make a tradeoff to decide whether restoring the behavior would have a greater negative impact than retaining the new behavior. We will always make this decision with due consideration to the implications of each unique situation.\n\nYou can minimize the risk of your modules being affected by late-reported regressions by upgrading promptly to new minor and patch releases of Terraform and reporting any compatibility problems you encounter in Terraform's GitHub repository.\n\nPragmatic Exceptions\n\nWe are making the promises above in good faith, with the intent that your investment in writing Terraform modules or automation will not be invalidated by future changes to Terraform. However, broad promises like the above can't possibly cover all nuances of practical problems that might arise as we continue to develop Terraform.\n\nFor that reason, there are some situations where we may still need to make changes that may impact existing modules or automation:\n\nSecurity problems: We may become aware of a design problem that has an important security impact. Depending on our determination of the associated risk, we may choose to break compatibility to achieve a more secure system.\nExternal Dependencies: Terraform's behavior depends on interfaces provided by external codebases, including your chosen operating system and including some remote network services for situations such as module and provider installation. These external systems can change outside of our control, including potentially removing or changing features that Terraform's own features depend on. In that case, if there is no suitable replacement mechanism then we may need to change Terraform's design to work within the new constraints.\nOpt-in Compatibility Breaks: The design of a language new feature may require changing the behavior or configuration representation of an existing feature. If so, we will typically make the new feature opt-in only in order to avoid breaking existing modules, but if you change your module to opt in to the new feature then you may also then be required to change other parts of your configuration to work with the new language design.\nBugs in New Features: If we introduce a new feature to Terraform and the initial implementation has problems that cause it to not match the documented design intent at release, we may make a follow-up release that corrects the implementation to match the documented design, even if that represents a minor compatibility regression compared to the initial implementation. However, once a feature is well-established and in common use we will usually defer to the implemented behavior and instead change the documentation to reflect it.\nRegressions in Existing Features: If we learn that a new Terraform release includes a regression for an existing feature that wasn't detected during the development and prerelease periods, and that learning comes promptly after the new release, we will typically restore the previous behavior at the expense of technically therefore breaking compatibility with the behavior of the new release, under the assumption that more users will have systems affected by the regression than will have systems depending on the newly-introduced behavior.\nLate-reported regressions: As described in the previous section, if we learn that there was an unintentional regression of a rarely-used feature or combination of features in a much earlier release then restoring the previous behavior may appear as a regression to later adopters. If we believe that fixing the regression would affect more users than the regression itself affects then we may choose to accept the regression as the new promised behavior.\nSituations we cannot anticipate: Although we've made an effort to consider various specific exceptional situations here, Terraform and its development process are not isolated from broader context, and so we must consider that there may be situations that we cannot possibly anticipate that would affect the future of Terraform. In those situations, we will always do our best to find a course of action that will minimize as much as possible the impact to existing modules and automation.\n\nOur intent with these pragmatic exceptions is only to acknowledge that there will always be situations that general compatibility promises cannot address. We will use these exceptions only with due consideration and as a last resort.\n\nOn this page:\n\nTerraform v1.x Compatibility Promises\nThe Terraform Language\nWorkflow\nUpgrading and Downgrading\nProviders\nExternal Modules\nProvisioners\nState Storage Backends\nCommunity-maintained State Storage Backends\nSupported Platforms\nLater Revisions to These Promises\nAppendices\nHow We Will Keep These Promises\nTheme\nDark\nLight\nSystem\nCertifications\nSystem Status\nCookie Manager\nTerms of Use\nSecurity\nPrivacy\nTrademark Policy\nTrade Controls\nGive Feedback\n(opens in new tab)\nWe use cookies & other similar technology to collect data to improve your experience on our site, as described in our Privacy Policy and Cookie Policy.\nManage PreferencesACCEPT"
  },
  {
    "title": "Terraform v1.x Compatibility Promises | Terraform | HashiCorp Developer",
    "url": "https://developer.hashicorp.com/terraform/language/v1.5.x/v1-compatibility-promises",
    "html": "Skip to main content\nNews\nHashiCorp to join IBM to accelerate multi-cloud automation.\nRead the blog →\nDismiss alert\nTerraform\nInstall\nTutorials\nDocumentation\nRegistry\n(opens in new tab)\nTry Cloud\n(opens in new tab)\nSearch\nCommand or control key\nK key\nTerraform Home\nConfiguration Language\nConfiguration Language\nTerraform Language\nOverview\nFiles and Directories\nSyntax\nResources\nData Sources\nProviders\nVariables and Outputs\nModules\nChecks\nImport\nExpressions\nFunctions\nTerraform Settings\nState\nUpgrading to Terraform v1.5\nv1.x Compatibility Promises\nTerraform Internals\nResources\nTutorial Library\nCertifications\nCommunity Forum\n(opens in new tab)\nSupport\n(opens in new tab)\nGitHub\n(opens in new tab)\nTerraform Registry\n(opens in new tab)\n\nYou are viewing documentation for version v1.5.x. View latest version.\n\nDeveloper\nTerraform\nConfiguration Language v1.5.x\nv1.x Compatibility Promises\nv1.5.x\nTerraform v1.x Compatibility Promises\n\nThe release of Terraform v1.0 represents an important milestone in the development of the Terraform language and workflow. Terraform v1.0 is a stable platform for describing and managing infrastructure.\n\nIn this release we're defining a number of Terraform behaviors that we intend to remain compatible with throughout the 1.x releases:\n\nA large subset of Terraform language features.\nA more conservative subset of the Terraform CLI workflow commands.\nThe wire protocol for communication between Terraform Core and Terraform providers.\nThe wire protocols for installation of Terraform providers and external Terraform modules.\n\nOur intention is that Terraform modules written for Terraform v1.0 will continue to plan and apply successfully, without required changes, throughout the v1.x releases.\n\nWe also intend that automation built around the workflow subset described in this document will work without changes in all future v1.x releases.\n\nFinally, we intend that providers built against the currently-documented provider wire protocol will be compatible with all future Terraform v1.x releases targeting the same operating system and architecture, without the need for source code modification or binary recompilation.\n\nIn short, we aim to make upgrades between v1.x releases straightforward, requiring no changes to your configuration, no extra commands to run upgrade steps, and no changes to any automation you've set up around Terraform.\n\nThe Terraform v1.x series will be actively maintained for at least 18 months after v1.0.\n\nThe following sections include some specific guidance on what we will promise throughout the v1.x series, for those who would like full details. At a higher level though, we don't intend to make any changes that would cause existing modules or automation to require changes when upgrading to a new v1.x release. We will generally treat compatibility problems in new Terraform CLI releases as bugs to be fixed unless there was a very significant justification for the change, such as in addressing a critical security problem or matching with a breaking change to a remote dependency that isn't directly under our control.\n\nThe Terraform Language\n\nThe main Terraform Language includes the language syntax, the top-level structures such as resource, module, and provider blocks, the \"meta-arguments\" in those blocks, and the documented semantics and behaviors for the operators and built-in functions available for use in expressions.\n\nThere is not a single formal specification for the Terraform language, but the Configuration section of the documentation on the Terraform website serves as a description of the language features and their intended behaviors.\n\nThe following top-level blocks and their defined \"meta-arguments\" (that is, arguments defined by Terraform Core rather than by external plugins such as providers) will retain their current functionality:\n\nresource and data blocks to declare resources, including their nested block types lifecycle, connection, and provisioner, and their meta-argument provider.\nmodule blocks to call other modules, and its meta-argument providers.\nThe count, for_each, and depends_on meta-arguments in resource, data, and module blocks.\nprovider blocks to configure providers, and the alias meta-argument.\nvariable, output, and locals blocks for declaring the various kinds of named values in a module.\nterraform blocks, including the nested required_version and required_providers arguments, and nested backend blocks for backend configuration.\n\nWe also intend to keep compatibility with all expression operators and built-in functions, with the exception of references to terraform.workspace, whose behavior may change as part of future changes to the workspace model.\n\nWe intend to retain broad compatibility with Terraform language features, with a few specific caveats:\n\nWe consider a configuration to be valid if Terraform can create and apply a plan for it without reporting any errors.\n\nA configuration that currently produces errors might generate different errors or exhibit other non-error behaviors in a future version of Terraform. A configuration that generates errors during the apply phase might generate similar errors at an earlier phase in future, because we generally consider it better to detect errors in as early a phase as possible.\n\nGenerally-speaking, the compatibility promises described in this document apply only to valid configurations. Handling of invalid configurations is always subject to change in future Terraform releases.\n\nIf the actual behavior of a feature differs from what we explicitly documented as the feature's behavior, we will usually treat that as a bug and change the feature to match the documentation, although we will avoid making such changes if they seem likely to cause broad compatibility problems. We cannot promise to always remain \"bug-compatible\" with previous releases, but we will consider such fixes carefully to minimize their impact.\n\nAny experimental features may change or may be removed entirely from future releases. Terraform always produces a warning when an experimental language feature is active, to make you aware of that risk. We don't recommend using experimental features in production modules.\n\nWe will introduce new language features, and if you start using them then your configuration won't work with prior Terraform versions that didn't support those features yet.\n\nTerraform Providers are separate plugins which can change independently of Terraform Core and are therefore not subject to these compatibility promises. If you upgrade any of the providers you are using then you might need to change provider or resource configurations related to those providers.\n\nA small number of features remain deprecated with explicit warnings in Terraform v1.0. Those deprecation cycles will end in a future v1.x release, at which point we will remove the corresponding features.\n\nWorkflow\n\nThere is a set of often used Terraform workflows, which we are calling protected workflows. We will not remove these commands, subcommands, and flags or make backward-incompatible changes to protected workflow functionality. If we accidentally change these, we will consider backwards-incompatible changes to core workflows as bugs to be fixed. For a list of the command and option combinations that are part of protected workflows, see Protected Workflow Commands. There is another set of commands that we are explicitly not making compatibility promises about, because we expect their functionality to change in v1.x releases: see Commands That Might Change.\n\nThe supported ways for external software to interact with Terraform are via the JSON output modes offered by some commands and via exit status codes. We may extend certain JSON formats with new object properties but we will not remove or make breaking changes to the definitions of existing properties.\n\nNatural language command output or log output is not a stable interface and may change in any new version. If you write software that parses this output then it may need to be updated when you upgrade Terraform. If you need access to data that is not currently available via one of the machine-readable JSON interfaces, we suggest opening a feature request to discuss your use-case.\n\nUpgrading and Downgrading\n\nThroughout the v1.x series of releases, we intend that you should be able to switch to a newer Terraform version and use it just as before, without any special upgrade steps.\n\nYou should be able to upgrade from any v1.x release to any later v1.x release. You might also be able to downgrade to an earlier v1.x release, but that isn't guaranteed: later releases may introduce new features that earlier versions cannot understand, including new storage formats for Terraform state snapshots.\n\nIf you make use of features introduced in a later v1.x release, your configuration won't be compatible with releases that predate that feature. For example, if a language feature is added in v1.3 and you start using it, your Terraform configuration will no longer be compatible with Terraform v1.2.\n\nProviders\n\nTerraform providers are separate plugins which communicate with Terraform using a documented protocol. Therefore these compatibility promises can only cover the \"client\" side of this protocol as implemented by Terraform Core; the behaviors of individual providers, including which resource types they support and which arguments they expect, are decided by the provider development teams and can change independently of Terraform Core releases.\n\nIf you upgrade to a new version of a provider then you might need to change the parts of your configuration which are interpreted by that provider, even if you are still using a Terraform v1.x release.\n\nProvider Installation Methods\n\nTerraform normally installs providers from a provider registry implementing the Provider Registry Protocol, version 1. All Terraform v1.x releases will remain compatible with that protocol, and so correctly-implemented provider registries will stay compatible.\n\nTerraform also supports installation of providers from local filesystem directories (filesystem mirrors) and from network mirrors (implementing the Provider Mirror Protocol. All Terraform v1.x releases will remain compatible with those installation methods, including the Implied Local Mirror Directories.\n\nSpecific provider registries or network mirrors are run independently from Terraform itself and so their own behaviors are not subject to these compatibility promises.\n\nProvider Protocol Versions\n\nThe current major version of the provider plugin protocol as of Terraform v1.0 is version 5, which is defined by a combination of a Protocol Buffers schema describing the physical wire formats and by additional prose documentation describing the expected provider behaviors.\n\nWe will support protocol version 5 throughout the Terraform v1.x releases. If we make new minor revisions to protocol version 5 in later releases then we will design them such that existing plugins will continue to work, as long as they correctly implemented the protocol.\n\nWe may introduce new major versions of the protocol during the v1.x series. If so, we will continue to support protocol version 5 alongside those new versions. Individual provider teams might decide to remove support for protocol version 5 in later releases, in which case those new provider releases will not be compatible with all of the Terraform v1.x releases.\n\nExternal Modules\n\nTerraform modules are reusable infrastructure components written in the Terraform language. Some modules are \"external\" in the sense that Terraform automatically installs them from a location other than the current configuration directory, in which case their contents could change independently of changes to your local modules, of the providers you use, and of Terraform itself.\n\nModule Installation Methods\n\nTerraform supports installing child modules from a number of different module source types. We will continue to support all of the existing source types throughout the v1.x releases.\n\nOne of the supported source types is a module registry implementing the Module Registry Protocol version 1. All Terraform v1.x releases will remain compatible with correct implementations of that protocol.\n\nSome module source types work directly with services or protocols defined and run by third parties. Although we will not remove Terraform's own client-side support for those, we cannot guarantee that their owners will keep those services running or that they will remain compatible with Terraform's client implementations.\n\nExternal Module Compatibility\n\nIf your configuration depends on external modules, newer versions of those modules may include breaking changes. External modules are not part of Terraform and are therefore not subject to these compatibility promises.\n\nProvisioners\n\nWe will maintain compatibility for the file, local-exec, and remote-exec provisioner types through all v1.x releases.\n\nSome additional vendor-specific provisioners were available in earlier Terraform versions but were deprecated in Terraform v0.13 and removed in Terraform v0.15.\n\nTerraform supports loading additional provisioners as plugins from certain local filesystem directories. We'll continue to support that throughout the Terraform v1.x releases, but since such plugins are separate from Terraform Core itself their own behaviors cannot be subject to these compatibility promises. However, we will continue to support the plugin wire protocol as defined in Terraform v1.0 throughout the v1.x releases, and so correctly-implemented provisioner plugins should remain compatible with future Terraform releases.\n\nState Storage Backends\n\nWhen you use remote state, Terraform interacts with remote services over the network in order to store and manage locks for Terraform state.\n\nFor historical reasons, all supported state storage backends are included as part of Terraform CLI but not all are supported directly by the Terraform Team. Only the following backends maintained by the Terraform team are subject to compatibility promises:\n\nlocal (the default, when you are not using remote state)\nhttp\n\nThe other state storage backends are maintained by external teams via contributions to the Terraform CLI codebase, and so their expected configuration arguments or behaviors might change even in v1.x releases, although we will aim to still ensure a good migration path in such cases, where possible.\n\nWe are considering allowing external state storage backend implementations via plugins, similar to provider plugins. If we introduce such a mechanism during the v1.x releases then you may need to make configuration changes in order to use those plugins, and state storage backends other than those listed above may be removed from later versions of Terraform CLI once equivalent plugins are available.\n\nThe remote Backend and Terraform Cloud\n\nThe remote backend is maintained by the Terraform Cloud team and so its behavior may change along with ongoing changes to Terraform Cloud.\n\nThere will be a supported mechanism to use Terraform CLI with Terraform Cloud throughout the v1.x releases, but the exact details may change. Terraform Cloud evolves independently of Terraform CLI and is therefore not subject to these compatibility promises.\n\nCommunity-maintained State Storage Backends\n\nThe azurerm, consul, s3, and kubernetes backends are maintained by other teams at HashiCorp. Those teams intend to continue basic maintenence at the level of bug fixes through the v1.x releases, unless we implement a plugin protocol for backends at which point development of these backends is likely to continue in the external plugins only, which may require configuration changes to switch to the plugin equivalents.\n\nThe cos, oss, pg, gcs, and etcdv3 backends are maintained by outside contributors and are not subject to these compatibility promises.\n\nUnmaintained State Storage Backends\n\nThe artifactory, etcdv2, manta, and swift state storage backends do not currently have any maintainers and thus remain in Terraform CLI releases on a best-effort basis. They may be removed in later v1.x releases, and will not be updated in case of any breaking changes to the services they integrate with.\n\nSupported Platforms\n\nThroughout the v1.x series we will continue to produce official releases for the following platforms, and make changes as necessary to support new releases of these operating systems:\n\nmacOS on x64 CPUs (darwin_amd64)\nWindows on x64 CPUs (windows_amd64)\nLinux on x64, 32-bit ARMv6, and 64-bit ARMv8 (linux_amd64, linux_arm, and linux_arm64 respectively)\n\nOver time we may require newer versions of these operating systems. For example, subsequent Terraform releases in the v1.x series might end support for earlier versions of macOS or Windows, or earlier Linux kernel releases.\n\nWe have historically produced official releases for a number of other platforms as a convenience to users of those platforms, and we have no current plans to stop publishing them but we cannot promise ongoing releases or bug fixes for the other platforms throughout the v1.x series. We do not routinely test Terraform on any platforms other than those listed above.\n\nWe might add support for new platforms in later v1.x releases. If so, earlier Terraform releases prior to that support will not be available on those platforms.\n\nAll Terraform plugins, including provider plugins, are separate programs that have their own policies for which platforms they support. We cannot guarantee that all providers currently support or will continue to support the platforms listed above, even though Terraform CLI itself will support them.\n\nLater Revisions to These Promises\n\nWe may extend or refine these promises throughout the v1.x series in order to describe promises related to new features or to clarify existing promises if we find via feedback that our earlier statements had been unclear.\n\nPromises for new features will be additive in the sense that they will add further promises without retracting any existing ones. For promises that only apply to later v1.x releases we will mention the earliest version(s) those promises apply to.\n\nEven if we don't add an explicit statement to this document, we intend that any non-experimental features added in later v1.x releases will remain compatible at least through the remainder of the v1.x series, unless otherwise stated.\n\nAppendices\nProtected Workflow Commands\n\nThe following is the list of Terraform CLI subcommands and options that are subject to these compatibility promises. If you build automation around these commands then it should be compatible with all later v1.x releases.\n\nAs noted above, compatibility with external software is limited to explicitly-machine-readable output (-json and -raw modes) and exit codes. Any natural-language output from these commands might change in later releases.\n\ninit\n-backend=false\n-backend-config=FILE\n-backend-config=\"KEY=VALUE\"\n-force-copy\n-get=false\n-input=false\n-migrate-state\n-no-color\n-plugin-dir=DIR\n-reconfigure\n-upgrade\nvalidate\n-json\n-no-color\nplan\n-compact-warnings\n-destroy\n-detailed-exitcode\n-lock=false\n-lock-timeout=DURATION\n-input=false\n-json\n-no-color\n-out=FILE\n-parallelism=N\n-refresh=false\n-refresh-only\n-replace=ADDRESS\n-target=ADDRESS\n-var 'NAME=VALUE'\n-var-file=FILE\napply\n-auto-approve\n-compact-warnings\n-lock=false\n-lock-timeout=DURATION\n-input=false\n-json\n-no-color\n-parallelism=N\n-refresh=false\n-refresh-only\n-replace=ADDRESS\n-target=ADDRESS\n-var 'NAME=VALUE'\n-var-file=FILE\nshow\n-no-color\n-json\n(both with and without a plan file)\nproviders (with no subcommand)\nproviders lock\n-fs-mirror=PATH\n-net-mirror=URL\n-platform=OS_ARCH\nproviders mirror\n-platform=OS_ARCH\nproviders schema\n-json\nfmt\n-list=false\n-write=false\n-diff\n-recursive\n-check\nversion\n-json\noutput\n-no-color\n-json\n-raw\ntaint\n-allow-missing\n-lock=false\n-lock-timeout=DURATION\n-ignore-remote-version\nuntaint\n-allow-missing\n-lock=false\n-lock-timeout=DURATION\n-ignore-remote-version\nforce-unlock\n-force\nstate list\n-id=ID\nstate pull\nstate push\n-force\n-lock=false\n-lock-timeout=DURATION\nstate show\n-ignore-remote-version\nlogin\n\nFor commands or options not in the above list, we will still avoid breaking changes where possible, but can't promise full compatibility throughout the v1.x series. If you are building automation around Terraform, use only the commands above to avoid the need for changes when upgrading.\n\nPlease note that although Terraform's internal logs (via the TF_LOG environment variable) are available in a JSON format, the particular syntax or structure of those log lines is not a supported integration interface. The logs are available as JSON only to help with ad-hoc filtering and processing of logs by Terraform developers.\n\nCommands That Might Change\n\nAll of the following commands and their subcommands/options are not subject to compatibility promises, either because we have existing plans to improve them during the v1.x series or because we are aware of shortcomings in their design that might require breaking changes for ongoing maintenence.\n\nWhile you can freely use these commands when running Terraform interactively as long as they remain supported, we don't recommend using them as part of any automation unless you are willing to potentially update that automation when upgrading to a later v1.x release.\n\ndestroy (consider terraform apply -destroy instead)\nconsole\nget (consider terraform init instead)\ngraph\nimport\npush\nrefresh (consider terraform apply -refresh-only instead)\nstate mv\nstate replace-provider\nstate rm\nall subcommands of workspace (and its deprecated alias env)\n\nWhile we do intend to retain support for the main use-cases associated with these commands in future releases, we cannot promise to retain the exact command names or options used to meet those use-cases.\n\nHow We Will Keep These Promises\nAutomated Regression Testing\n\nThe Terraform codebase includes various unit and integration tests intended to help us to notice accidental behavior regressions before they ship in a stable version.\n\nHowever, Terraform is a relatively complex system with many different features that can interact in interesting ways. In the past we've seen reports of behavior differences that appeared only when combining two or more features in a way we hadn't previously anticipated or written automated tests for.\n\nIn each case we have both implemented a change to resolve the compatibility problem and added one or more integration tests representing the behavior of that combination of features. We intend to continue this approach, so we can improve Terraform's test coverage over time.\n\nPrerelease Versions\n\nWe intend that most accidental changes in behavior covered by these promises will be caught by existing tests. However, we also accept that our test suite can never have perfect coverage of all possible feature interactions or other edge cases, and so we aim for each significant change to be included in both alpha and beta releases before eventual inclusion in a final release.\n\nFor minor releases we will typically also issue at least one release candidate prior to the final release. A release candidate represents that planned development is concluded and that we've fixed any regressions reported based on the alpha and beta releases, and thus the final release that follows should typically match exactly or almost exactly its most recent release candidate.\n\nRegressions in Final Releases\n\nFor more obscure combinations of features it is possible that a regression could be undetected during prerelease the prerelease periods and thus included in a final release.\n\nIf someone finds and reports such a regression soon after its release then we will treat it as a bug and fix it to restore the previous behavior in future releases, unless there is a very significant justification such as a security advisory. In these cases, we'll typically recommend anyone affected by the regression remain on the previous version until the problem is fixed and then skip forward directly to the new release containing that fix.\n\nYou can minimize the risk of being affected by missed regressions in final releases by proactively testing modules against alpha, beta, and release candidate packages. We recommend doing so only in isolated development or staging environments rather than against your production infrastructure. If you find a change in behavior in a prerelease build that seems contrary to the promises in this document, please open an issue in Terraform's GitHub repository to discuss it.\n\nLate-reported Regressions\n\nIn the most extreme case, there may be a regression with a combination of features that is so rare that it remains undetected for a long time.\n\nAfter a change has been included in more releases it becomes increasingly likely that other users will have depended on the newer behavior and thus we will need to make a tradeoff to decide whether restoring the behavior would have a greater negative impact than retaining the new behavior. We will always make this decision with due consideration to the implications of each unique situation.\n\nYou can minimize the risk of your modules being affected by late-reported regressions by upgrading promptly to new minor and patch releases of Terraform and reporting any compatibility problems you encounter in Terraform's GitHub repository.\n\nPragmatic Exceptions\n\nWe are making the promises above in good faith, with the intent that your investment in writing Terraform modules or automation will not be invalidated by future changes to Terraform. However, broad promises like the above can't possibly cover all nuances of practical problems that might arise as we continue to develop Terraform.\n\nFor that reason, there are some situations where we may still need to make changes that may impact existing modules or automation:\n\nSecurity problems: We may become aware of a design problem that has an important security impact. Depending on our determination of the associated risk, we may choose to break compatibility to achieve a more secure system.\nExternal Dependencies: Terraform's behavior depends on interfaces provided by external codebases, including your chosen operating system and including some remote network services for situations such as module and provider installation. These external systems can change outside of our control, including potentially removing or changing features that Terraform's own features depend on. In that case, if there is no suitable replacement mechanism then we may need to change Terraform's design to work within the new constraints.\nOpt-in Compatibility Breaks: The design of a language new feature may require changing the behavior or configuration representation of an existing feature. If so, we will typically make the new feature opt-in only in order to avoid breaking existing modules, but if you change your module to opt in to the new feature then you may also then be required to change other parts of your configuration to work with the new language design.\nBugs in New Features: If we introduce a new feature to Terraform and the initial implementation has problems that cause it to not match the documented design intent at release, we may make a follow-up release that corrects the implementation to match the documented design, even if that represents a minor compatibility regression compared to the initial implementation. However, once a feature is well-established and in common use we will usually defer to the implemented behavior and instead change the documentation to reflect it.\nRegressions in Existing Features: If we learn that a new Terraform release includes a regression for an existing feature that wasn't detected during the development and prerelease periods, and that learning comes promptly after the new release, we will typically restore the previous behavior at the expense of technically therefore breaking compatibility with the behavior of the new release, under the assumption that more users will have systems affected by the regression than will have systems depending on the newly-introduced behavior.\nLate-reported regressions: As described in the previous section, if we learn that there was an unintentional regression of a rarely-used feature or combination of features in a much earlier release then restoring the previous behavior may appear as a regression to later adopters. If we believe that fixing the regression would affect more users than the regression itself affects then we may choose to accept the regression as the new promised behavior.\nSituations we cannot anticipate: Although we've made an effort to consider various specific exceptional situations here, Terraform and its development process are not isolated from broader context, and so we must consider that there may be situations that we cannot possibly anticipate that would affect the future of Terraform. In those situations, we will always do our best to find a course of action that will minimize as much as possible the impact to existing modules and automation.\n\nOur intent with these pragmatic exceptions is only to acknowledge that there will always be situations that general compatibility promises cannot address. We will use these exceptions only with due consideration and as a last resort.\n\nOn this page:\n\nTerraform v1.x Compatibility Promises\nThe Terraform Language\nWorkflow\nUpgrading and Downgrading\nProviders\nExternal Modules\nProvisioners\nState Storage Backends\nCommunity-maintained State Storage Backends\nSupported Platforms\nLater Revisions to These Promises\nAppendices\nHow We Will Keep These Promises\nTheme\nDark\nLight\nSystem\nCertifications\nSystem Status\nCookie Manager\nTerms of Use\nSecurity\nPrivacy\nTrademark Policy\nTrade Controls\nGive Feedback\n(opens in new tab)\nWe use cookies & other similar technology to collect data to improve your experience on our site, as described in our Privacy Policy and Cookie Policy.\nManage PreferencesACCEPT"
  },
  {
    "title": "Upgrading to Terraform v1.4 | Terraform | HashiCorp Developer",
    "url": "https://developer.hashicorp.com/terraform/language/v1.4.x/upgrade-guides",
    "html": "Skip to main content\nNews\nHashiCorp to join IBM to accelerate multi-cloud automation.\nRead the blog →\nDismiss alert\nTerraform\nInstall\nTutorials\nDocumentation\nRegistry\n(opens in new tab)\nTry Cloud\n(opens in new tab)\nSearch\nCommand or control key\nK key\nTerraform Home\nConfiguration Language\nConfiguration Language\nTerraform Language\nOverview\nStyle Guide\nFiles and Directories\nSyntax\nResources\nData Sources\nProviders\nVariables and Outputs\nModules\nExpressions\nFunctions\nTerraform Settings\nState\nUpgrading to Terraform v1.4\nv1.x Compatibility Promises\nTerraform Internals\nResources\nTutorial Library\nCertifications\nCommunity Forum\n(opens in new tab)\nSupport\n(opens in new tab)\nGitHub\n(opens in new tab)\nTerraform Registry\n(opens in new tab)\n\nYou are viewing documentation for version v1.4.x. View latest version.\n\nDeveloper\nTerraform\nConfiguration Language v1.4.x\nUpgrading to Terraform v1.4\nv1.4.x\nUpgrading to Terraform v1.4\n\nTip: Use the version selector to view the upgrade guides for older Terraform versions.\n\nTerraform v1.4 is a minor release in the stable Terraform v1.0 series.\n\nTerraform v1.4 honors the Terraform v1.0 Compatibility Promises, but there are some behavior changes outside of those promises that may affect a small number of users. Specifically, the following updates may require additional upgrade steps:\n\nImportant Provider caching workflow change during terraform init\nHostname interpretation during terraform init\n\nSee the full changelog for more details. If you encounter any problems during upgrading which are not by this guide, or if the migration instructions don't work for you, please start a topic in the Terraform community forum to discuss it.\n\nProvider caching during terraform init\n\nThis change affects those who rely upon global provider caching and not the dependency lock file.\n\nterraform init now ignores entries in the optional global provider cache directory unless they match a checksum already tracked in the current configuration's dependency lock file.\n\nBefore this change Terraform could not determine the full set of checksums to include in the lockfile when installing a new provider for the first time. Now it can. Once the lock file has been updated to include a checksum covering the item in the global cache, Terraform will then use the cache entry for subsequent installation of the same provider package.\n\nFor more details and how to keep using the prior undesirable behavior, please see the documentation.\n\nHostname interpretation during terraform init\n\nWhen interpreting the hostname portion of a provider source address or the address of a module in a module registry, Terraform now uses non-transitional IDNA2008 mapping rules instead of the transitional mapping rules. Terraform no longer accepts the characters ß (U+00DF, \"LATIN SMALL LETTER SHARP S\") and ς (U+03C2, \"GREEK SMALL LETTER FINAL SIGMA\") Use the available alternative forms for both characters instead.\n\nThis change better adheres to the the WHATWG URL spec's rules for interpreting non-ASCII domain names. Terraform tries to interpret host names the same way that web browsers do. For some hostnames containing non-ASCII characters this may cause Terraform to now request a different \"punycode\" hostname when resolving.\n\nOn this page:\n\nUpgrading to Terraform v1.4\nProvider caching during terraform init\nHostname interpretation during terraform init\nTheme\nDark\nLight\nSystem\nCertifications\nSystem Status\nCookie Manager\nTerms of Use\nSecurity\nPrivacy\nTrademark Policy\nTrade Controls\nGive Feedback\n(opens in new tab)\nWe use cookies & other similar technology to collect data to improve your experience on our site, as described in our Privacy Policy and Cookie Policy.\nManage PreferencesACCEPT"
  },
  {
    "title": "Upgrading to Terraform v1.5 | Terraform | HashiCorp Developer",
    "url": "https://developer.hashicorp.com/terraform/language/v1.5.x/upgrade-guides",
    "html": "Skip to main content\nNews\nHashiCorp to join IBM to accelerate multi-cloud automation.\nRead the blog →\nDismiss alert\nTerraform\nInstall\nTutorials\nDocumentation\nRegistry\n(opens in new tab)\nTry Cloud\n(opens in new tab)\nSearch\nCommand or control key\nK key\nTerraform Home\nConfiguration Language\nConfiguration Language\nTerraform Language\nOverview\nFiles and Directories\nSyntax\nResources\nData Sources\nProviders\nVariables and Outputs\nModules\nChecks\nImport\nExpressions\nFunctions\nTerraform Settings\nState\nUpgrading to Terraform v1.5\nv1.x Compatibility Promises\nTerraform Internals\nResources\nTutorial Library\nCertifications\nCommunity Forum\n(opens in new tab)\nSupport\n(opens in new tab)\nGitHub\n(opens in new tab)\nTerraform Registry\n(opens in new tab)\n\nYou are viewing documentation for version v1.5.x. View latest version.\n\nDeveloper\nTerraform\nConfiguration Language v1.5.x\nUpgrading to Terraform v1.5\nv1.5.x\nUpgrading to Terraform v1.5\n\nTip: Use the version selector to view the upgrade guides for older Terraform versions.\n\nTerraform v1.5 is a minor release in the stable Terraform v1.0 series.\n\nTerraform v1.5 honors the Terraform v1.0 Compatibility Promises, but there are some behavior changes outside of those promises that may affect a small number of users. Specifically, the following updates may require additional upgrade steps:\n\nEnd of support for older macOS releases\nLinux DNS resolver changes\n\nSee the full changelog for more details. If you encounter any problems during upgrading which are not covered this guide, please start a new topic in the Terraform community forum to discuss it.\n\nEnd of support for older macOS releases\n\nTerraform v1.5 will be the last release supported on macOS 10.13 High Sierra and macOS 10.14 Mojave, both of which are no longer maintained by Apple.\n\nTerraform v1.5 itself supports these older macOS versions, but we strongly recommend upgrading during the v1.5 release period so that you'll be ready to use Terraform v1.6 once it is released.\n\nLinux DNS resolver changes\n\nTerraform on Linux uses a built-in DNS resolver rather than using the DNS resolver from the platform's C library, because this allows Terraform to run on systems with many different C libraries.\n\nIn Terraform v1.5, the DNS resolver will now notice when you have set the trust-ad option in your /etc/resolve.conf file, and will respond by setting the \"authentic data\" option in outgoing DNS requests to better match the behavior of the GNU libc DNS resolver.\n\nTerraform does not pay any attention to the corresponding option in responses, but some DNSSEC-aware recursive resolvers return different responses when the request option isn't set. This should therefore avoid some potential situations where a DNS request from Terraform might get a different response than a similar request from other software on your system.\n\nWe don't expect this behavior change to be significant for most Terraform users.\n\nNote that this change affects only DNS requests made by Terraform CLI itself, and not requests made by providers. Provider plugins are separate programs which handle DNS resolution themselves and so may have different behavior.\n\nOn this page:\n\nUpgrading to Terraform v1.5\nEnd of support for older macOS releases\nLinux DNS resolver changes\nTheme\nDark\nLight\nSystem\nCertifications\nSystem Status\nCookie Manager\nTerms of Use\nSecurity\nPrivacy\nTrademark Policy\nTrade Controls\nGive Feedback\n(opens in new tab)\nWe use cookies & other similar technology to collect data to improve your experience on our site, as described in our Privacy Policy and Cookie Policy.\nManage PreferencesACCEPT"
  },
  {
    "title": "Upgrading to Terraform v1.6 | Terraform | HashiCorp Developer",
    "url": "https://developer.hashicorp.com/terraform/language/v1.6.x/upgrade-guides",
    "html": "Skip to main content\nNews\nHashiCorp to join IBM to accelerate multi-cloud automation.\nRead the blog →\nDismiss alert\nTerraform\nInstall\nTutorials\nDocumentation\nRegistry\n(opens in new tab)\nTry Cloud\n(opens in new tab)\nSearch\nCommand or control key\nK key\nTerraform Home\nConfiguration Language\nConfiguration Language\nTerraform Language\nOverview\nStyle Guide\nFiles and Directories\nSyntax\nResources\nData Sources\nProviders\nVariables and Outputs\nModules\nMoved block\nChecks\nImport\nExpressions\nFunctions\nTerraform Settings\nState\nTests\nUpgrading to Terraform v1.6\nv1.x Compatibility Promises\nTerraform Internals\nResources\nTutorial Library\nCertifications\nCommunity Forum\n(opens in new tab)\nSupport\n(opens in new tab)\nGitHub\n(opens in new tab)\nTerraform Registry\n(opens in new tab)\n\nYou are viewing documentation for version v1.6.x. View latest version.\n\nDeveloper\nTerraform\nConfiguration Language v1.6.x\nUpgrading to Terraform v1.6\nv1.6.x\nUpgrading to Terraform v1.6\n\nTip: Use the version selector to view the upgrade guides for older Terraform versions.\n\nTerraform v1.6 is a minor release in the stable Terraform v1.0 series.\n\nTerraform v1.6 honors the Terraform v1.0 Compatibility Promises, but there are some behavior changes outside of those promises that may affect a small number of users. Specifically, the following updates may require additional upgrade steps:\n\nEnd of experimental period for terraform test\nDeprecated parameters for the S3 backend\n\nSee the full changelog for more details. If you encounter any problems during upgrading which are not covered this guide, please start a new topic in the Terraform community forum to discuss it.\n\nTerraform Test\n\nThe previous experimental terraform test command has been deprecated and replaced with a fully supported and finalized terraform test command.\n\nThere are substantial differences between the previous experimental approach and the finalized approach:\n\nThe builtin test provider, terraform.io/builtin/test, has been removed and a dedicated syntax introduced for testing files.\nA new .tftest.hcl file extension has been introduced for testing files, allowing a change into the directory structure and test file layout.\nTest assertions and conditions execute with extended scope and access to the configuration under test.\n\nThe major differences are discussed here, for more information consult the CLI and Language documentation.\n\nDirectory structure\n\nPreviously, test files would be placed within their own subdirectories underneath the tests directory from the configuration directory. The following example contains three test files using the experimental framework:\n\nmain.tf\n\noutputs.tf\n\nproviders.tf\n\nvariables.tf\n\ntests/\n\n  defaults/\n\n    test_defaults.tf\n\n  maximums/\n\n    test_maximums.tf\n\n  minimums/\n\n    test_minimums.tf\n\nCopy\n\nWith the new directory structure, tests are defined using the new .tftest.hcl file extension and do not need to be embedded within subdirectories. To help with organization, test files can, optionally, be embedded within a test directory. The name for this test directory defaults to tests, but can be overridden with the -test-directory flag.\n\nThe following examples are both valid directory structures for test files in the updated framework:\n\nmain.tf\n\noutputs.tf\n\nproviders.tf\n\nvariables.tf\n\ndefaults.tftest.hcl\n\nmaximums.tftest.hcl\n\nminimums.tftest.hcl\n\n\n\nmain.tf\n\noutputs.tf\n\nproviders.tf\n\nvariables.tf\n\ntests/\n\n  defaults.tftest.hcl\n\n  maximums.tftest.hcl\n\n  minimums.tftest.hcl\n\nCopy\nTest structure and assertions\n\nPreviously, a test file would contain a module call and a collection of resources from the builtin test provider:\n\n# tests/defaults/test_defaults.tf\n\n\n\nterraform{\n\n  required_providers {\n\n    test = {\n\n      source = \"terraform.io/builtin/test\"\n\n    }\n\n  }\n\n}\n\n\n\nmodule \"main\" {\n\n  source = \"../..\"\n\n}\n\n\n\nresource \"test_assertions\" \"api_url\" {\n\n  component = \"api_url\"\n\n\n\n  equal \"scheme\" {\n\n    description = \"default scheme is https\"\n\n    got         = module.main.scheme\n\n    want        = \"https\"\n\n  }\n\n  check \"port_number\" {\n\n    description = \"default port number is 8080\"\n\n    condition   = can(regex(\":8080$\", module.main.authority))\n\n  }\n\n}\n\nCopy\n\nWith the new framework each test file is made up of a series of run blocks. Each run block represents a single terraform plan or a terraform apply operation executed against the main configuration. Assertions from within these run blocks can access outputs, variables, resources, and local values from the main configuration directly.\n\n# tests/defaults.tftest.hcl\n\n\n\nrun \"test_defaults\" {\n\n  assert {\n\n    condition     = output.scheme == \"https\"\n\n    error_message = \"default scheme should be https\"\n\n  }\n\n\n\n  assert {\n\n    condition     = can(regex(\":8080\", output.authority))\n\n    error_message = \"default port number should be 8080\"\n\n  }\n\n}\n\nCopy\n\nThe above examples demonstrates the differences in layout, scope and access between the two approaches. In the experimental framework, access is granted as if the configuration was being called like a normal module call. In the released framework, assertions execute as if they are custom conditions defined within the main configuration directly.\n\nThe run block also applies or plans the main configuration by default, there is no need for the specific module call seen in the experimental framework.\n\nS3 Backend\n\nWe updated the S3 backend in Terraform 1.6.0 so that it more closely matches the AWS provider configuration. As a result, the backend has new and deprecated fields. Refer to the release notes for additional information.\n\nThe major deprecations are discussed here. Refer to the S3 backend documentation for information about all deprecations.\n\nWe removed the configuration for assuming an IAM role from several top-level attributes and consolidated them into the assume_role attribute.\n\nThe following example shows the configuration in Terraform 1.5.6 and older for assuming the IAM role arn:aws:iam::123456789012:role/example with a session name example-session and a session duration of 15 minutes:\n\nterraform {\n\n  backend \"s3\" {\n\n    # additional configuration omitted for brevity\n\n    role_arn                     = \"arn:aws:iam::123456789012:role/example\"\n\n    session_name                 = \"example-session\"\n\n    assume_role_duration_seconds = 900\n\n  }\n\n}\n\nCopy\n\nThe configuration in Terraform 1.6.0 is:\n\nterraform {\n\n  backend \"s3\" {\n\n    # additional configuration omitted for brevity\n\n    assume_role = {\n\n      role_arn     = \"arn:aws:iam::123456789012:role/example\"\n\n      session_name = \"example-session\"\n\n      duration     = \"15m\"\n\n    }\n\n  }\n\n}\n\nCopy\n\nWe removed the configuration for overriding AWS API endpoints from several top-level attributes and consolidated them into the endpoints attribute. The following endpoint attributes are now nested under the endpoint attribute:\n\ns3\ndynamodb\niam\nsts\n\nThe endpoint attribute replaces the following top-level attributes:\n\nendpoint (for S3),\ndynamodb_endpoint,\niam_endpoint\nsts_endpoint\n\nOn this page:\n\nUpgrading to Terraform v1.6\nTerraform Test\nS3 Backend\nTheme\nDark\nLight\nSystem\nCertifications\nSystem Status\nCookie Manager\nTerms of Use\nSecurity\nPrivacy\nTrademark Policy\nTrade Controls\nGive Feedback\n(opens in new tab)\nWe use cookies & other similar technology to collect data to improve your experience on our site, as described in our Privacy Policy and Cookie Policy.\nManage PreferencesACCEPT"
  },
  {
    "title": "Upgrading to Terraform v1.7 | Terraform | HashiCorp Developer",
    "url": "https://developer.hashicorp.com/terraform/language/v1.7.x/upgrade-guides",
    "html": "Skip to main content\nNews\nHashiCorp to join IBM to accelerate multi-cloud automation.\nRead the blog →\nDismiss alert\nTerraform\nInstall\nTutorials\nDocumentation\nRegistry\n(opens in new tab)\nTry Cloud\n(opens in new tab)\nSearch\nCommand or control key\nK key\nTerraform Home\nConfiguration Language\nConfiguration Language\nTerraform Language\nOverview\nStyle Guide\nFiles and Directories\nSyntax\nResources\nData Sources\nProviders\nVariables and Outputs\nModules\nMoved block\nChecks\nImport\nExpressions\nFunctions\nTerraform Settings\nState\nTests\nUpgrading to Terraform v1.7\nv1.x Compatibility Promises\nTerraform Internals\nResources\nTutorial Library\nCertifications\nCommunity Forum\n(opens in new tab)\nSupport\n(opens in new tab)\nGitHub\n(opens in new tab)\nTerraform Registry\n(opens in new tab)\n\nYou are viewing documentation for version v1.7.x. View latest version.\n\nDeveloper\nTerraform\nConfiguration Language v1.7.x\nUpgrading to Terraform v1.7\nv1.7.x\nUpgrading to Terraform v1.7\n\nTip: Use the version selector to view the upgrade guides for older Terraform versions.\n\nTerraform v1.7 is a minor release in the stable Terraform v1.0 series.\n\nTerraform v1.7 honors the Terraform v1.0 Compatibility Promises, but there are some behavior changes outside of those promises that may affect a small number of users. Specifically, the following updates may require additional upgrade steps:\n\nValidations and checks in the state file\nDeprecated parameters for the S3 backend\n\nSee the full changelog for more details. If you encounter any problems during upgrading which are not covered this guide, please start a new topic in the Terraform community forum to discuss it.\n\nValidations and Checks\n\nDue to a state interoperability issue (#33770, #34014) in earlier versions of Terraform, state files created by the Terraform v1.7.x series may not be compatible with the following Terraform versions:\n\nTerraform v1.3.0 through v1.3.9\nTerraform v1.4.0 through v1.4.6\nTerraform v1.5.0 through v1.5.6\n\nIf your v1.7.x state file contains check blocks or input validations, it will not be compatible with the above versions. Attempting to load a state file created by Terraform v1.7.x in one of the above versions will result in an error similar to Error refreshing state: unsupported checkable object \"var\". This will particularly affect configurations using the terraform_remote_state data source to load state files created by the v1.7.x series.\n\nTo prevent this issue, users should upgrade any usage of the affected versions to the following patch releases in the relevant minor release series before attempting to process state files created by Terraform v1.7.x:\n\nTerraform v1.3.x series users should upgrade to v1.3.10\nTerraform v1.4.x series users should upgrade to v1.4.7\nTerraform v1.5.x series users should upgrade to v1.5.7\n\nTerraform versions prior to v1.3.0 and equal to or after v1.6.0 are not affected by this issue.\n\nS3 Backend\n\nIn Terraform 1.7.0 the S3 backend will begin phasing out the legacy credential chain evaluation order by defaulting use_legacy_workflow to false and deprecating the argument. This will bring the default behavior of the backend into alignment with the AWS SDKs and CLI. The legacy behavior can be preserved by setting this argument to true.\n\nterraform {\n\n  backend \"s3\" {\n\n    # additional configuration omitted for brevity\n\n    use_legacy_workflow = true\n\n  }\n\n}\n\nCopy\n\nIn Terraform 1.8.0 this argument will be removed, and the S3 backend will always use the default AWS SDK for Go credential chain evaluation order.\n\nOn this page:\n\nUpgrading to Terraform v1.7\nValidations and Checks\nS3 Backend\nTheme\nDark\nLight\nSystem\nCertifications\nSystem Status\nCookie Manager\nTerms of Use\nSecurity\nPrivacy\nTrademark Policy\nTrade Controls\nGive Feedback\n(opens in new tab)\nWe use cookies & other similar technology to collect data to improve your experience on our site, as described in our Privacy Policy and Cookie Policy.\nManage PreferencesACCEPT"
  },
  {
    "title": "Upgrading to Terraform v1.8 | Terraform | HashiCorp Developer",
    "url": "https://developer.hashicorp.com/terraform/language/v1.9.x/upgrade-guides",
    "html": "Skip to main content\nNews\nHashiCorp to join IBM to accelerate multi-cloud automation.\nRead the blog →\nDismiss alert\nTerraform\nInstall\nTutorials\nDocumentation\nRegistry\n(opens in new tab)\nTry Cloud\n(opens in new tab)\nSearch\nCommand or control key\nK key\nTerraform Home\nConfiguration Language\nConfiguration Language\nTerraform Language\nOverview\nStyle Guide\nFiles and Directories\nSyntax\nResources\nData Sources\nProviders\nVariables and Outputs\nModules\nChecks\nImport\nExpressions\nFunctions\nTerraform Settings\nState\nTests\nUpgrading to Terraform v1.7\nv1.x Compatibility Promises\nTerraform Internals\nResources\nTutorial Library\nCertifications\nCommunity Forum\n(opens in new tab)\nSupport\n(opens in new tab)\nGitHub\n(opens in new tab)\nTerraform Registry\n(opens in new tab)\n\nYou are viewing documentation for pre-release version v1.9.0 (alpha). View latest version.\n\nDeveloper\nTerraform\nConfiguration Language v1.9.x\nUpgrading to Terraform v1.7\nv1.9.0 (alpha)\nUpgrading to Terraform v1.8\n\nTip: Use the version selector to view the upgrade guides for older Terraform versions.\n\nTerraform v1.8 is a minor release in the stable Terraform v1.0 series.\n\nTerraform v1.8 honors the Terraform v1.0 Compatibility Promises, but there are some behavior changes outside of those promises that may affect a small number of users. Specifically, the following updates may require additional upgrade steps:\n\nuse_legacy_workflow is no longer available for the S3 backend\nPossible spurious changes when refreshing\nEnding support for macOS 10.15 Catalina\nMinor change to jsonencode function results\n\nSee the full changelog for more details. If you encounter any problems during upgrading which are not covered this guide, please start a new topic in the Terraform community forum to discuss it.\n\nS3 Backend authentication changes\n\nTerraform v1.7 began the deprecation of a legacy approach to authentication, making the use_legacy_workflow argument default to false and thus making the old authentication workflow opt-in.\n\nTerraform v1.8 completes this deprecation process by removing the use_legacy_workflow argument. The old behavior is no longer available, and so you will need to adopt the new behavior when upgrading to Terraform v1.8.\n\nThe new implementation follows the authentication process implemented in the official AWS SDK for Go, which is therefore more consistent with other AWS tools such as the official AWS CLI.\n\nPossible spurious changes when refreshing\n\nIf you use the -refresh-only or -refresh=false planning options for your first plan after upgrading, Terraform might show resource instance diffs without any visible changes. This does not affect plans created with both of those options disabled.\n\nPrevious versions of Terraform used a mixture of both dynamic and static tracking of sensitive values in resource instance attributes. That meant that, for example, correctly honoring sensitive values when interpreting the terraform show -json output required considering both the dynamic sensitivity information directly in the output and static sensitivity information in the provider schema.\n\nTo simplify handling of sensitivity in these cases, Terraform now copies the schema-based sensitivity information into the state along with the dynamic information. Terraform must therefore perform a one-time backfill update of the state metadata for resource types which have sensitive attributes.\n\nWhen using the default planning options Terraform should handle this update quietly, as part of the refresh step performed during planning. However, if you use the -refresh-only or -refresh=false option then you will effectively disable one half of this process, causing the UI to report spurious changes that affect only the metadata in the state.\n\nThese no-change metadata updates should not cause any problems, and will be resolved once a plan has been applied using Terraform v1.8. If you are concerned about a particular plan then try removing the -refresh-only or -refresh=false option, which should then quiet the spurious change.\n\nEnding support for macOS 10.15 Catalina\n\nTerraform v1.8 is the last series that will support macOS 10.15 Catalina. The next minor release series will require macOS 11 Big Sur or later.\n\nMinor change to jsonencode function results\n\nIn previous versions of Terraform, the jsonencode function encoded the control characters U+0008 (backspace) and U+000C (form feed) in strings using the unicode escape syntax: \\u0008 and \\u000c respectively.\n\nTerraform now follows the JSON idiom more closely by using \\b for backspace and \\f for form feed. These shorter encodings are equivalent for a correct JSON parser, but are more readable for humans due to being mnemonics.\n\nThese two control characters are relatively rarely used in practical JSON and so we don't expect that this change will have significant impact. If you are using them then this may cause the following effects:\n\nIf you are using jsonencode to produce JSON-encoded data for consumption by a JSON parser that doesn't correctly support these short encoding forms then it may not be able to parse the new results. Terraform implements JSON encoding as defined in IETF RFC 7159, which requires that parsers support these shorter encodings.\n\nIf you are using jsonencode to populate a resource argument where the underlying provider does not perform JSON normalization, the provider might propose changing the affected object to use the new encoding form. As long as the remote system correctly implements JSON, this update should not change the meaning of the JSON document.\n\nThis change only affects strings that include these two specific control characters. If you do not use these control characters in the strings you pass to jsonencode then this change will have no effect for you.\n\nOn this page:\n\nUpgrading to Terraform v1.8\nS3 Backend authentication changes\nPossible spurious changes when refreshing\nEnding support for macOS 10.15 Catalina\nMinor change to jsonencode function results\nTheme\nDark\nLight\nSystem\nCertifications\nSystem Status\nCookie Manager\nTerms of Use\nSecurity\nPrivacy\nTrademark Policy\nTrade Controls\nGive Feedback\n(opens in new tab)\nWe use cookies & other similar technology to collect data to improve your experience on our site, as described in our Privacy Policy and Cookie Policy.\nManage PreferencesACCEPT"
  },
  {
    "title": "The depends_on Meta-Argument - Configuration Language | Terraform | HashiCorp Developer",
    "url": "https://developer.hashicorp.com/terraform/language/meta-arguments/depends_on",
    "html": "Skip to main content\nNews\nHashiCorp to join IBM to accelerate multi-cloud automation.\nRead the blog →\nDismiss alert\nTerraform\nInstall\nTutorials\nDocumentation\nRegistry\n(opens in new tab)\nTry Cloud\n(opens in new tab)\nSearch\nCommand or control key\nK key\nTerraform Home\nConfiguration Language\nConfiguration Language\nStyle Guide\nFiles and Directories\nSyntax\nResources\nOverview\nResource Blocks\nResource Behavior\nMeta-Arguments\ndepends_on\ncount\nfor_each\nprovider\nlifecycle\nProvisioners\nThe terraform_data Resource Type\nData Sources\nProviders\nVariables and Outputs\nModules\nMoved block\nChecks\nImport\nExpressions\nFunctions\nTerraform Settings\nState\nTests\nUpgrading to Terraform v1.7\nv1.x Compatibility Promises\nTerraform Internals\nResources\nTutorial Library\nCertifications\nCommunity Forum\n(opens in new tab)\nSupport\n(opens in new tab)\nGitHub\n(opens in new tab)\nTerraform Registry\n(opens in new tab)\nDeveloper\nTerraform\nConfiguration Language\nMeta-Arguments\ndepends_on\nv1.8.x (latest)\nThe depends_on Meta-Argument\n\nUse the depends_on meta-argument to handle hidden resource or module dependencies that Terraform cannot automatically infer. You only need to explicitly specify a dependency when a resource or module relies on another resource's behavior but does not access any of that resource's data in its arguments.\n\nNote: Module support for depends_on was added in Terraform version 0.13, and prior versions can only use it with resources.\n\nProcessing and Planning Consequences\n\nThe depends_on meta-argument instructs Terraform to complete all actions on the dependency object (including Read actions) before performing actions on the object declaring the dependency. When the dependency object is an entire module, depends_on affects the order in which Terraform processes all of the resources and data sources associated with that module. Refer to Resource Dependencies and Data Resource Dependencies for more details.\n\nYou should use depends_on as a last resort because it can cause Terraform to create more conservative plans that replace more resources than necessary. For example, Terraform may treat more values as unknown “(known after apply)” because it is uncertain what changes will occur on the upstream object. This is especially likely when you use depends_on for modules.\n\nInstead of depends_on, we recommend using expression references to imply dependencies when possible. Expression references let Terraform understand which value the reference derives from and avoid planning changes if that particular value hasn’t changed, even if other parts of the upstream object have planned changes.\n\nUsage\n\nYou can use the depends_on meta-argument in module blocks and in all resource blocks, regardless of resource type. It requires a list of references to other resources or child modules in the same calling module. This list cannot include arbitrary expressions because the depends_on value must be known before Terraform knows resource relationships and thus before it can safely evaluate expressions.\n\nWe recommend always including a comment that explains why using depends_on is necessary. The following example uses depends_on to handle a \"hidden\" dependency on the aws_iam_instance_profile.example.\n\nresource \"aws_iam_role\" \"example\" {\n\n  name = \"example\"\n\n\n\n  # assume_role_policy is omitted for brevity in this example. Refer to the\n\n  # documentation for aws_iam_role for a complete example.\n\n  assume_role_policy = \"...\"\n\n}\n\n\n\nresource \"aws_iam_instance_profile\" \"example\" {\n\n  # Because this expression refers to the role, Terraform can infer\n\n  # automatically that the role must be created first.\n\n  role = aws_iam_role.example.name\n\n}\n\n\n\nresource \"aws_iam_role_policy\" \"example\" {\n\n  name   = \"example\"\n\n  role   = aws_iam_role.example.name\n\n  policy = jsonencode({\n\n    \"Statement\" = [{\n\n      # This policy allows software running on the EC2 instance to\n\n      # access the S3 API.\n\n      \"Action\" = \"s3:*\",\n\n      \"Effect\" = \"Allow\",\n\n    }],\n\n  })\n\n}\n\n\n\nresource \"aws_instance\" \"example\" {\n\n  ami           = \"ami-a1b2c3d4\"\n\n  instance_type = \"t2.micro\"\n\n\n\n  # Terraform can infer from this that the instance profile must\n\n  # be created before the EC2 instance.\n\n  iam_instance_profile = aws_iam_instance_profile.example\n\n\n\n  # However, if software running in this EC2 instance needs access\n\n  # to the S3 API in order to boot properly, there is also a \"hidden\"\n\n  # dependency on the aws_iam_role_policy that Terraform cannot\n\n  # automatically infer, so it must be declared explicitly:\n\n  depends_on = [\n\n    aws_iam_role_policy.example\n\n  ]\n\n}\n\nCopy\nEdit this page on GitHub\n\nOn this page:\n\nThe depends_on Meta-Argument\nProcessing and Planning Consequences\nUsage\nTheme\nDark\nLight\nSystem\nCertifications\nSystem Status\nCookie Manager\nTerms of Use\nSecurity\nPrivacy\nTrademark Policy\nTrade Controls\nGive Feedback\n(opens in new tab)\nWe use cookies & other similar technology to collect data to improve your experience on our site, as described in our Privacy Policy and Cookie Policy.\nManage PreferencesACCEPT"
  },
  {
    "title": "HashiCorp Developer",
    "url": "https://developer.hashicorp.com/terraform/language/v1.1.x/checks",
    "html": "News\nHashiCorp to join IBM to accelerate multi-cloud automation.\nRead the blog →\nDismiss alert\nProducts\nTutorials\nSearch\nCommand or control key\nK key\nThis page does not exist for version v1.1.x.\n\nPlease select either the most recent version or a valid version that includes the page you are looking for.\n\nGo back to Documentation\nTheme\nCertifications\nSystem Status\nCookie Manager\nTerms of Use\nSecurity\nPrivacy\nTrademark Policy\nTrade Controls\nGive Feedback\n(opens in new tab)"
  },
  {
    "title": "HashiCorp Developer",
    "url": "https://developer.hashicorp.com/terraform/language/v1.3.x/checks",
    "html": "News\nHashiCorp to join IBM to accelerate multi-cloud automation.\nRead the blog →\nDismiss alert\nProducts\nTutorials\nSearch\nCommand or control key\nK key\nThis page does not exist for version v1.3.x.\n\nPlease select either the most recent version or a valid version that includes the page you are looking for.\n\nGo back to Documentation\nTheme\nCertifications\nSystem Status\nCookie Manager\nTerms of Use\nSecurity\nPrivacy\nTrademark Policy\nTrade Controls\nGive Feedback\n(opens in new tab)"
  },
  {
    "title": "HashiCorp Developer",
    "url": "https://developer.hashicorp.com/terraform/language/v1.2.x/checks",
    "html": "News\nHashiCorp to join IBM to accelerate multi-cloud automation.\nRead the blog →\nDismiss alert\nProducts\nTutorials\nSearch\nCommand or control key\nK key\nThis page does not exist for version v1.2.x.\n\nPlease select either the most recent version or a valid version that includes the page you are looking for.\n\nGo back to Documentation\nTheme\nCertifications\nSystem Status\nCookie Manager\nTerms of Use\nSecurity\nPrivacy\nTrademark Policy\nTrade Controls\nGive Feedback\n(opens in new tab)"
  },
  {
    "title": "HashiCorp Developer",
    "url": "https://developer.hashicorp.com/terraform/language/v1.4.x/checks",
    "html": "News\nHashiCorp to join IBM to accelerate multi-cloud automation.\nRead the blog →\nDismiss alert\nProducts\nTutorials\nSearch\nCommand or control key\nK key\nThis page does not exist for version v1.4.x.\n\nPlease select either the most recent version or a valid version that includes the page you are looking for.\n\nGo back to Documentation\nTheme\nCertifications\nSystem Status\nCookie Manager\nTerms of Use\nSecurity\nPrivacy\nTrademark Policy\nTrade Controls\nGive Feedback\n(opens in new tab)"
  },
  {
    "title": "Checks - Configuration Language | Terraform | HashiCorp Developer",
    "url": "https://developer.hashicorp.com/terraform/language/v1.5.x/checks",
    "html": "Skip to main content\nNews\nHashiCorp to join IBM to accelerate multi-cloud automation.\nRead the blog →\nDismiss alert\nTerraform\nInstall\nTutorials\nDocumentation\nRegistry\n(opens in new tab)\nTry Cloud\n(opens in new tab)\nSearch\nCommand or control key\nK key\nTerraform Home\nConfiguration Language\nConfiguration Language\nTerraform Language\nOverview\nFiles and Directories\nSyntax\nResources\nData Sources\nProviders\nVariables and Outputs\nModules\nChecks\nImport\nExpressions\nFunctions\nTerraform Settings\nState\nUpgrading to Terraform v1.5\nv1.x Compatibility Promises\nTerraform Internals\nResources\nTutorial Library\nCertifications\nCommunity Forum\n(opens in new tab)\nSupport\n(opens in new tab)\nGitHub\n(opens in new tab)\nTerraform Registry\n(opens in new tab)\n\nYou are viewing documentation for version v1.5.x. View latest version.\n\nDeveloper\nTerraform\nConfiguration Language v1.5.x\nChecks\nv1.5.x\nChecks\n\nNote: Check blocks are only available in Terraform v1.5.0 and later.\n\nThe check block can validate your infrastructure outside the usual resource lifecycle. Check blocks address a gap between post-apply and functional validation of infrastructure.\n\nHands-on: Try the Validate Infrastructure Using Checks tutorial.\n\nCheck blocks allow you to define custom conditions that execute on every Terraform plan or apply operation without affecting the overall status of an operation. Check blocks execute as the last step of a plan or apply after Terraform has planned or provisioned your infrastructure.\n\nSyntax\n\nYou can declare a check block with a local name, zero-to-one scoped data sources, and one-to-many assertions.\n\nThe following example loads the Terraform website and validates that it returns the expected status code 200.\n\ncheck \"health_check\" {\n\n  data \"http\" \"terraform_io\" {\n\n    url = \"https://www.terraform.io\"\n\n  }\n\n\n\n  assert {\n\n    condition = data.http.terraform_io.status_code == 200\n\n    error_message = \"${data.http.terraform_io.url} returned an unhealthy status code\"\n\n  }\n\n}\n\nCopy\nScoped data sources\n\nYou can use any data source from any provider as a scoped data source within a check block.\n\nA check block can optionally contain a nested (a.k.a. scoped) data source. This data block behaves like an external data source, except you can not reference it outside its enclosing check block. Additionally, if a scoped data source's provider raises any errors, they are masked as warnings and do not prevent Terraform from continuing operation execution.\n\nYou can use a scoped data source to validate the status of a piece of infrastructure outside of the usual Terraform resource lifecycle. In the above example, if the terraform_io data source fails to load, you receive a warning instead of a blocking error, which would occur if you declared this data source outside of a check block.\n\nMeta-Arguments\n\nScoped data sources support the depends_on and provider meta-arguments. Scoped data sources do not support the count orfor_each meta-arguments.\n\ndepends_on\n\nThe depends_on meta-argument can be particularly powerful when used within scoped data sources.\n\nThe first time Terraform creates the initial plan for our previous example, the plan fails because Terraform has not applied its configuration yet. Meaning this test fails because Terraform must still create the resources to make this website exist. Therefore, the first time Terraform runs this check, it always throws a potentially distracting error message.\n\nYou can fix this by adding depends_on to your scoped data source, ensuring it depends on an essential piece of your site's infrastructure, such as the load balancer. The check returns known after apply until that crucial piece of your website is ready. This strategy avoids producing unnecessary warnings during setup, and the check executes during subsequent plans and applies.\n\nOne problem with this strategy is that if the resource your scoped data source depends_on changes, the check block returns known after apply until Terraform has updated that resource. Depending on your use case, this behavior could be acceptable or problematic.\n\nWe recommend implementing the depends_on meta-argument if your scoped data source depends on the existence of another resource without referencing it directly.\n\nAssertions\n\nCheck blocks validate your custom assertions using assert blocks. Each check block must have at least one, but potentially many, assert blocks. Each assert block has a condition attribute and an error_message attribute.\n\nUnlike other custom conditions, assertions do not affect Terraform's execution of an operation. A failed assertion reports a warning without halting the ongoing operation. This contrasts with other custom conditions, such as a postcondition, where Terraform produces an error immediately, halting the operation and blocking the application or planning of future resources.\n\nCondition arguments within assert blocks can refer to scoped data sources within the enclosing check block and any variables, resources, data sources, or module outputs within the current module.\n\nLearn more about assertions.\n\nMeta-Arguments\n\nCheck blocks do not currently support meta-arguments. We are still collecting feedback on this feature, so if your use case would benefit from check blocks supporting meta-arguments, please let us know.\n\nContinuous validation in Terraform Cloud\n\nTerraform Cloud can automatically validate whether checks in a workspace’s configuration continue to pass after Terraform provisions new infrastructure. See Continuous Validation for details.\n\nChoosing Checks or other Custom Conditions\n\nCheck blocks offer the most flexible validation solution within Terraform. You can reference outputs, variables, resources, and data sources within check assertions. You can also use checks to model every alternate Custom Condition. However, that does not mean you should replace all your custom conditions with check blocks.\n\nThere are major behavioral differences between check block assertions and other custom conditions, the main one being that check blocks do not affect Terraform's execution of an operation. You can use this non-blocking behavior to decide the best type of validation for your use case.\n\nOutputs and variables\n\nOutput postconditions and variable validations both make assertions around inputs and outputs.\n\nThis is one of the cases where you might want Terraform to block further execution.\n\nFor example, it is not helpful for Terraform to warn that an input variable is invalid after it applies an entire configuration with that input variable. In this case, a check block would warn of the invalid input variable without interrupting the operation. A validation block for the same input variable would alert you of the invalid variable and halt the plan or apply operation.\n\nResource Preconditions and Postconditions\n\nThe difference between preconditions and postconditions and check blocks is more nuanced.\n\nPreconditions are unique amongst the custom conditions in that they execute before a resource change is applied or planned. Choosing Between Preconditions and Postconditions offers guidance on choosing between a precondition and a postcondition, and the same topics also apply to choosing between a precondition and a check block.\n\nYou can often use postconditions interchangeably with check blocks to validate resources and data sources.\n\nFor example, you can rewrite the above check block example to use a postcondition instead. The below code uses a postcondition block to validate that the Terraform website returns the expected status code of 200.\n\ndata \"http\" \"terraform_io\" {\n\n  url = \"https://www.terraform.io\"\n\n\n\n  lifecycle {\n\n    postcondition {\n\n        condition = self.status_code == 200\n\n        error_message = \"${self.url} returned an unhealthy status code\"\n\n    }\n\n  }\n\n}\n\nCopy\n\nBoth the check and postcondition block examples validate that the Terraform website returns a 200 status code during a plan or an apply operation. The difference between the two blocks is how each handles failure.\n\nIf a postcondition block fails, it blocks Terraform from executing the current operation. If a check block fails, it does not block Terraform from executing an operation.\n\nIf the above example's postcondition fails, it is impossible to recover from. Terraform blocks any future plan or apply operations if your postcondition is unsatisfied during the planning stage. This problem occurs because the postcondition does not directly depend on Terraform configuration, but instead on the complex interactions between multiple resources.\n\nWe recommend using check blocks to validate the status of infrastructure as a whole. We only recommend using postconditions when you want a guarantee on a single resource based on that resource's configuration.\n\nOn this page:\n\nChecks\nSyntax\nContinuous validation in Terraform Cloud\nChoosing Checks or other Custom Conditions\nTheme\nDark\nLight\nSystem\nCertifications\nSystem Status\nCookie Manager\nTerms of Use\nSecurity\nPrivacy\nTrademark Policy\nTrade Controls\nGive Feedback\n(opens in new tab)\nWe use cookies & other similar technology to collect data to improve your experience on our site, as described in our Privacy Policy and Cookie Policy.\nManage PreferencesACCEPT"
  },
  {
    "title": "Checks - Configuration Language | Terraform | HashiCorp Developer",
    "url": "https://developer.hashicorp.com/terraform/language/v1.6.x/checks",
    "html": "Skip to main content\nNews\nHashiCorp to join IBM to accelerate multi-cloud automation.\nRead the blog →\nDismiss alert\nTerraform\nInstall\nTutorials\nDocumentation\nRegistry\n(opens in new tab)\nTry Cloud\n(opens in new tab)\nSearch\nCommand or control key\nK key\nTerraform Home\nConfiguration Language\nConfiguration Language\nTerraform Language\nOverview\nStyle Guide\nFiles and Directories\nSyntax\nResources\nData Sources\nProviders\nVariables and Outputs\nModules\nMoved block\nChecks\nImport\nExpressions\nFunctions\nTerraform Settings\nState\nTests\nUpgrading to Terraform v1.6\nv1.x Compatibility Promises\nTerraform Internals\nResources\nTutorial Library\nCertifications\nCommunity Forum\n(opens in new tab)\nSupport\n(opens in new tab)\nGitHub\n(opens in new tab)\nTerraform Registry\n(opens in new tab)\n\nYou are viewing documentation for version v1.6.x. View latest version.\n\nDeveloper\nTerraform\nConfiguration Language v1.6.x\nChecks\nv1.6.x\nChecks\n\nNote: Check blocks are only available in Terraform v1.5.0 and later.\n\nThe check block can validate your infrastructure outside the usual resource lifecycle. Check blocks address a gap between post-apply and functional validation of infrastructure.\n\nHands-on: Try the Validate Infrastructure Using Checks tutorial.\n\nCheck blocks allow you to define custom conditions that execute on every Terraform plan or apply operation without affecting the overall status of an operation. Check blocks execute as the last step of a plan or apply after Terraform has planned or provisioned your infrastructure.\n\nSyntax\n\nYou can declare a check block with a local name, zero-to-one scoped data sources, and one-to-many assertions.\n\nThe following example loads the Terraform website and validates that it returns the expected status code 200.\n\ncheck \"health_check\" {\n\n  data \"http\" \"terraform_io\" {\n\n    url = \"https://www.terraform.io\"\n\n  }\n\n\n\n  assert {\n\n    condition = data.http.terraform_io.status_code == 200\n\n    error_message = \"${data.http.terraform_io.url} returned an unhealthy status code\"\n\n  }\n\n}\n\nCopy\nScoped data sources\n\nYou can use any data source from any provider as a scoped data source within a check block.\n\nA check block can optionally contain a nested (a.k.a. scoped) data source. This data block behaves like an external data source, except you can not reference it outside its enclosing check block. Additionally, if a scoped data source's provider raises any errors, they are masked as warnings and do not prevent Terraform from continuing operation execution.\n\nYou can use a scoped data source to validate the status of a piece of infrastructure outside of the usual Terraform resource lifecycle. In the above example, if the terraform_io data source fails to load, you receive a warning instead of a blocking error, which would occur if you declared this data source outside of a check block.\n\nMeta-Arguments\n\nScoped data sources support the depends_on and provider meta-arguments. Scoped data sources do not support the count orfor_each meta-arguments.\n\ndepends_on\n\nThe depends_on meta-argument can be particularly powerful when used within scoped data sources.\n\nThe first time Terraform creates the initial plan for our previous example, the plan fails because Terraform has not applied its configuration yet. Meaning this test fails because Terraform must still create the resources to make this website exist. Therefore, the first time Terraform runs this check, it always throws a potentially distracting error message.\n\nYou can fix this by adding depends_on to your scoped data source, ensuring it depends on an essential piece of your site's infrastructure, such as the load balancer. The check returns known after apply until that crucial piece of your website is ready. This strategy avoids producing unnecessary warnings during setup, and the check executes during subsequent plans and applies.\n\nOne problem with this strategy is that if the resource your scoped data source depends_on changes, the check block returns known after apply until Terraform has updated that resource. Depending on your use case, this behavior could be acceptable or problematic.\n\nWe recommend implementing the depends_on meta-argument if your scoped data source depends on the existence of another resource without referencing it directly.\n\nAssertions\n\nCheck blocks validate your custom assertions using assert blocks. Each check block must have at least one, but potentially many, assert blocks. Each assert block has a condition attribute and an error_message attribute.\n\nUnlike other custom conditions, assertions do not affect Terraform's execution of an operation. A failed assertion reports a warning without halting the ongoing operation. This contrasts with other custom conditions, such as a postcondition, where Terraform produces an error immediately, halting the operation and blocking the application or planning of future resources.\n\nCondition arguments within assert blocks can refer to scoped data sources within the enclosing check block and any variables, resources, data sources, or module outputs within the current module.\n\nLearn more about assertions.\n\nMeta-Arguments\n\nCheck blocks do not currently support meta-arguments. We are still collecting feedback on this feature, so if your use case would benefit from check blocks supporting meta-arguments, please let us know.\n\nContinuous validation in Terraform Cloud\n\nTerraform Cloud can automatically validate whether checks in a workspace’s configuration continue to pass after Terraform provisions new infrastructure. See Continuous Validation for details.\n\nChoosing Checks or other Custom Conditions\n\nCheck blocks offer the most flexible validation solution within Terraform. You can reference outputs, variables, resources, and data sources within check assertions. You can also use checks to model every alternate Custom Condition. However, that does not mean you should replace all your custom conditions with check blocks.\n\nThere are major behavioral differences between check block assertions and other custom conditions, the main one being that check blocks do not affect Terraform's execution of an operation. You can use this non-blocking behavior to decide the best type of validation for your use case.\n\nOutputs and variables\n\nOutput postconditions and variable validations both make assertions around inputs and outputs.\n\nThis is one of the cases where you might want Terraform to block further execution.\n\nFor example, it is not helpful for Terraform to warn that an input variable is invalid after it applies an entire configuration with that input variable. In this case, a check block would warn of the invalid input variable without interrupting the operation. A validation block for the same input variable would alert you of the invalid variable and halt the plan or apply operation.\n\nResource Preconditions and Postconditions\n\nThe difference between preconditions and postconditions and check blocks is more nuanced.\n\nPreconditions are unique amongst the custom conditions in that they execute before a resource change is applied or planned. Choosing Between Preconditions and Postconditions offers guidance on choosing between a precondition and a postcondition, and the same topics also apply to choosing between a precondition and a check block.\n\nYou can often use postconditions interchangeably with check blocks to validate resources and data sources.\n\nFor example, you can rewrite the above check block example to use a postcondition instead. The below code uses a postcondition block to validate that the Terraform website returns the expected status code of 200.\n\ndata \"http\" \"terraform_io\" {\n\n  url = \"https://www.terraform.io\"\n\n\n\n  lifecycle {\n\n    postcondition {\n\n        condition = self.status_code == 200\n\n        error_message = \"${self.url} returned an unhealthy status code\"\n\n    }\n\n  }\n\n}\n\nCopy\n\nBoth the check and postcondition block examples validate that the Terraform website returns a 200 status code during a plan or an apply operation. The difference between the two blocks is how each handles failure.\n\nIf a postcondition block fails, it blocks Terraform from executing the current operation. If a check block fails, it does not block Terraform from executing an operation.\n\nIf the above example's postcondition fails, it is impossible to recover from. Terraform blocks any future plan or apply operations if your postcondition is unsatisfied during the planning stage. This problem occurs because the postcondition does not directly depend on Terraform configuration, but instead on the complex interactions between multiple resources.\n\nWe recommend using check blocks to validate the status of infrastructure as a whole. We only recommend using postconditions when you want a guarantee on a single resource based on that resource's configuration.\n\nOn this page:\n\nChecks\nSyntax\nContinuous validation in Terraform Cloud\nChoosing Checks or other Custom Conditions\nTheme\nDark\nLight\nSystem\nCertifications\nSystem Status\nCookie Manager\nTerms of Use\nSecurity\nPrivacy\nTrademark Policy\nTrade Controls\nGive Feedback\n(opens in new tab)\nWe use cookies & other similar technology to collect data to improve your experience on our site, as described in our Privacy Policy and Cookie Policy.\nManage PreferencesACCEPT"
  },
  {
    "title": "Checks - Configuration Language | Terraform | HashiCorp Developer",
    "url": "https://developer.hashicorp.com/terraform/language/v1.7.x/checks",
    "html": "Skip to main content\nNews\nHashiCorp to join IBM to accelerate multi-cloud automation.\nRead the blog →\nDismiss alert\nTerraform\nInstall\nTutorials\nDocumentation\nRegistry\n(opens in new tab)\nTry Cloud\n(opens in new tab)\nSearch\nCommand or control key\nK key\nTerraform Home\nConfiguration Language\nConfiguration Language\nTerraform Language\nOverview\nStyle Guide\nFiles and Directories\nSyntax\nResources\nData Sources\nProviders\nVariables and Outputs\nModules\nMoved block\nChecks\nImport\nExpressions\nFunctions\nTerraform Settings\nState\nTests\nUpgrading to Terraform v1.7\nv1.x Compatibility Promises\nTerraform Internals\nResources\nTutorial Library\nCertifications\nCommunity Forum\n(opens in new tab)\nSupport\n(opens in new tab)\nGitHub\n(opens in new tab)\nTerraform Registry\n(opens in new tab)\n\nYou are viewing documentation for version v1.7.x. View latest version.\n\nDeveloper\nTerraform\nConfiguration Language v1.7.x\nChecks\nv1.7.x\nChecks\n\nNote: Check blocks are only available in Terraform v1.5.0 and later.\n\nThe check block can validate your infrastructure outside the usual resource lifecycle. Check blocks address a gap between post-apply and functional validation of infrastructure.\n\nHands-on: Try the Validate Infrastructure Using Checks tutorial.\n\nCheck blocks allow you to define custom conditions that execute on every Terraform plan or apply operation without affecting the overall status of an operation. Check blocks execute as the last step of a plan or apply after Terraform has planned or provisioned your infrastructure.\n\nSyntax\n\nYou can declare a check block with a local name, zero-to-one scoped data sources, and one-to-many assertions.\n\nThe following example loads the Terraform website and validates that it returns the expected status code 200.\n\ncheck \"health_check\" {\n\n  data \"http\" \"terraform_io\" {\n\n    url = \"https://www.terraform.io\"\n\n  }\n\n\n\n  assert {\n\n    condition = data.http.terraform_io.status_code == 200\n\n    error_message = \"${data.http.terraform_io.url} returned an unhealthy status code\"\n\n  }\n\n}\n\nCopy\nScoped data sources\n\nYou can use any data source from any provider as a scoped data source within a check block.\n\nA check block can optionally contain a nested (a.k.a. scoped) data source. This data block behaves like an external data source, except you can not reference it outside its enclosing check block. Additionally, if a scoped data source's provider raises any errors, they are masked as warnings and do not prevent Terraform from continuing operation execution.\n\nYou can use a scoped data source to validate the status of a piece of infrastructure outside of the usual Terraform resource lifecycle. In the above example, if the terraform_io data source fails to load, you receive a warning instead of a blocking error, which would occur if you declared this data source outside of a check block.\n\nMeta-Arguments\n\nScoped data sources support the depends_on and provider meta-arguments. Scoped data sources do not support the count orfor_each meta-arguments.\n\ndepends_on\n\nThe depends_on meta-argument can be particularly powerful when used within scoped data sources.\n\nThe first time Terraform creates the initial plan for our previous example, the plan fails because Terraform has not applied its configuration yet. Meaning this test fails because Terraform must still create the resources to make this website exist. Therefore, the first time Terraform runs this check, it always throws a potentially distracting error message.\n\nYou can fix this by adding depends_on to your scoped data source, ensuring it depends on an essential piece of your site's infrastructure, such as the load balancer. The check returns known after apply until that crucial piece of your website is ready. This strategy avoids producing unnecessary warnings during setup, and the check executes during subsequent plans and applies.\n\nOne problem with this strategy is that if the resource your scoped data source depends_on changes, the check block returns known after apply until Terraform has updated that resource. Depending on your use case, this behavior could be acceptable or problematic.\n\nWe recommend implementing the depends_on meta-argument if your scoped data source depends on the existence of another resource without referencing it directly.\n\nAssertions\n\nCheck blocks validate your custom assertions using assert blocks. Each check block must have at least one, but potentially many, assert blocks. Each assert block has a condition attribute and an error_message attribute.\n\nUnlike other custom conditions, assertions do not affect Terraform's execution of an operation. A failed assertion reports a warning without halting the ongoing operation. This contrasts with other custom conditions, such as a postcondition, where Terraform produces an error immediately, halting the operation and blocking the application or planning of future resources.\n\nCondition arguments within assert blocks can refer to scoped data sources within the enclosing check block and any variables, resources, data sources, or module outputs within the current module.\n\nLearn more about assertions.\n\nMeta-Arguments\n\nCheck blocks do not currently support meta-arguments. We are still collecting feedback on this feature, so if your use case would benefit from check blocks supporting meta-arguments, please let us know.\n\nContinuous validation in Terraform Cloud\n\nTerraform Cloud can automatically validate whether checks in a workspace’s configuration continue to pass after Terraform provisions new infrastructure. See Continuous Validation for details.\n\nChoosing Checks or other Custom Conditions\n\nCheck blocks offer the most flexible validation solution within Terraform. You can reference outputs, variables, resources, and data sources within check assertions. You can also use checks to model every alternate Custom Condition. However, that does not mean you should replace all your custom conditions with check blocks.\n\nThere are major behavioral differences between check block assertions and other custom conditions, the main one being that check blocks do not affect Terraform's execution of an operation. You can use this non-blocking behavior to decide the best type of validation for your use case.\n\nOutputs and variables\n\nOutput postconditions and variable validations both make assertions around inputs and outputs.\n\nThis is one of the cases where you might want Terraform to block further execution.\n\nFor example, it is not helpful for Terraform to warn that an input variable is invalid after it applies an entire configuration with that input variable. In this case, a check block would warn of the invalid input variable without interrupting the operation. A validation block for the same input variable would alert you of the invalid variable and halt the plan or apply operation.\n\nResource Preconditions and Postconditions\n\nThe difference between preconditions and postconditions and check blocks is more nuanced.\n\nPreconditions are unique amongst the custom conditions in that they execute before a resource change is applied or planned. Choosing Between Preconditions and Postconditions offers guidance on choosing between a precondition and a postcondition, and the same topics also apply to choosing between a precondition and a check block.\n\nYou can often use postconditions interchangeably with check blocks to validate resources and data sources.\n\nFor example, you can rewrite the above check block example to use a postcondition instead. The below code uses a postcondition block to validate that the Terraform website returns the expected status code of 200.\n\ndata \"http\" \"terraform_io\" {\n\n  url = \"https://www.terraform.io\"\n\n\n\n  lifecycle {\n\n    postcondition {\n\n        condition = self.status_code == 200\n\n        error_message = \"${self.url} returned an unhealthy status code\"\n\n    }\n\n  }\n\n}\n\nCopy\n\nBoth the check and postcondition block examples validate that the Terraform website returns a 200 status code during a plan or an apply operation. The difference between the two blocks is how each handles failure.\n\nIf a postcondition block fails, it blocks Terraform from executing the current operation. If a check block fails, it does not block Terraform from executing an operation.\n\nIf the above example's postcondition fails, it is impossible to recover from. Terraform blocks any future plan or apply operations if your postcondition is unsatisfied during the planning stage. This problem occurs because the postcondition does not directly depend on Terraform configuration, but instead on the complex interactions between multiple resources.\n\nWe recommend using check blocks to validate the status of infrastructure as a whole. We only recommend using postconditions when you want a guarantee on a single resource based on that resource's configuration.\n\nOn this page:\n\nChecks\nSyntax\nContinuous validation in Terraform Cloud\nChoosing Checks or other Custom Conditions\nTheme\nDark\nLight\nSystem\nCertifications\nSystem Status\nCookie Manager\nTerms of Use\nSecurity\nPrivacy\nTrademark Policy\nTrade Controls\nGive Feedback\n(opens in new tab)\nWe use cookies & other similar technology to collect data to improve your experience on our site, as described in our Privacy Policy and Cookie Policy.\nManage PreferencesACCEPT"
  },
  {
    "title": "Checks - Configuration Language | Terraform | HashiCorp Developer",
    "url": "https://developer.hashicorp.com/terraform/language/v1.9.x/checks",
    "html": "Skip to main content\nNews\nHashiCorp to join IBM to accelerate multi-cloud automation.\nRead the blog →\nDismiss alert\nTerraform\nInstall\nTutorials\nDocumentation\nRegistry\n(opens in new tab)\nTry Cloud\n(opens in new tab)\nSearch\nCommand or control key\nK key\nTerraform Home\nConfiguration Language\nConfiguration Language\nTerraform Language\nOverview\nStyle Guide\nFiles and Directories\nSyntax\nResources\nData Sources\nProviders\nVariables and Outputs\nModules\nChecks\nImport\nExpressions\nFunctions\nTerraform Settings\nState\nTests\nUpgrading to Terraform v1.7\nv1.x Compatibility Promises\nTerraform Internals\nResources\nTutorial Library\nCertifications\nCommunity Forum\n(opens in new tab)\nSupport\n(opens in new tab)\nGitHub\n(opens in new tab)\nTerraform Registry\n(opens in new tab)\n\nYou are viewing documentation for pre-release version v1.9.0 (alpha). View latest version.\n\nDeveloper\nTerraform\nConfiguration Language v1.9.x\nChecks\nv1.9.0 (alpha)\nChecks\n\nNote: Check blocks are only available in Terraform v1.5.0 and later.\n\nThe check block can validate your infrastructure outside the usual resource lifecycle. Check blocks address a gap between post-apply and functional validation of infrastructure.\n\nHands-on: Try the Validate Infrastructure Using Checks tutorial.\n\nCheck blocks allow you to define custom conditions that execute on every Terraform plan or apply operation without affecting the overall status of an operation. Check blocks execute as the last step of a plan or apply after Terraform has planned or provisioned your infrastructure.\n\nSyntax\n\nYou can declare a check block with a local name, zero-to-one scoped data sources, and one-to-many assertions.\n\nThe following example loads the Terraform website and validates that it returns the expected status code 200.\n\ncheck \"health_check\" {\n\n  data \"http\" \"terraform_io\" {\n\n    url = \"https://www.terraform.io\"\n\n  }\n\n\n\n  assert {\n\n    condition = data.http.terraform_io.status_code == 200\n\n    error_message = \"${data.http.terraform_io.url} returned an unhealthy status code\"\n\n  }\n\n}\n\nCopy\nScoped data sources\n\nYou can use any data source from any provider as a scoped data source within a check block.\n\nA check block can optionally contain a nested (a.k.a. scoped) data source. This data block behaves like an external data source, except you can not reference it outside its enclosing check block. Additionally, if a scoped data source's provider raises any errors, they are masked as warnings and do not prevent Terraform from continuing operation execution.\n\nYou can use a scoped data source to validate the status of a piece of infrastructure outside of the usual Terraform resource lifecycle. In the above example, if the terraform_io data source fails to load, you receive a warning instead of a blocking error, which would occur if you declared this data source outside of a check block.\n\nMeta-Arguments\n\nScoped data sources support the depends_on and provider meta-arguments. Scoped data sources do not support the count orfor_each meta-arguments.\n\ndepends_on\n\nThe depends_on meta-argument can be particularly powerful when used within scoped data sources.\n\nThe first time Terraform creates the initial plan for our previous example, the plan fails because Terraform has not applied its configuration yet. Meaning this test fails because Terraform must still create the resources to make this website exist. Therefore, the first time Terraform runs this check, it always throws a potentially distracting error message.\n\nYou can fix this by adding depends_on to your scoped data source, ensuring it depends on an essential piece of your site's infrastructure, such as the load balancer. The check returns known after apply until that crucial piece of your website is ready. This strategy avoids producing unnecessary warnings during setup, and the check executes during subsequent plans and applies.\n\nOne problem with this strategy is that if the resource your scoped data source depends_on changes, the check block returns known after apply until Terraform has updated that resource. Depending on your use case, this behavior could be acceptable or problematic.\n\nWe recommend implementing the depends_on meta-argument if your scoped data source depends on the existence of another resource without referencing it directly.\n\nAssertions\n\nCheck blocks validate your custom assertions using assert blocks. Each check block must have at least one, but potentially many, assert blocks. Each assert block has a condition attribute and an error_message attribute.\n\nUnlike other custom conditions, assertions do not affect Terraform's execution of an operation. A failed assertion reports a warning without halting the ongoing operation. This contrasts with other custom conditions, such as a postcondition, where Terraform produces an error immediately, halting the operation and blocking the application or planning of future resources.\n\nCondition arguments within assert blocks can refer to scoped data sources within the enclosing check block and any variables, resources, data sources, or module outputs within the current module.\n\nLearn more about assertions.\n\nMeta-Arguments\n\nCheck blocks do not currently support meta-arguments. We are still collecting feedback on this feature, so if your use case would benefit from check blocks supporting meta-arguments, please let us know.\n\nContinuous validation in HCP Terraform\n\nHCP Terraform can automatically validate whether checks in a workspace’s configuration continue to pass after Terraform provisions new infrastructure. See Continuous Validation for details.\n\nChoosing Checks or other Custom Conditions\n\nCheck blocks offer the most flexible validation solution within Terraform. You can reference outputs, variables, resources, and data sources within check assertions. You can also use checks to model every alternate Custom Condition. However, that does not mean you should replace all your custom conditions with check blocks.\n\nThere are major behavioral differences between check block assertions and other custom conditions, the main one being that check blocks do not affect Terraform's execution of an operation. You can use this non-blocking behavior to decide the best type of validation for your use case.\n\nOutputs and variables\n\nOutput postconditions and variable validations both make assertions around inputs and outputs.\n\nThis is one of the cases where you might want Terraform to block further execution.\n\nFor example, it is not helpful for Terraform to warn that an input variable is invalid after it applies an entire configuration with that input variable. In this case, a check block would warn of the invalid input variable without interrupting the operation. A validation block for the same input variable would alert you of the invalid variable and halt the plan or apply operation.\n\nResource Preconditions and Postconditions\n\nThe difference between preconditions and postconditions and check blocks is more nuanced.\n\nPreconditions are unique amongst the custom conditions in that they execute before a resource change is applied or planned. Choosing Between Preconditions and Postconditions offers guidance on choosing between a precondition and a postcondition, and the same topics also apply to choosing between a precondition and a check block.\n\nYou can often use postconditions interchangeably with check blocks to validate resources and data sources.\n\nFor example, you can rewrite the above check block example to use a postcondition instead. The below code uses a postcondition block to validate that the Terraform website returns the expected status code of 200.\n\ndata \"http\" \"terraform_io\" {\n\n  url = \"https://www.terraform.io\"\n\n\n\n  lifecycle {\n\n    postcondition {\n\n        condition = self.status_code == 200\n\n        error_message = \"${self.url} returned an unhealthy status code\"\n\n    }\n\n  }\n\n}\n\nCopy\n\nBoth the check and postcondition block examples validate that the Terraform website returns a 200 status code during a plan or an apply operation. The difference between the two blocks is how each handles failure.\n\nIf a postcondition block fails, it blocks Terraform from executing the current operation. If a check block fails, it does not block Terraform from executing an operation.\n\nIf the above example's postcondition fails, it is impossible to recover from. Terraform blocks any future plan or apply operations if your postcondition is unsatisfied during the planning stage. This problem occurs because the postcondition does not directly depend on Terraform configuration, but instead on the complex interactions between multiple resources.\n\nWe recommend using check blocks to validate the status of infrastructure as a whole. We only recommend using postconditions when you want a guarantee on a single resource based on that resource's configuration.\n\nOn this page:\n\nChecks\nSyntax\nContinuous validation in HCP Terraform\nChoosing Checks or other Custom Conditions\nTheme\nDark\nLight\nSystem\nCertifications\nSystem Status\nCookie Manager\nTerms of Use\nSecurity\nPrivacy\nTrademark Policy\nTrade Controls\nGive Feedback\n(opens in new tab)\nWe use cookies & other similar technology to collect data to improve your experience on our site, as described in our Privacy Policy and Cookie Policy.\nManage PreferencesACCEPT"
  },
  {
    "title": "The lifecycle Meta-Argument - Configuration Language | Terraform | HashiCorp Developer",
    "url": "https://developer.hashicorp.com/terraform/language/meta-arguments/lifecycle",
    "html": "Skip to main content\nNews\nHashiCorp to join IBM to accelerate multi-cloud automation.\nRead the blog →\nDismiss alert\nTerraform\nInstall\nTutorials\nDocumentation\nRegistry\n(opens in new tab)\nTry Cloud\n(opens in new tab)\nSearch\nCommand or control key\nK key\nTerraform Home\nConfiguration Language\nConfiguration Language\nStyle Guide\nFiles and Directories\nSyntax\nResources\nOverview\nResource Blocks\nResource Behavior\nMeta-Arguments\ndepends_on\ncount\nfor_each\nprovider\nlifecycle\nProvisioners\nThe terraform_data Resource Type\nData Sources\nProviders\nVariables and Outputs\nModules\nMoved block\nChecks\nImport\nExpressions\nFunctions\nTerraform Settings\nState\nTests\nUpgrading to Terraform v1.7\nv1.x Compatibility Promises\nTerraform Internals\nResources\nTutorial Library\nCertifications\nCommunity Forum\n(opens in new tab)\nSupport\n(opens in new tab)\nGitHub\n(opens in new tab)\nTerraform Registry\n(opens in new tab)\nDeveloper\nTerraform\nConfiguration Language\nMeta-Arguments\nlifecycle\nv1.8.x (latest)\nThe lifecycle Meta-Argument\n\nHands-on: Try the Lifecycle Management tutorial.\n\nThe Resource Behavior page describes the general lifecycle for resources. Some details of that behavior can be customized using the special nested lifecycle block within a resource block body:\n\nresource \"azurerm_resource_group\" \"example\" {\n\n  # ...\n\n\n\n  lifecycle {\n\n    create_before_destroy = true\n\n  }\n\n}\n\nCopy\nSyntax and Arguments\n\nlifecycle is a nested block that can appear within a resource block. The lifecycle block and its contents are meta-arguments, available for all resource blocks regardless of type.\n\nThe arguments available within a lifecycle block are create_before_destroy, prevent_destroy, ignore_changes, and replace_triggered_by.\n\ncreate_before_destroy (bool) - By default, when Terraform must change a resource argument that cannot be updated in-place due to remote API limitations, Terraform will instead destroy the existing object and then create a new replacement object with the new configured arguments.\n\nThe create_before_destroy meta-argument changes this behavior so that the new replacement object is created first, and the prior object is destroyed after the replacement is created.\n\nThis is an opt-in behavior because many remote object types have unique name requirements or other constraints that must be accommodated for both a new and an old object to exist concurrently. Some resource types offer special options to append a random suffix onto each object name to avoid collisions, for example. Terraform CLI cannot automatically activate such features, so you must understand the constraints for each resource type before using create_before_destroy with it.\n\nNote that Terraform propagates and applies the create_before_destroy meta-attribute behaviour to all resource dependencies. For example, if create_before_destroy is enabled on resource A but not on resource B, but resource A is dependent on resource B, then Terraform enables create_before_destroy for resource B implicitly by default and stores it to the state file. You cannot override create_before_destroy to false on resource B because that would imply dependency cycles in the graph.\n\nDestroy provisioners of this resource do not run if create_before_destroy is set to true. This GitHub issue contains more details.\n\nprevent_destroy (bool) - This meta-argument, when set to true, will cause Terraform to reject with an error any plan that would destroy the infrastructure object associated with the resource, as long as the argument remains present in the configuration.\n\nThis can be used as a measure of safety against the accidental replacement of objects that may be costly to reproduce, such as database instances. However, it will make certain configuration changes impossible to apply, and will prevent the use of the terraform destroy command once such objects are created, and so this option should be used sparingly.\n\nSince this argument must be present in configuration for the protection to apply, note that this setting does not prevent the remote object from being destroyed if the resource block were removed from configuration entirely: in that case, the prevent_destroy setting is removed along with it, and so Terraform will allow the destroy operation to succeed.\n\nignore_changes (list of attribute names) - By default, Terraform detects any difference in the current settings of a real infrastructure object and plans to update the remote object to match configuration.\n\nThe ignore_changes feature is intended to be used when a resource is created with references to data that may change in the future, but should not affect said resource after its creation. In some rare cases, settings of a remote object are modified by processes outside of Terraform, which Terraform would then attempt to \"fix\" on the next run. In order to make Terraform share management responsibilities of a single object with a separate process, the ignore_changes meta-argument specifies resource attributes that Terraform should ignore when planning updates to the associated remote object.\n\nThe arguments corresponding to the given attribute names are considered when planning a create operation, but are ignored when planning an update. The arguments are the relative address of the attributes in the resource. Map and list elements can be referenced using index notation, like tags[\"Name\"] and list[0] respectively.\n\nresource \"aws_instance\" \"example\" {\n\n  # ...\n\n\n\n  lifecycle {\n\n    ignore_changes = [\n\n      # Ignore changes to tags, e.g. because a management agent\n\n      # updates these based on some ruleset managed elsewhere.\n\n      tags,\n\n    ]\n\n  }\n\n}\n\nCopy\n\nInstead of a list, the special keyword all may be used to instruct Terraform to ignore all attributes, which means that Terraform can create and destroy the remote object but will never propose updates to it.\n\nOnly attributes defined by the resource type can be ignored. ignore_changes cannot be applied to itself or to any other meta-arguments.\n\nreplace_triggered_by (list of resource or attribute references) - Added in Terraform 1.2. Replaces the resource when any of the referenced items change. Supply a list of expressions referencing managed resources, instances, or instance attributes. When used in a resource that uses count or for_each, you can use count.index or each.key in the expression to reference specific instances of other resources that are configured with the same count or collection.\n\nReferences trigger replacement in the following conditions:\n\nIf the reference is to a resource with multiple instances, a plan to update or replace any instance will trigger replacement.\nIf the reference is to a single resource instance, a plan to update or replace that instance will trigger replacement.\nIf the reference is to a single attribute of a resource instance, any change to the attribute value will trigger replacement.\n\nYou can only reference managed resources in replace_triggered_by expressions. This lets you modify these expressions without forcing replacement.\n\nresource \"aws_appautoscaling_target\" \"ecs_target\" {\n\n  # ...\n\n  lifecycle {\n\n    replace_triggered_by = [\n\n      # Replace `aws_appautoscaling_target` each time this instance of\n\n      # the `aws_ecs_service` is replaced.\n\n      aws_ecs_service.svc.id\n\n    ]\n\n  }\n\n}\n\nCopy\n\nreplace_triggered_by allows only resource addresses because the decision is based on the planned actions for all of the given resources. Plain values such as local values or input variables do not have planned actions of their own, but you can treat them with a resource-like lifecycle by using them with the terraform_data resource type.\n\nCustom Condition Checks\n\nYou can add precondition and postcondition blocks with a lifecycle block to specify assumptions and guarantees about how resources and data sources operate. The following examples creates a precondition that checks whether the AMI is properly configured.\n\nresource \"aws_instance\" \"example\" {\n\n  instance_type = \"t2.micro\"\n\n  ami           = \"ami-abc123\"\n\n\n\n  lifecycle {\n\n    # The AMI ID must refer to an AMI that contains an operating system\n\n    # for the `x86_64` architecture.\n\n    precondition {\n\n      condition     = data.aws_ami.example.architecture == \"x86_64\"\n\n      error_message = \"The selected AMI must be for the x86_64 architecture.\"\n\n    }\n\n  }\n\n}\n\nCopy\n\nCustom conditions can help capture assumptions, helping future maintainers understand the configuration design and intent. They also return useful information about errors earlier and in context, helping consumers more easily diagnose issues in their configurations.\n\nRefer to Custom Conditions for more details.\n\nLiteral Values Only\n\nThe lifecycle settings all affect how Terraform constructs and traverses the dependency graph. As a result, only literal values can be used because the processing happens too early for arbitrary expression evaluation.\n\nEdit this page on GitHub\n\nOn this page:\n\nThe lifecycle Meta-Argument\nSyntax and Arguments\nCustom Condition Checks\nLiteral Values Only\nTheme\nDark\nLight\nSystem\nCertifications\nSystem Status\nCookie Manager\nTerms of Use\nSecurity\nPrivacy\nTrademark Policy\nTrade Controls\nGive Feedback\n(opens in new tab)\nWe use cookies & other similar technology to collect data to improve your experience on our site, as described in our Privacy Policy and Cookie Policy.\nManage PreferencesACCEPT"
  },
  {
    "title": "Resources - Configuration Language | Terraform | HashiCorp Developer",
    "url": "https://developer.hashicorp.com/terraform/language/resources/syntax#meta-arguments",
    "html": "Skip to main content\nNews\nHashiCorp to join IBM to accelerate multi-cloud automation.\nRead the blog →\nDismiss alert\nTerraform\nInstall\nTutorials\nDocumentation\nRegistry\n(opens in new tab)\nTry Cloud\n(opens in new tab)\nSearch\nCommand or control key\nK key\nTerraform Home\nConfiguration Language\nConfiguration Language\nStyle Guide\nFiles and Directories\nSyntax\nResources\nOverview\nResource Blocks\nResource Behavior\nMeta-Arguments\nProvisioners\nThe terraform_data Resource Type\nData Sources\nProviders\nVariables and Outputs\nModules\nMoved block\nChecks\nImport\nExpressions\nFunctions\nTerraform Settings\nState\nTests\nUpgrading to Terraform v1.7\nv1.x Compatibility Promises\nTerraform Internals\nResources\nTutorial Library\nCertifications\nCommunity Forum\n(opens in new tab)\nSupport\n(opens in new tab)\nGitHub\n(opens in new tab)\nTerraform Registry\n(opens in new tab)\nDeveloper\nTerraform\nConfiguration Language\nResources\nResource Blocks\nv1.8.x (latest)\nResource Blocks\n\nHands-on: Try the Terraform: Get Started tutorials.\n\nResources are the most important element in the Terraform language. Each resource block describes one or more infrastructure objects, such as virtual networks, compute instances, or higher-level components such as DNS records.\n\nFor information about how Terraform manages resources after applying a configuration, refer to Resource Behavior.\n\nResource Syntax\n\nA resource block declares a resource of a specific type with a specific local name. Terraform uses the name when referring to the resource in the same module, but it has no meaning outside that module's scope.\n\nIn the following example, the aws_instance resource type is named web. The resource type and name must be unique within a module because they serve as an identifier for a given resource.\n\nresource \"aws_instance\" \"web\" {\n\n  ami           = \"ami-a1b2c3d4\"\n\n  instance_type = \"t2.micro\"\n\n}\n\nCopy\n\nWithin the block body (between { and }) are the configuration arguments for the resource itself. The arguments often depend on the resource type. In this example, both ami and instance_type are special arguments for the aws_instance resource type.\n\nNote: Resource names must start with a letter or underscore, and may contain only letters, digits, underscores, and dashes.\n\nResource declarations can include more advanced features, such as single resource declarations that produce multiple similar remote objects, but only a small subset is required for initial use.\n\nResource Types\n\nEach resource is associated with a single resource type, which determines the kind of infrastructure object it manages and what arguments and other attributes the resource supports.\n\nProviders\n\nA provider is a plugin for Terraform that offers a collection of resource types. Each resource type is implemented by a provider. A provider provides resources to manage a single cloud or on-premises infrastructure platform. Providers are distributed separately from Terraform, but Terraform can automatically install most providers when initializing a working directory.\n\nTo manage resources, a Terraform module must specify the required providers. Refer to Provider Requirements for additional information.\n\nMost providers need some configuration to access their remote API, which is provided by the root module. Refer to Provider Configuration for additional information.\n\nBased on a resource type's name, Terraform can usually determine which provider to use. By convention, resource type names start with their provider's preferred local name. When using multiple configurations of a provider or non-preferred local provider names, you must use the provider meta-argument to manually choose a provider configuration.\n\nResource Arguments\n\nMost of the arguments within the body of a resource block are specific to the selected resource type. The resource type's documentation lists which arguments are available and how their values should be formatted.\n\nThe values for resource arguments can make full use of expressions and other dynamic Terraform language features.\n\nMeta-arguments are defined by Terraform and apply across all resource types.\n\nDocumentation for Resource Types\n\nEvery Terraform provider has its own documentation, describing its resource types and their arguments.\n\nSome provider documentation is still part of Terraform's core documentation, but the Terraform Registry is the main home for all publicly available provider docs.\n\nWhen viewing a provider's page on the Terraform Registry, you can click the Documentation link in the header to browse its documentation. The documentation is versioned. To choose a different version of the provider documentation, click on the version in the provider breadcrumbs to choose a version from the drop-down menu.\n\nMeta-Arguments\n\nThe Terraform language defines the following meta-arguments, which can be used with any resource type to change the behavior of resources:\n\ndepends_on, for specifying hidden dependencies\ncount, for creating multiple resource instances according to a count\nfor_each, to create multiple instances according to a map, or set of strings\nprovider, for selecting a non-default provider configuration\nlifecycle, for lifecycle customizations\nprovisioner, for taking extra actions after resource creation\nRemoving Resources\n\nNote: The removed block is available in Terraform v1.7 and later. For earlier Terraform versions, you can use the terraform state rm CLI command as a separate step.\n\nTo remove a resource from Terraform, simply delete the resource block from your Terraform configuration.\n\nBy default, after you remove the resource block, Terraform will plan to destroy any real infrastructure object managed by that resource.\n\nSometimes you may wish to remove a resource from your Terraform configuration without destroying the real infrastructure object it manages. In this case, the resource will be removed from the Terraform state, but the real infrastructure object will not be destroyed.\n\nTo declare that a resource was removed from Terraform configuration but that its managed object should not be destroyed, remove the resource block from your configuration and replace it with a removed block:\n\nremoved {\n\n  from = aws_instance.example\n\n\n\n  lifecycle {\n\n    destroy = false\n\n  }\n\n}\n\nCopy\n\nThe from argument is the address of the resource you want to remove, without any instance keys (such as \"aws_instance.example[1]\").\n\nThe lifecycle block is required. The destroy argument determines whether Terraform will attempt to destroy the object managed by the resource or not. A value of false means that Terraform will remove the resource from state without destroying it.\n\nCustom Condition Checks\n\nYou can use precondition and postcondition blocks to specify assumptions and guarantees about how the resource operates. The following example creates a precondition that checks whether the AMI is properly configured.\n\nresource \"aws_instance\" \"example\" {\n\n  instance_type = \"t2.micro\"\n\n  ami           = \"ami-abc123\"\n\n\n\n  lifecycle {\n\n    # The AMI ID must refer to an AMI that contains an operating system\n\n    # for the `x86_64` architecture.\n\n    precondition {\n\n      condition     = data.aws_ami.example.architecture == \"x86_64\"\n\n      error_message = \"The selected AMI must be for the x86_64 architecture.\"\n\n    }\n\n  }\n\n}\n\nCopy\n\nCustom condition checks can help capture assumptions so that future maintainers understand the configuration design and intent. They also return useful information about errors earlier and in context, helping consumers to diagnose issues in their configuration.\n\nOperation Timeouts\n\nSome resource types provide a special timeouts nested block argument that allows you to customize how long certain operations are allowed to take before being considered to have failed. For example, aws_db_instance allows configurable timeouts for create, update, and delete operations.\n\nTimeouts are handled entirely by the resource type implementation in the provider, but resource types offering these features follow the convention of defining a child block called timeouts that has a nested argument named after each operation that has a configurable timeout value. Each of these arguments takes a string representation of a duration, such as \"60m\" for 60 minutes, \"10s\" for ten seconds, or \"2h\" for two hours.\n\nresource \"aws_db_instance\" \"example\" {\n\n  # ...\n\n\n\n  timeouts {\n\n    create = \"60m\"\n\n    delete = \"2h\"\n\n  }\n\n}\n\nCopy\n\nThe set of configurable operations is chosen by each resource type. Most resource types do not support the timeouts block at all. Consult the documentation for each resource type to see which operations it offers for configuration, if any.\n\nEdit this page on GitHub\n\nOn this page:\n\nResource Blocks\nResource Syntax\nResource Types\nMeta-Arguments\nRemoving Resources\nCustom Condition Checks\nOperation Timeouts\nTheme\nDark\nLight\nSystem\nCertifications\nSystem Status\nCookie Manager\nTerms of Use\nSecurity\nPrivacy\nTrademark Policy\nTrade Controls\nGive Feedback\n(opens in new tab)\nWe use cookies & other similar technology to collect data to improve your experience on our site, as described in our Privacy Policy and Cookie Policy.\nManage PreferencesACCEPT"
  },
  {
    "title": "The Resource provider Meta-Argument - Configuration Language | Terraform | HashiCorp Developer",
    "url": "https://developer.hashicorp.com/terraform/language/meta-arguments/resource-provider",
    "html": "Skip to main content\nNews\nHashiCorp to join IBM to accelerate multi-cloud automation.\nRead the blog →\nDismiss alert\nTerraform\nInstall\nTutorials\nDocumentation\nRegistry\n(opens in new tab)\nTry Cloud\n(opens in new tab)\nSearch\nCommand or control key\nK key\nTerraform Home\nConfiguration Language\nConfiguration Language\nStyle Guide\nFiles and Directories\nSyntax\nResources\nOverview\nResource Blocks\nResource Behavior\nMeta-Arguments\ndepends_on\ncount\nfor_each\nprovider\nlifecycle\nProvisioners\nThe terraform_data Resource Type\nData Sources\nProviders\nVariables and Outputs\nModules\nMoved block\nChecks\nImport\nExpressions\nFunctions\nTerraform Settings\nState\nTests\nUpgrading to Terraform v1.7\nv1.x Compatibility Promises\nTerraform Internals\nResources\nTutorial Library\nCertifications\nCommunity Forum\n(opens in new tab)\nSupport\n(opens in new tab)\nGitHub\n(opens in new tab)\nTerraform Registry\n(opens in new tab)\nDeveloper\nTerraform\nConfiguration Language\nMeta-Arguments\nresource-provider\nv1.8.x (latest)\nThe Resource provider Meta-Argument\n\nThe provider meta-argument specifies which provider configuration to use for a resource, overriding Terraform's default behavior of selecting one based on the resource type name. Its value should be an unquoted <PROVIDER>.<ALIAS> reference.\n\nAs described in Provider Configuration, you can optionally create multiple configurations for a single provider (usually to manage resources in different regions of multi-region services). Each provider can have one default configuration, and any number of alternate configurations that include an extra name segment (or \"alias\").\n\nBy default, Terraform interprets the initial word in the resource type name (separated by underscores) as the local name of a provider, and uses that provider's default configuration. For example, the resource type google_compute_instance is associated automatically with the default configuration for the provider named google.\n\nBy using the provider meta-argument, you can select an alternate provider configuration for a resource:\n\n# default configuration\n\nprovider \"google\" {\n\n  region = \"us-central1\"\n\n}\n\n\n\n# alternate configuration, whose alias is \"europe\"\n\nprovider \"google\" {\n\n  alias  = \"europe\"\n\n  region = \"europe-west1\"\n\n}\n\n\n\nresource \"google_compute_instance\" \"example\" {\n\n  # This \"provider\" meta-argument selects the google provider\n\n  # configuration whose alias is \"europe\", rather than the\n\n  # default configuration.\n\n  provider = google.europe\n\n\n\n  # ...\n\n}\n\nCopy\n\nA resource always has an implicit dependency on its associated provider, to ensure that the provider is fully configured before any resource actions are taken.\n\nThe provider meta-argument expects a <PROVIDER>.<ALIAS> reference, which does not need to be quoted. Arbitrary expressions are not permitted for provider because it must be resolved while Terraform is constructing the dependency graph, before it is safe to evaluate expressions.\n\nEdit this page on GitHub\nTheme\nDark\nLight\nSystem\nCertifications\nSystem Status\nCookie Manager\nTerms of Use\nSecurity\nPrivacy\nTrademark Policy\nTrade Controls\nGive Feedback\n(opens in new tab)\nWe use cookies & other similar technology to collect data to improve your experience on our site, as described in our Privacy Policy and Cookie Policy.\nManage PreferencesACCEPT"
  },
  {
    "title": "Custom Conditions - Configuration Language | Terraform | HashiCorp Developer",
    "url": "https://developer.hashicorp.com/terraform/language/expressions/custom-conditions#preconditions-and-postconditions",
    "html": "Skip to main content\nNews\nHashiCorp to join IBM to accelerate multi-cloud automation.\nRead the blog →\nDismiss alert\nTerraform\nInstall\nTutorials\nDocumentation\nRegistry\n(opens in new tab)\nTry Cloud\n(opens in new tab)\nSearch\nCommand or control key\nK key\nTerraform Home\nConfiguration Language\nConfiguration Language\nStyle Guide\nFiles and Directories\nSyntax\nResources\nData Sources\nProviders\nVariables and Outputs\nModules\nMoved block\nChecks\nImport\nExpressions\nOverview\nTypes and Values\nStrings and Templates\nReferences to Values\nOperators\nFunction Calls\nConditional Expressions\nFor Expressions\nSplat Expressions\nDynamic Blocks\nCustom Conditions\nType Constraints\nVersion Constraints\nFunctions\nTerraform Settings\nState\nTests\nUpgrading to Terraform v1.7\nv1.x Compatibility Promises\nTerraform Internals\nResources\nTutorial Library\nCertifications\nCommunity Forum\n(opens in new tab)\nSupport\n(opens in new tab)\nGitHub\n(opens in new tab)\nTerraform Registry\n(opens in new tab)\nDeveloper\nTerraform\nConfiguration Language\nExpressions\nCustom Conditions\nv1.8.x (latest)\nCustom Conditions\n\nYou can create conditions that produce custom error messages for several types of objects in a configuration. For example, you can add a condition to an input variable that checks whether incoming image IDs are formatted properly. Custom conditions can capture assumptions, helping future maintainers understand the configuration design and intent. They also return useful information about errors earlier and in context, helping consumers more easily diagnose issues in their configurations.\n\nHands On: Try the Validate Infrastructure Using Checks tutorial to learn how to use check blocks. Try the Validate Modules with Custom Conditions tutorial to learn how to use other custom conditions.\n\nThis page explains the following:\n\nCreating checks with assertions to verify your infrastructure as a whole (Terraform v1.5.0 and later)\nCreating validation conditions for input variables (Terraform v0.13.0 and later)\nCreating preconditions and postconditions for resources, data sources, and outputs (Terraform v1.2.0 and later)\nWriting effective condition expressions and error messages\nWhen Terraform evaluates custom conditions during the plan and apply cycle\nSelecting a Custom Condition for your use case\n\nTerraform's different custom conditions are best suited to various situations. Use the following broad guidelines to select the best custom condition for your use case:\n\nCheck blocks with assertions validate your infrastructure as a whole. Additionally, check blocks do not prevent or block the overall execution of Terraform operations.\nValidation conditions or output postconditions can ensure your configuration's inputs and outputs meet specific requirements.\nResource preconditions and postconditions can validate that Terraform produces your configuration with predictable results.\n\nFor more information on when to use certain custom conditions, see Choosing Between Preconditions and Postconditions and Choosing Checks or Other Custom Conditions.\n\nInput Variable Validation\n\nNote: Input variable validation is available in Terraform v0.13.0 and later.\n\nAdd one or more validation blocks within the variable block to specify custom conditions. Each validation requires a condition argument, an expression that must use the value of the variable to return true if the value is valid, or false if it is invalid. The expression can refer only to the containing variable and must not produce errors.\n\nIf the condition evaluates to false, Terraform produces an error message that includes the result of the error_message expression. If you declare multiple validations, Terraform returns error messages for all failed conditions.\n\nThe following example checks whether the AMI ID has valid syntax.\n\nvariable \"image_id\" {\n\n  type        = string\n\n  description = \"The id of the machine image (AMI) to use for the server.\"\n\n\n\n  validation {\n\n    condition     = length(var.image_id) > 4 && substr(var.image_id, 0, 4) == \"ami-\"\n\n    error_message = \"The image_id value must be a valid AMI id, starting with \\\"ami-\\\".\"\n\n  }\n\n}\n\nCopy\n\nIf the failure of an expression determines the validation decision, use the can function as demonstrated in the following example.\n\nvariable \"image_id\" {\n\n  type        = string\n\n  description = \"The id of the machine image (AMI) to use for the server.\"\n\n\n\n  validation {\n\n    # regex(...) fails if it cannot find a match\n\n    condition     = can(regex(\"^ami-\", var.image_id))\n\n    error_message = \"The image_id value must be a valid AMI id, starting with \\\"ami-\\\".\"\n\n  }\n\n}\n\nCopy\nPreconditions and Postconditions\n\nNote: Preconditions and postconditions are available in Terraform v1.2.0 and later.\n\nUse precondition and postcondition blocks to create custom rules for resources, data sources, and outputs.\n\nTerraform checks a precondition before evaluating the object it is associated with and checks a postcondition after evaluating the object. Terraform evaluates custom conditions as early as possible, but must defer conditions that depend on unknown values until the apply phase. Refer to Conditions Checked Only During Apply for more details.\n\nUsage\n\nEach precondition and postcondition requires a condition argument. This is an expression that must return true if the conditition is fufilled or false if it is invalid. The expression can refer to any other objects in the same module, as long as the references do not create cyclic dependencies. Resource postconditions can also use the self object to refer to attributes of each instance of the resource where they are configured.\n\nIf the condition evaluates to false, Terraform will produce an error message that includes the result of the error_message expression. If you declare multiple preconditions or postconditions, Terraform returns error messages for all failed conditions.\n\nThe following example uses a postcondition to detect if the caller accidentally provided an AMI intended for the wrong system component.\n\ndata \"aws_ami\" \"example\" {\n\n  id = var.aws_ami_id\n\n\n\n  lifecycle {\n\n    # The AMI ID must refer to an existing AMI that has the tag \"nomad-server\".\n\n    postcondition {\n\n      condition     = self.tags[\"Component\"] == \"nomad-server\"\n\n      error_message = \"tags[\\\"Component\\\"] must be \\\"nomad-server\\\".\"\n\n    }\n\n  }\n\n}\n\nCopy\nResources and Data Sources\n\nThe lifecycle block inside a resource or data block can include both precondition and postcondition blocks.\n\nTerraform evaluates precondition blocks after evaluating existing count and for_each arguments. This lets Terraform evaluate the precondition separately for each instance and then make each.key, count.index, etc. available to those conditions. Terraform also evaluates preconditions before evaluating the resource's configuration arguments. Preconditions can take precedence over argument evaluation errors.\nTerraform evaluates postcondition blocks after planning and applying changes to a managed resource, or after reading from a data source. Postcondition failures prevent changes to other resources that depend on the failing resource.\n\nIn most cases, we do not recommend including both a data block and a resource block that both represent the same object in the same configuration. Doing so can prevent Terraform from understanding that the data block result can be affected by changes in the resource block. However, when you need to check a result of a resource block that the resource itself does not directly export, you can use a data block to check that object safely as long as you place the check as a direct postcondition of the data block. This tells Terraform that the data block is serving as a check of an object defined elsewhere, allowing Terraform to perform actions in the correct order.\n\nOutputs\n\nAn output block can include a precondition block.\n\nPreconditions can serve a symmetrical purpose to input variable validation blocks. Whereas input variable validation checks assumptions the module makes about its inputs, preconditions check guarantees that the module makes about its outputs. You can use preconditions to prevent Terraform from saving an invalid new output value in the state. You can also use them to preserve a valid output value from the previous apply, if applicable.\n\nTerraform evaluates output value preconditions before evaluating the value expression to finalize the result. Preconditions can take precedence over potential errors in the value expression.\n\nExamples\n\nThe following example shows use cases for preconditions and postconditions. The preconditions and postconditions declare the following assumptions and guarantees.\n\nThe AMI ID must refer to an AMI that contains an operating system for the x86_64 architecture. The precondition would detect if the caller accidentally built an AMI for a different architecture, which may not be able to run the software this virtual machine is intended to host.\n\nThe EC2 instance must be allocated a public DNS hostname. In Amazon Web Services, EC2 instances are assigned public DNS hostnames only if they belong to a virtual network configured in a certain way. The postcondition would detect if the selected virtual network is not configured correctly, prompting the user to debug the network settings.\n\nThe EC2 instance will have an encrypted root volume. The postcondition ensures that the root volume is encrypted, even though the software running in this EC2 instance would probably still operate as expected on an unencrypted volume. This lets Terraform produce an error immediately, before any other components rely on the new EC2 instance.\n\n\n\ndata \"aws_ami\" \"example\" {\n\n  owners = [\"amazon\"]\n\n\n\n  filter {\n\n    name   = \"image-id\"\n\n    values = [\"ami-abc123\"]\n\n  }\n\n}\n\n\n\nresource \"aws_instance\" \"example\" {\n\n  instance_type = \"t3.micro\"\n\n  ami           = data.aws_ami.example.id\n\n\n\n  lifecycle {\n\n    # The AMI ID must refer to an AMI that contains an operating system\n\n    # for the `x86_64` architecture.\n\n    precondition {\n\n      condition     = data.aws_ami.example.architecture == \"x86_64\"\n\n      error_message = \"The selected AMI must be for the x86_64 architecture.\"\n\n    }\n\n\n\n    # The EC2 instance must be allocated a public DNS hostname.\n\n    postcondition {\n\n      condition     = self.public_dns != \"\"\n\n      error_message = \"EC2 instance must be in a VPC that has public DNS hostnames enabled.\"\n\n    }\n\n  }\n\n}\n\n\n\ndata \"aws_ebs_volume\" \"example\" {\n\n  # Use data resources that refer to other resources to\n\n  # load extra data that isn't directly exported by a resource.\n\n  #\n\n  # Read the details about the root storage volume for the EC2 instance\n\n  # declared by aws_instance.example, using the exported ID.\n\n\n\n  filter {\n\n    name = \"volume-id\"\n\n    values = [aws_instance.example.root_block_device.volume_id]\n\n  }\n\n\n\n  # Whenever a data resource is verifying the result of a managed resource\n\n  # declared in the same configuration, you MUST write the checks as\n\n  # postconditions of the data resource. This ensures Terraform will wait\n\n  # to read the data resource until after any changes to the managed resource\n\n  # have completed.\n\n  lifecycle {\n\n    # The EC2 instance will have an encrypted root volume.\n\n    postcondition {\n\n      condition     = self.encrypted\n\n      error_message = \"The server's root volume is not encrypted.\"\n\n    }\n\n  }\n\n}\n\n\n\noutput \"api_base_url\" {\n\n  value = \"https://${aws_instance.example.private_dns}:8433/\"\n\n}\n\nCopy\nChoosing Between Preconditions and Postconditions\n\nYou can often implement a validation check as either a postcondition of the resource producing the data or as a precondition of a resource or output value using the data. To decide which is most appropriate, consider whether the check is representing either an assumption or a guarantee.\n\nUse Preconditions for Assumptions\n\nAn assumption is a condition that must be true in order for the configuration of a particular resource to be usable. For example, an aws_instance configuration can have the assumption that the given AMI will always be configured for the x86_64 CPU architecture.\n\nWe recommend using preconditions for assumptions, so that future maintainers can find them close to the other expressions that rely on that condition. This lets them understand more about what that resource is intended to allow.\n\nUse Postconditions for Guarantees\n\nA guarantee is a characteristic or behavior of an object that the rest of the configuration should be able to rely on. For example, an aws_instance configuration can have the guarantee that an EC2 instance will be running in a network that assigns it a private DNS record.\n\nWe recommend using postconditions for guarantees, so that future maintainers can find them close to the resource configuration that is responsible for implementing those guarantees. This lets them more easily determine which behaviors they should preserve when changing the configuration.\n\nAdditional Decision Factors\n\nYou should also consider the following questions when creating preconditions and postconditions.\n\nWhich resource or output value would be most helpful to report in the error message? Terraform will always report errors in the location where the condition was declared.\nWhich approach is more convenient? If a particular resource has many dependencies that all make an assumption about that resource, it can be pragmatic to declare that once as a post-condition of the resource, rather than declaring it many times as preconditions on each of the dependencies.\nIs it helpful to declare the same or similar conditions as both preconditions and postconditions? This can be useful if the postcondition is in a different module than the precondition because it lets the modules verify one another as they evolve independently.\nChecks with Assertions\n\nNote: Check blocks and their assertions are only available in Terraform v1.5.0 and later.\n\nCheck blocks can validate your infrastructure outside the usual resource lifecycle. You can add custom conditions via assert blocks, which execute at the end of the plan and apply stages and produce warnings to notify you of problems within your infrastructure.\n\nYou can add one or more assert blocks within a check block to verify custom conditions. Each assertion requires a condition argument, a boolean expression that should return true if the intended assumption or guarantee is fulfilled or false if it does not. Your condition expression can refer to any resource, data source, or variable available to the surrounding check block.\n\nThe following example uses a check block with an assertion to verify the Terraform website is healthy.\n\ncheck \"health_check\" {\n\n  data \"http\" \"terraform_io\" {\n\n    url = \"https://www.terraform.io\"\n\n  }\n\n\n\n  assert {\n\n    condition = data.http.terraform_io.status_code == 200\n\n    error_message = \"${data.http.terraform_io.url} returned an unhealthy status code\"\n\n  }\n\n}\n\nCopy\n\nIf the condition evaluates to false, Terraform produces an error message that includes the result of the error_message expression. If you declare multiple assertions, Terraform returns error messages for all failed conditions.\n\nContinuous Validation in HCP Terraform\n\nHCP Terraform can automatically check whether the checks in a workspace’s configuration continue to pass after Terraform provisions the infrastructure. For example, you can write a check to continuously monitor the validity of an API gateway certificate. Continuous validation alerts you when the condition fails, so you can update the certificate and avoid errors the next time you want to update your infrastructure. Refer to Continuous Validation in the HCP Terraform documentation for details.\n\nCondition Expressions\n\nCheck assertions, input variable validation, preconditions, and postconditions all require a condition argument. This is a boolean expression that should return true if the intended assumption or guarantee is fulfilled or false if it does not.\n\nYou can use any of Terraform's built-in functions or language operators in a condition as long as the expression is valid and returns a boolean result. The following language features are particularly useful when writing condition expressions.\n\nLogical Operators\n\nUse the logical operators && (AND), || (OR), and ! (NOT) to combine multiple conditions together.\n\n  condition = var.name != \"\" && lower(var.name) == var.name\n\nCopy\n\nYou can also use arithmetic operators (e.g. a + b), equality operators (eg., a == b) and comparison operators (e.g., a < b). Refer to Arithmetic and Logical Operators for details.\n\ncontains Function\n\nUse the contains function to test whether a given value is one of a set of predefined valid values.\n\n  condition = contains([\"STAGE\", \"PROD\"], var.environment)\n\nCopy\nlength Function\n\nUse the length function to test a collection's length and require a non-empty list or map.\n\n  condition = length(var.items) != 0\n\nCopy\n\nThis is a better approach than directly comparing with another collection using == or !=. This is because the comparison operators can only return true if both operands have exactly the same type, which is often ambiguous for empty collections.\n\nfor Expressions\n\nUse for expressions in conjunction with the functions alltrue and anytrue to test whether a condition holds for all or for any elements of a collection.\n\n  condition = alltrue([\n\n    for v in var.instances : contains([\"t2.micro\", \"m3.medium\"], v.type)\n\n  ])\n\nCopy\ncan Function\n\nUse the can function to concisely use the validity of an expression as a condition. It returns true if its given expression evaluates successfully and false if it returns any error, so you can use various other functions that typically return errors as a part of your condition expressions.\n\nFor example, you can use can with regex to test if a string matches a particular pattern because regex returns an error when given a non-matching string.\n\n  condition = can(regex(\"^[a-z]+$\", var.name))\n\nCopy\n\nYou can also use can with the type conversion functions to test whether a value is convertible to a type or type constraint.\n\n  # This remote output value must have a value that can\n\n  # be used as a string, which includes strings themselves\n\n  # but also allows numbers and boolean values.\n\n  condition = can(tostring(data.terraform_remote_state.example.outputs[\"name\"]))\n\nCopy\n  # This remote output value must be convertible to a list\n\n  # type of with element type.\n\n  condition = can(tolist(data.terraform_remote_state.example.outputs[\"items\"]))\n\nCopy\n\nYou can also use can with attribute access or index operators to test whether a collection or structural value has a particular element or index.\n\n  # var.example must have an attribute named \"foo\"\n\n  condition = can(var.example.foo)\n\nCopy\n  # var.example must be a sequence with at least one element\n\n  condition = can(var.example[0])\n\n  # (although it would typically be clearer to write this as a\n\n  # test like length(var.example) > 0 to better represent the\n\n  # intent of the condition.)\n\nCopy\nself Object\n\nUse the self object in postcondition blocks to refer to attributes of the instance under evaluation.\n\nresource \"aws_instance\" \"example\" {\n\n  instance_type = \"t2.micro\"\n\n  ami           = \"ami-abc123\"\n\n\n\n  lifecycle {\n\n    postcondition {\n\n      condition     = self.instance_state == \"running\"\n\n      error_message = \"EC2 instance must be running.\"\n\n    }\n\n  }\n\n}\n\nCopy\neach and count Objects\n\nIn blocks where for_each or count are set, use each and count objects to refer to other resources that are expanded in a chain.\n\nvariable \"vpc_cidrs\" {\n\n  type = set(string)\n\n}\n\n\n\ndata \"aws_vpc\" \"example\" {\n\n  for_each = var.vpc_cidrs\n\n\n\n  filter {\n\n    name   = \"cidr\"\n\n    values = [each.key]\n\n  }\n\n}\n\n\n\nresource \"aws_internet_gateway\" \"example\" {\n\n  for_each = data.aws_vpc.example\n\n  vpc_id = each.value.id\n\n\n\n  lifecycle {\n\n    precondition {\n\n      condition     = data.aws_vpc.example[each.key].state == \"available\"\n\n      error_message = \"VPC ${each.key} must be available.\"\n\n    }\n\n  }\n\n}\n\nCopy\nError Messages\n\nInput variable validations, preconditions, and postconditions all must include the error_message argument. This contains the text that Terraform will include as part of error messages when it detects an unmet condition.\n\nError: Resource postcondition failed\n\n\n\n  with data.aws_ami.example,\n\n  on ec2.tf line 19, in data \"aws_ami\" \"example\":\n\n  72:       condition     = self.tags[\"Component\"] == \"nomad-server\"\n\n    |----------------\n\n    | self.tags[\"Component\"] is \"consul-server\"\n\n\n\nThe selected AMI must be tagged with the Component value \"nomad-server\".\n\nCopy\n\nThe error_message argument can be any expression that evaluates to a string. This includes literal strings, heredocs, and template expressions. You can use the format function to convert items of null, list, or map types into a formatted string. Multi-line error messages are supported, and lines with leading whitespace will not be word wrapped.\n\nWe recommend writing error messages as one or more full sentences in a style similar to Terraform's own error messages. Terraform will show the message alongside the name of the resource that detected the problem and any external values included in the condition expression.\n\nConditions Checked Only During Apply\n\nTerraform evaluates custom conditions as early as possible.\n\nInput variable validations can only refer to the variable value, so Terraform always evaluates them immediately. Check assertions, preconditions, and postconditions depend on Terraform evaluating whether the value(s) associated with the condition are known before or after applying the configuration.\n\nKnown before apply: Terraform checks the condition during the planning phase. For example, Terraform can know the value of an image ID during planning as long as it is not generated from another resource.\nKnown after apply: Terraform delays checking that condition until the apply phase. For example, AWS only assigns the root volume ID when it starts an EC2 instance, so Terraform cannot know this value until apply.\n\nDuring the apply phase, a failed precondition will prevent Terraform from implementing planned actions for the associated resource. However, a failed postcondition will halt processing after Terraform has already implemented these actions. The failed postcondition prevents any further downstream actions that rely on the resource, but does not undo the actions Terraform has already taken.\n\nTerraform typically has less information during the initial creation of a full configuration than when applying subsequent changes. Therefore, Terraform may check conditions during apply for initial creation and then check them during planning for subsequent updates.\n\nEdit this page on GitHub\n\nOn this page:\n\nCustom Conditions\nSelecting a Custom Condition for your use case\nInput Variable Validation\nPreconditions and Postconditions\nChecks with Assertions\nCondition Expressions\nError Messages\nConditions Checked Only During Apply\nTheme\nDark\nLight\nSystem\nCertifications\nSystem Status\nCookie Manager\nTerms of Use\nSecurity\nPrivacy\nTrademark Policy\nTrade Controls\nGive Feedback\n(opens in new tab)\nWe use cookies & other similar technology to collect data to improve your experience on our site, as described in our Privacy Policy and Cookie Policy.\nManage PreferencesACCEPT"
  },
  {
    "title": "Resource Behavior - Configuration Language | Terraform | HashiCorp Developer",
    "url": "https://developer.hashicorp.com/terraform/language/resources/behavior#resource-dependencies",
    "html": "Skip to main content\nNews\nHashiCorp to join IBM to accelerate multi-cloud automation.\nRead the blog →\nDismiss alert\nTerraform\nInstall\nTutorials\nDocumentation\nRegistry\n(opens in new tab)\nTry Cloud\n(opens in new tab)\nSearch\nCommand or control key\nK key\nTerraform Home\nConfiguration Language\nConfiguration Language\nStyle Guide\nFiles and Directories\nSyntax\nResources\nOverview\nResource Blocks\nResource Behavior\nMeta-Arguments\nProvisioners\nThe terraform_data Resource Type\nData Sources\nProviders\nVariables and Outputs\nModules\nMoved block\nChecks\nImport\nExpressions\nFunctions\nTerraform Settings\nState\nTests\nUpgrading to Terraform v1.7\nv1.x Compatibility Promises\nTerraform Internals\nResources\nTutorial Library\nCertifications\nCommunity Forum\n(opens in new tab)\nSupport\n(opens in new tab)\nGitHub\n(opens in new tab)\nTerraform Registry\n(opens in new tab)\nDeveloper\nTerraform\nConfiguration Language\nResources\nResource Behavior\nv1.8.x (latest)\nResource Behavior\n\nA resource block declares that you want a particular infrastructure object to exist with the given settings. If you are writing a new configuration for the first time, the resources it defines will exist only in the configuration, and will not yet represent real infrastructure objects in the target platform.\n\nApplying a Terraform configuration is the process of creating, updating, and destroying real infrastructure objects in order to make their settings match the configuration.\n\nHow Terraform Applies a Configuration\n\nWhen Terraform creates a new infrastructure object represented by a resource block, the identifier for that real object is saved in Terraform's state, allowing it to be updated and destroyed in response to future changes. For resource blocks that already have an associated infrastructure object in the state, Terraform compares the actual configuration of the object with the arguments given in the configuration and, if necessary, updates the object to match the configuration.\n\nIn summary, applying a Terraform configuration will:\n\nCreate resources that exist in the configuration but are not associated with a real infrastructure object in the state.\nDestroy resources that exist in the state but no longer exist in the configuration.\nUpdate in-place resources whose arguments have changed.\nDestroy and re-create resources whose arguments have changed but which cannot be updated in-place due to remote API limitations.\n\nThis general behavior applies for all resources, regardless of type. The details of what it means to create, update, or destroy a resource are different for each resource type, but this standard set of verbs is common across them all.\n\nThe meta-arguments within resource blocks, documented in the sections below, allow some details of this standard resource behavior to be customized on a per-resource basis.\n\nAccessing Resource Attributes\n\nExpressions within a Terraform module can access information about resources in the same module, and you can use that information to help configure other resources. Use the <RESOURCE TYPE>.<NAME>.<ATTRIBUTE> syntax to reference a resource attribute in an expression.\n\nIn addition to arguments specified in the configuration, resources often provide read-only attributes with information obtained from the remote API; this often includes things that can't be known until the resource is created, like the resource's unique random ID.\n\nMany providers also include data sources, which are a special type of resource used only for looking up information.\n\nFor a list of the attributes a resource or data source type provides, consult its documentation; these are generally included in a second list below its list of configurable arguments.\n\nFor more information about referencing resource attributes in expressions, see Expressions: References to Resource Attributes.\n\nResource Dependencies\n\nMost resources in a configuration don't have any particular relationship, and Terraform can make changes to several unrelated resources in parallel.\n\nHowever, some resources must be processed after other specific resources; sometimes this is because of how the resource works, and sometimes the resource's configuration just requires information generated by another resource.\n\nMost resource dependencies are handled automatically. Terraform analyses any expressions within a resource block to find references to other objects, and treats those references as implicit ordering requirements when creating, updating, or destroying resources. Since most resources with behavioral dependencies on other resources also refer to those resources' data, it's usually not necessary to manually specify dependencies between resources.\n\nHowever, some dependencies cannot be recognized implicitly in configuration. For example, if Terraform must manage access control policies and take actions that require those policies to be present, there is a hidden dependency between the access policy and a resource whose creation depends on it. In these rare cases, the depends_on meta-argument can explicitly specify a dependency.\n\nYou can also use the replace_triggered_by meta-argument to add dependencies between otherwise independent resources. It forces Terraform to replace the parent resource when there is a change to a referenced resource or resource attribute.\n\nLocal-only Resources\n\nWhile most resource types correspond to an infrastructure object type that is managed via a remote network API, there are certain specialized resource types that operate only within Terraform itself, calculating some results and saving those results in the state for future use.\n\nFor example, local-only resource types exist for generating private keys, issuing self-signed TLS certificates, and even generating random ids. While these resource types often have a more marginal purpose than those managing \"real\" infrastructure objects, they can be useful as glue to help connect together other resources.\n\nThe behavior of local-only resources is the same as all other resources, but their result data exists only within the Terraform state. \"Destroying\" such a resource means only to remove it from the state, discarding its data.\n\nEdit this page on GitHub\n\nOn this page:\n\nResource Behavior\nHow Terraform Applies a Configuration\nAccessing Resource Attributes\nResource Dependencies\nLocal-only Resources\nTheme\nDark\nLight\nSystem\nCertifications\nSystem Status\nCookie Manager\nTerms of Use\nSecurity\nPrivacy\nTrademark Policy\nTrade Controls\nGive Feedback\n(opens in new tab)\nWe use cookies & other similar technology to collect data to improve your experience on our site, as described in our Privacy Policy and Cookie Policy.\nManage PreferencesACCEPT"
  },
  {
    "title": "Data Sources - Configuration Language | Terraform | HashiCorp Developer",
    "url": "https://developer.hashicorp.com/terraform/language/v1.1.x/data-sources",
    "html": "Skip to main content\nNews\nHashiCorp to join IBM to accelerate multi-cloud automation.\nRead the blog →\nDismiss alert\nTerraform\nInstall\nTutorials\nDocumentation\nRegistry\n(opens in new tab)\nTry Cloud\n(opens in new tab)\nSearch\nCommand or control key\nK key\nTerraform Home\nConfiguration Language\nConfiguration Language\nTerraform Language\nOverview\nFiles and Directories\nSyntax\nResources\nData Sources\nProviders\nVariables and Outputs\nModules\nExpressions\nFunctions\nTerraform Settings\nState\nUpgrade Guides\nHistorical docs: 0.11 and Older\nResources\nTutorial Library\nCertifications\nCommunity Forum\n(opens in new tab)\nSupport\n(opens in new tab)\nGitHub\n(opens in new tab)\nTerraform Registry\n(opens in new tab)\n\nYou are viewing documentation for version v1.1 and earlier. View latest version.\n\nDeveloper\nTerraform\nConfiguration Language v1.1.x\nData Sources\nv1.1 and earlier\nData Sources\n\nData sources allow Terraform to use information defined outside of Terraform, defined by another separate Terraform configuration, or modified by functions.\n\nHands-on: Try the Query Data Sources tutorial on HashiCorp Learn.\n\nEach provider may offer data sources alongside its set of resource types.\n\nUsing Data Sources\n\nA data source is accessed via a special kind of resource known as a data resource, declared using a data block:\n\ndata \"aws_ami\" \"example\" {\n\n  most_recent = true\n\n\n\n  owners = [\"self\"]\n\n  tags = {\n\n    Name   = \"app-server\"\n\n    Tested = \"true\"\n\n  }\n\n}\n\nCopy\n\nA data block requests that Terraform read from a given data source (\"aws_ami\") and export the result under the given local name (\"example\"). The name is used to refer to this resource from elsewhere in the same Terraform module, but has no significance outside of the scope of a module.\n\nThe data source and name together serve as an identifier for a given resource and so must be unique within a module.\n\nWithin the block body (between { and }) are query constraints defined by the data source. Most arguments in this section depend on the data source, and indeed in this example most_recent, owners and tags are all arguments defined specifically for the aws_ami data source.\n\nWhen distinguishing from data resources, the primary kind of resource (as declared by a resource block) is known as a managed resource. Both kinds of resources take arguments and export attributes for use in configuration, but while managed resources cause Terraform to create, update, and delete infrastructure objects, data resources cause Terraform only to read objects. For brevity, managed resources are often referred to just as \"resources\" when the meaning is clear from context.\n\nData Source Arguments\n\nEach data resource is associated with a single data source, which determines the kind of object (or objects) it reads and what query constraint arguments are available.\n\nEach data source in turn belongs to a provider, which is a plugin for Terraform that offers a collection of resource types and data sources that most often belong to a single cloud or on-premises infrastructure platform.\n\nMost of the items within the body of a data block are defined by and specific to the selected data source, and these arguments can make full use of expressions and other dynamic Terraform language features.\n\nHowever, there are some \"meta-arguments\" that are defined by Terraform itself and apply across all data sources. These arguments often have additional restrictions on what language features can be used with them, and are described in more detail in the following sections.\n\nData Resource Behavior\n\nIf the query constraint arguments for a data resource refer only to constant values or values that are already known, the data resource will be read and its state updated during Terraform's \"refresh\" phase, which runs prior to creating a plan. This ensures that the retrieved data is available for use during planning and so Terraform's plan will show the actual values obtained.\n\nQuery constraint arguments may refer to values that cannot be determined until after configuration is applied, such as the id of a managed resource that has not been created yet. In this case, reading from the data source is deferred until the apply phase, and any references to the results of the data resource elsewhere in configuration will themselves be unknown until after the configuration has been applied.\n\nLocal-only Data Sources\n\nWhile many data sources correspond to an infrastructure object type that is accessed via a remote network API, some specialized data sources operate only within Terraform itself, calculating some results and exposing them for use elsewhere.\n\nFor example, local-only data sources exist for rendering templates, reading local files, and rendering AWS IAM policies.\n\nThe behavior of local-only data sources is the same as all other data sources, but their result data exists only temporarily during a Terraform operation, and is re-calculated each time a new plan is created.\n\nData Resource Dependencies\n\nData resources have the same dependency resolution behavior as defined for managed resources. Setting the depends_on meta-argument within data blocks defers reading of the data source until after all changes to the dependencies have been applied.\n\nIn order to ensure that data sources are accessing the most up to date information possible in a wide variety of use cases, arguments directly referencing managed resources are treated the same as if the resource was listed in depends_on. This behavior can be avoided when desired by indirectly referencing the managed resource values through a local value.\n\nNOTE: In Terraform 0.12 and earlier, due to the data resource behavior of deferring the read until the apply phase when depending on values that are not yet known, using depends_on with data resources will force the read to always be deferred to the apply phase, and therefore a configuration that uses depends_on with a data resource can never converge. Due to this behavior, we do not recommend using depends_on with data resources.\n\nMultiple Resource Instances\n\nData resources support count and for_each meta-arguments as defined for managed resources, with the same syntax and behavior.\n\nAs with managed resources, when count or for_each is present it is important to distinguish the resource itself from the multiple resource instances it creates. Each instance will separately read from its data source with its own variant of the constraint arguments, producing an indexed result.\n\nSelecting a Non-default Provider Configuration\n\nData resources support the provider meta-argument as defined for managed resources, with the same syntax and behavior.\n\nLifecycle Customizations\n\nData resources do not currently have any customization settings available for their lifecycle, but the lifecycle nested block is reserved in case any are added in future versions.\n\nExample\n\nA data source configuration looks like the following:\n\n# Find the latest available AMI that is tagged with Component = web\n\ndata \"aws_ami\" \"web\" {\n\n  filter {\n\n    name   = \"state\"\n\n    values = [\"available\"]\n\n  }\n\n\n\n  filter {\n\n    name   = \"tag:Component\"\n\n    values = [\"web\"]\n\n  }\n\n\n\n  most_recent = true\n\n}\n\nCopy\nDescription\n\nThe data block creates a data instance of the given type (first block label) and name (second block label). The combination of the type and name must be unique.\n\nWithin the block (the { }) is configuration for the data instance. The configuration is dependent on the type; as with resources, each provider on the Terraform Registry has its own documentation for configuring and using the data types it provides.\n\nEach data instance will export one or more attributes, which can be used in other resources as reference expressions of the form data.<TYPE>.<NAME>.<ATTRIBUTE>. For example:\n\nresource \"aws_instance\" \"web\" {\n\n  ami           = data.aws_ami.web.id\n\n  instance_type = \"t1.micro\"\n\n}\n\nCopy\nMeta-Arguments\n\nAs data sources are essentially a read only subset of resources, they also support the same meta-arguments of resources with the exception of the lifecycle configuration block.\n\nNon-Default Provider Configurations\n\nSimilarly to resources, when a module has multiple configurations for the same provider you can specify which configuration to use with the provider meta-argument:\n\ndata \"aws_ami\" \"web\" {\n\n  provider = aws.west\n\n\n\n  # ...\n\n}\n\nCopy\n\nSee The Resource provider Meta-Argument for more information.\n\nData Source Lifecycle\n\nIf the arguments of a data instance contain no references to computed values, such as attributes of resources that have not yet been created, then the data instance will be read and its state updated during Terraform's \"refresh\" phase, which by default runs prior to creating a plan. This ensures that the retrieved data is available for use during planning and the diff will show the real values obtained.\n\nData instance arguments may refer to computed values, in which case the attributes of the instance itself cannot be resolved until all of its arguments are defined. In this case, refreshing the data instance will be deferred until the \"apply\" phase, and all interpolations of the data instance attributes will show as \"computed\" in the plan since the values are not yet known.\n\nOn this page:\n\nData Sources\nUsing Data Sources\nData Source Arguments\nData Resource Behavior\nLocal-only Data Sources\nData Resource Dependencies\nMultiple Resource Instances\nSelecting a Non-default Provider Configuration\nLifecycle Customizations\nExample\nDescription\nMeta-Arguments\nData Source Lifecycle\nTheme\nDark\nLight\nSystem\nCertifications\nSystem Status\nCookie Manager\nTerms of Use\nSecurity\nPrivacy\nTrademark Policy\nTrade Controls\nGive Feedback\n(opens in new tab)\nWe use cookies & other similar technology to collect data to improve your experience on our site, as described in our Privacy Policy and Cookie Policy.\nManage PreferencesACCEPT"
  },
  {
    "title": "Data Sources - Configuration Language | Terraform | HashiCorp Developer",
    "url": "https://developer.hashicorp.com/terraform/language/v1.3.x/data-sources",
    "html": "Skip to main content\nNews\nHashiCorp to join IBM to accelerate multi-cloud automation.\nRead the blog →\nDismiss alert\nTerraform\nInstall\nTutorials\nDocumentation\nRegistry\n(opens in new tab)\nTry Cloud\n(opens in new tab)\nSearch\nCommand or control key\nK key\nTerraform Home\nConfiguration Language\nConfiguration Language\nTerraform Language\nOverview\nStyle Guide\nFiles and Directories\nSyntax\nResources\nData Sources\nProviders\nVariables and Outputs\nModules\nMoved block\nExpressions\nFunctions\nTerraform Settings\nState\nUpgrading to Terraform v1.3\nv1.x Compatibility Promises\nTerraform Internals\nResources\nTutorial Library\nCertifications\nCommunity Forum\n(opens in new tab)\nSupport\n(opens in new tab)\nGitHub\n(opens in new tab)\nTerraform Registry\n(opens in new tab)\n\nYou are viewing documentation for version v1.3.x. View latest version.\n\nDeveloper\nTerraform\nConfiguration Language v1.3.x\nData Sources\nv1.3.x\nData Sources\n\nData sources allow Terraform to use information defined outside of Terraform, defined by another separate Terraform configuration, or modified by functions.\n\nHands-on: Try the Query Data Sources tutorial.\n\nEach provider may offer data sources alongside its set of resource types.\n\nUsing Data Sources\n\nA data source is accessed via a special kind of resource known as a data resource, declared using a data block:\n\ndata \"aws_ami\" \"example\" {\n\n  most_recent = true\n\n\n\n  owners = [\"self\"]\n\n  tags = {\n\n    Name   = \"app-server\"\n\n    Tested = \"true\"\n\n  }\n\n}\n\nCopy\n\nA data block requests that Terraform read from a given data source (\"aws_ami\") and export the result under the given local name (\"example\"). The name is used to refer to this resource from elsewhere in the same Terraform module, but has no significance outside of the scope of a module.\n\nThe data source and name together serve as an identifier for a given resource and so must be unique within a module.\n\nWithin the block body (between { and }) are query constraints defined by the data source. Most arguments in this section depend on the data source, and indeed in this example most_recent, owners and tags are all arguments defined specifically for the aws_ami data source.\n\nWhen distinguishing from data resources, the primary kind of resource (as declared by a resource block) is known as a managed resource. Both kinds of resources take arguments and export attributes for use in configuration, but while managed resources cause Terraform to create, update, and delete infrastructure objects, data resources cause Terraform only to read objects. For brevity, managed resources are often referred to just as \"resources\" when the meaning is clear from context.\n\nData Source Arguments\n\nEach data resource is associated with a single data source, which determines the kind of object (or objects) it reads and what query constraint arguments are available.\n\nEach data source in turn belongs to a provider, which is a plugin for Terraform that offers a collection of resource types and data sources that most often belong to a single cloud or on-premises infrastructure platform.\n\nMost of the items within the body of a data block are defined by and specific to the selected data source, and these arguments can make full use of expressions and other dynamic Terraform language features.\n\nHowever, there are some \"meta-arguments\" that are defined by Terraform itself and apply across all data sources. These arguments often have additional restrictions on what language features can be used with them, and are described in more detail in the following sections.\n\nData Resource Behavior\n\nTerraform reads data resources during the planning phase when possible, but announces in the plan when it must defer reading resources until the apply phase to preserve the order of operations. Terraform defers reading data resources in the following situations:\n\nAt least one of the given arguments is a managed resource attribute or other value that Terraform cannot predict until the apply step.\nThe data resource depends directly on a managed resource that itself has planned changes in the current plan.\nThe data resource has custom conditions and it depends directly or indirectly on a managed resource that itself has planned changes in the current plan.\n\nRefer to Data Resource Dependencies for details on what it means for a data resource to depend on other objects. Any resulting attribute of such a data resource will be unknown during planning, so it cannot be used in situations where values must be fully known.\n\nLocal-only Data Sources\n\nWhile many data sources correspond to an infrastructure object type that is accessed via a remote network API, some specialized data sources operate only within Terraform itself, calculating some results and exposing them for use elsewhere.\n\nFor example, local-only data sources exist for rendering templates, reading local files, and rendering AWS IAM policies.\n\nThe behavior of local-only data sources is the same as all other data sources, but their result data exists only temporarily during a Terraform operation, and is re-calculated each time a new plan is created.\n\nData Resource Dependencies\n\nData resources have the same dependency resolution behavior as defined for managed resources. Setting the depends_on meta-argument within data blocks defers reading of the data source until after all changes to the dependencies have been applied.\n\nIn order to ensure that data sources are accessing the most up to date information possible in a wide variety of use cases, arguments directly referencing managed resources are treated the same as if the resource was listed in depends_on. This behavior can be avoided when desired by indirectly referencing the managed resource values through a local value, unless the data resource itself has custom conditions.\n\nNOTE: In Terraform 0.12 and earlier, due to the data resource behavior of deferring the read until the apply phase when depending on values that are not yet known, using depends_on with data resources will force the read to always be deferred to the apply phase, and therefore a configuration that uses depends_on with a data resource can never converge. Due to this behavior, we do not recommend using depends_on with data resources.\n\nCustom Condition Checks\n\nYou can use precondition and postcondition blocks to specify assumptions and guarantees about how the data source operates. The following examples creates a postcondition that checks whether the AMI has the correct tags.\n\ndata \"aws_ami\" \"example\" {\n\n  id = var.aws_ami_id\n\n\n\n  lifecycle {\n\n    # The AMI ID must refer to an existing AMI that has the tag \"nomad-server\".\n\n    postcondition {\n\n      condition     = self.tags[\"Component\"] == \"nomad-server\"\n\n      error_message = \"tags[\\\"Component\\\"] must be \\\"nomad-server\\\".\"\n\n    }\n\n  }\n\n}\n\nCopy\n\nCustom conditions can help capture assumptions, helping future maintainers understand the configuration design and intent. They also return useful information about errors earlier and in context, helping consumers more easily diagnose issues in their configurations.\n\nRefer to Custom Condition Checks for more details.\n\nMultiple Resource Instances\n\nData resources support count and for_each meta-arguments as defined for managed resources, with the same syntax and behavior.\n\nAs with managed resources, when count or for_each is present it is important to distinguish the resource itself from the multiple resource instances it creates. Each instance will separately read from its data source with its own variant of the constraint arguments, producing an indexed result.\n\nSelecting a Non-default Provider Configuration\n\nData resources support the provider meta-argument as defined for managed resources, with the same syntax and behavior.\n\nLifecycle Customizations\n\nData resources do not have any customization settings available for their lifecycle. However, the lifecycle block is reserved for future versions.\n\nExample\n\nA data source configuration looks like the following:\n\n# Find the latest available AMI that is tagged with Component = web\n\ndata \"aws_ami\" \"web\" {\n\n  filter {\n\n    name   = \"state\"\n\n    values = [\"available\"]\n\n  }\n\n\n\n  filter {\n\n    name   = \"tag:Component\"\n\n    values = [\"web\"]\n\n  }\n\n\n\n  most_recent = true\n\n}\n\nCopy\nDescription\n\nThe data block creates a data instance of the given type (first block label) and name (second block label). The combination of the type and name must be unique.\n\nWithin the block (the { }) is configuration for the data instance. The configuration is dependent on the type; as with resources, each provider on the Terraform Registry has its own documentation for configuring and using the data types it provides.\n\nEach data instance will export one or more attributes, which can be used in other resources as reference expressions of the form data.<TYPE>.<NAME>.<ATTRIBUTE>. For example:\n\nresource \"aws_instance\" \"web\" {\n\n  ami           = data.aws_ami.web.id\n\n  instance_type = \"t1.micro\"\n\n}\n\nCopy\nMeta-Arguments\n\nAs data sources are essentially a read only subset of resources, they also support the same meta-arguments of resources with the exception of the lifecycle configuration block.\n\nNon-Default Provider Configurations\n\nSimilarly to resources, when a module has multiple configurations for the same provider you can specify which configuration to use with the provider meta-argument:\n\ndata \"aws_ami\" \"web\" {\n\n  provider = aws.west\n\n\n\n  # ...\n\n}\n\nCopy\n\nSee The Resource provider Meta-Argument for more information.\n\nData Source Lifecycle\n\nIf the arguments of a data instance contain no references to computed values, such as attributes of resources that have not yet been created, then the data instance will be read and its state updated during Terraform's \"refresh\" phase, which by default runs prior to creating a plan. This ensures that the retrieved data is available for use during planning and the diff will show the real values obtained.\n\nData instance arguments may refer to computed values, in which case the attributes of the instance itself cannot be resolved until all of its arguments are defined. In this case, refreshing the data instance will be deferred until the \"apply\" phase, and all interpolations of the data instance attributes will show as \"computed\" in the plan since the values are not yet known.\n\nOn this page:\n\nData Sources\nUsing Data Sources\nData Source Arguments\nData Resource Behavior\nLocal-only Data Sources\nData Resource Dependencies\nCustom Condition Checks\nMultiple Resource Instances\nSelecting a Non-default Provider Configuration\nLifecycle Customizations\nExample\nDescription\nMeta-Arguments\nData Source Lifecycle\nTheme\nDark\nLight\nSystem\nCertifications\nSystem Status\nCookie Manager\nTerms of Use\nSecurity\nPrivacy\nTrademark Policy\nTrade Controls\nGive Feedback\n(opens in new tab)\nWe use cookies & other similar technology to collect data to improve your experience on our site, as described in our Privacy Policy and Cookie Policy.\nManage PreferencesACCEPT"
  },
  {
    "title": "Data Sources - Configuration Language | Terraform | HashiCorp Developer",
    "url": "https://developer.hashicorp.com/terraform/language/v1.2.x/data-sources",
    "html": "Skip to main content\nNews\nHashiCorp to join IBM to accelerate multi-cloud automation.\nRead the blog →\nDismiss alert\nTerraform\nInstall\nTutorials\nDocumentation\nRegistry\n(opens in new tab)\nTry Cloud\n(opens in new tab)\nSearch\nCommand or control key\nK key\nTerraform Home\nConfiguration Language\nConfiguration Language\nTerraform Language\nOverview\nStyle Guide\nFiles and Directories\nSyntax\nResources\nData Sources\nProviders\nVariables and Outputs\nModules\nExpressions\nFunctions\nTerraform Settings\nState\nUpgrading to Terraform v1.2\nv1.x Compatibility Promises\nTerraform Internals\nResources\nTutorial Library\nCertifications\nCommunity Forum\n(opens in new tab)\nSupport\n(opens in new tab)\nGitHub\n(opens in new tab)\nTerraform Registry\n(opens in new tab)\n\nYou are viewing documentation for version v1.2.x. View latest version.\n\nDeveloper\nTerraform\nConfiguration Language v1.2.x\nData Sources\nv1.2.x\nData Sources\n\nData sources allow Terraform to use information defined outside of Terraform, defined by another separate Terraform configuration, or modified by functions.\n\nHands-on: Try the Query Data Sources tutorial.\n\nEach provider may offer data sources alongside its set of resource types.\n\nUsing Data Sources\n\nA data source is accessed via a special kind of resource known as a data resource, declared using a data block:\n\ndata \"aws_ami\" \"example\" {\n\n  most_recent = true\n\n\n\n  owners = [\"self\"]\n\n  tags = {\n\n    Name   = \"app-server\"\n\n    Tested = \"true\"\n\n  }\n\n}\n\nCopy\n\nA data block requests that Terraform read from a given data source (\"aws_ami\") and export the result under the given local name (\"example\"). The name is used to refer to this resource from elsewhere in the same Terraform module, but has no significance outside of the scope of a module.\n\nThe data source and name together serve as an identifier for a given resource and so must be unique within a module.\n\nWithin the block body (between { and }) are query constraints defined by the data source. Most arguments in this section depend on the data source, and indeed in this example most_recent, owners and tags are all arguments defined specifically for the aws_ami data source.\n\nWhen distinguishing from data resources, the primary kind of resource (as declared by a resource block) is known as a managed resource. Both kinds of resources take arguments and export attributes for use in configuration, but while managed resources cause Terraform to create, update, and delete infrastructure objects, data resources cause Terraform only to read objects. For brevity, managed resources are often referred to just as \"resources\" when the meaning is clear from context.\n\nData Source Arguments\n\nEach data resource is associated with a single data source, which determines the kind of object (or objects) it reads and what query constraint arguments are available.\n\nEach data source in turn belongs to a provider, which is a plugin for Terraform that offers a collection of resource types and data sources that most often belong to a single cloud or on-premises infrastructure platform.\n\nMost of the items within the body of a data block are defined by and specific to the selected data source, and these arguments can make full use of expressions and other dynamic Terraform language features.\n\nHowever, there are some \"meta-arguments\" that are defined by Terraform itself and apply across all data sources. These arguments often have additional restrictions on what language features can be used with them, and are described in more detail in the following sections.\n\nData Resource Behavior\n\nTerraform reads data resources during the planning phase when possible, but announces in the plan when it must defer reading resources until the apply phase to preserve the order of operations. Terraform defers reading data resources in the following situations:\n\nAt least one of the given arguments is a managed resource attribute or other value that Terraform cannot predict until the apply step.\nThe data resource depends directly on a managed resource that itself has planned changes in the current plan.\nThe data resource has custom conditions and it depends directly or indirectly on a managed resource that itself has planned changes in the current plan.\n\nRefer to Data Resource Dependencies for details on what it means for a data resource to depend on other objects. Any resulting attribute of such a data resource will be unknown during planning, so it cannot be used in situations where values must be fully known.\n\nLocal-only Data Sources\n\nWhile many data sources correspond to an infrastructure object type that is accessed via a remote network API, some specialized data sources operate only within Terraform itself, calculating some results and exposing them for use elsewhere.\n\nFor example, local-only data sources exist for rendering templates, reading local files, and rendering AWS IAM policies.\n\nThe behavior of local-only data sources is the same as all other data sources, but their result data exists only temporarily during a Terraform operation, and is re-calculated each time a new plan is created.\n\nData Resource Dependencies\n\nData resources have the same dependency resolution behavior as defined for managed resources. Setting the depends_on meta-argument within data blocks defers reading of the data source until after all changes to the dependencies have been applied.\n\nIn order to ensure that data sources are accessing the most up to date information possible in a wide variety of use cases, arguments directly referencing managed resources are treated the same as if the resource was listed in depends_on. This behavior can be avoided when desired by indirectly referencing the managed resource values through a local value, unless the data resource itself has custom conditions.\n\nNOTE: In Terraform 0.12 and earlier, due to the data resource behavior of deferring the read until the apply phase when depending on values that are not yet known, using depends_on with data resources will force the read to always be deferred to the apply phase, and therefore a configuration that uses depends_on with a data resource can never converge. Due to this behavior, we do not recommend using depends_on with data resources.\n\nCustom Condition Checks\n\nYou can use precondition and postcondition blocks to specify assumptions and guarantees about how the data source operates. The following examples creates a postcondition that checks whether the AMI has the correct tags.\n\ndata \"aws_ami\" \"example\" {\n\n  id = var.aws_ami_id\n\n\n\n  lifecycle {\n\n    # The AMI ID must refer to an existing AMI that has the tag \"nomad-server\".\n\n    postcondition {\n\n      condition     = self.tags[\"Component\"] == \"nomad-server\"\n\n      error_message = \"tags[\\\"Component\\\"] must be \\\"nomad-server\\\".\"\n\n    }\n\n  }\n\n}\n\nCopy\n\nCustom conditions can help capture assumptions, helping future maintainers understand the configuration design and intent. They also return useful information about errors earlier and in context, helping consumers more easily diagnose issues in their configurations.\n\nRefer to Custom Condition Checks for more details.\n\nMultiple Resource Instances\n\nData resources support count and for_each meta-arguments as defined for managed resources, with the same syntax and behavior.\n\nAs with managed resources, when count or for_each is present it is important to distinguish the resource itself from the multiple resource instances it creates. Each instance will separately read from its data source with its own variant of the constraint arguments, producing an indexed result.\n\nSelecting a Non-default Provider Configuration\n\nData resources support the provider meta-argument as defined for managed resources, with the same syntax and behavior.\n\nLifecycle Customizations\n\nData resources do not currently have any customization settings available for their lifecycle, but the lifecycle nested block is reserved in case any are added in future versions.\n\nExample\n\nA data source configuration looks like the following:\n\n# Find the latest available AMI that is tagged with Component = web\n\ndata \"aws_ami\" \"web\" {\n\n  filter {\n\n    name   = \"state\"\n\n    values = [\"available\"]\n\n  }\n\n\n\n  filter {\n\n    name   = \"tag:Component\"\n\n    values = [\"web\"]\n\n  }\n\n\n\n  most_recent = true\n\n}\n\nCopy\nDescription\n\nThe data block creates a data instance of the given type (first block label) and name (second block label). The combination of the type and name must be unique.\n\nWithin the block (the { }) is configuration for the data instance. The configuration is dependent on the type; as with resources, each provider on the Terraform Registry has its own documentation for configuring and using the data types it provides.\n\nEach data instance will export one or more attributes, which can be used in other resources as reference expressions of the form data.<TYPE>.<NAME>.<ATTRIBUTE>. For example:\n\nresource \"aws_instance\" \"web\" {\n\n  ami           = data.aws_ami.web.id\n\n  instance_type = \"t1.micro\"\n\n}\n\nCopy\nMeta-Arguments\n\nAs data sources are essentially a read only subset of resources, they also support the same meta-arguments of resources with the exception of the lifecycle configuration block.\n\nNon-Default Provider Configurations\n\nSimilarly to resources, when a module has multiple configurations for the same provider you can specify which configuration to use with the provider meta-argument:\n\ndata \"aws_ami\" \"web\" {\n\n  provider = aws.west\n\n\n\n  # ...\n\n}\n\nCopy\n\nSee The Resource provider Meta-Argument for more information.\n\nData Source Lifecycle\n\nIf the arguments of a data instance contain no references to computed values, such as attributes of resources that have not yet been created, then the data instance will be read and its state updated during Terraform's \"refresh\" phase, which by default runs prior to creating a plan. This ensures that the retrieved data is available for use during planning and the diff will show the real values obtained.\n\nData instance arguments may refer to computed values, in which case the attributes of the instance itself cannot be resolved until all of its arguments are defined. In this case, refreshing the data instance will be deferred until the \"apply\" phase, and all interpolations of the data instance attributes will show as \"computed\" in the plan since the values are not yet known.\n\nOn this page:\n\nData Sources\nUsing Data Sources\nData Source Arguments\nData Resource Behavior\nLocal-only Data Sources\nData Resource Dependencies\nCustom Condition Checks\nMultiple Resource Instances\nSelecting a Non-default Provider Configuration\nLifecycle Customizations\nExample\nDescription\nMeta-Arguments\nData Source Lifecycle\nTheme\nDark\nLight\nSystem\nCertifications\nSystem Status\nCookie Manager\nTerms of Use\nSecurity\nPrivacy\nTrademark Policy\nTrade Controls\nGive Feedback\n(opens in new tab)\nWe use cookies & other similar technology to collect data to improve your experience on our site, as described in our Privacy Policy and Cookie Policy.\nManage PreferencesACCEPT"
  },
  {
    "title": "Data Sources - Configuration Language | Terraform | HashiCorp Developer",
    "url": "https://developer.hashicorp.com/terraform/language/v1.4.x/data-sources",
    "html": "Skip to main content\nNews\nHashiCorp to join IBM to accelerate multi-cloud automation.\nRead the blog →\nDismiss alert\nTerraform\nInstall\nTutorials\nDocumentation\nRegistry\n(opens in new tab)\nTry Cloud\n(opens in new tab)\nSearch\nCommand or control key\nK key\nTerraform Home\nConfiguration Language\nConfiguration Language\nTerraform Language\nOverview\nStyle Guide\nFiles and Directories\nSyntax\nResources\nData Sources\nProviders\nVariables and Outputs\nModules\nExpressions\nFunctions\nTerraform Settings\nState\nUpgrading to Terraform v1.4\nv1.x Compatibility Promises\nTerraform Internals\nResources\nTutorial Library\nCertifications\nCommunity Forum\n(opens in new tab)\nSupport\n(opens in new tab)\nGitHub\n(opens in new tab)\nTerraform Registry\n(opens in new tab)\n\nYou are viewing documentation for version v1.4.x. View latest version.\n\nDeveloper\nTerraform\nConfiguration Language v1.4.x\nData Sources\nv1.4.x\nData Sources\n\nData sources allow Terraform to use information defined outside of Terraform, defined by another separate Terraform configuration, or modified by functions.\n\nHands-on: Try the Query Data Sources tutorial.\n\nEach provider may offer data sources alongside its set of resource types.\n\nUsing Data Sources\n\nA data source is accessed via a special kind of resource known as a data resource, declared using a data block:\n\ndata \"aws_ami\" \"example\" {\n\n  most_recent = true\n\n\n\n  owners = [\"self\"]\n\n  tags = {\n\n    Name   = \"app-server\"\n\n    Tested = \"true\"\n\n  }\n\n}\n\nCopy\n\nA data block requests that Terraform read from a given data source (\"aws_ami\") and export the result under the given local name (\"example\"). The name is used to refer to this resource from elsewhere in the same Terraform module, but has no significance outside of the scope of a module.\n\nThe data source and name together serve as an identifier for a given resource and so must be unique within a module.\n\nWithin the block body (between { and }) are query constraints defined by the data source. Most arguments in this section depend on the data source, and indeed in this example most_recent, owners and tags are all arguments defined specifically for the aws_ami data source.\n\nWhen distinguishing from data resources, the primary kind of resource (as declared by a resource block) is known as a managed resource. Both kinds of resources take arguments and export attributes for use in configuration, but while managed resources cause Terraform to create, update, and delete infrastructure objects, data resources cause Terraform only to read objects. For brevity, managed resources are often referred to just as \"resources\" when the meaning is clear from context.\n\nData Source Arguments\n\nEach data resource is associated with a single data source, which determines the kind of object (or objects) it reads and what query constraint arguments are available.\n\nEach data source in turn belongs to a provider, which is a plugin for Terraform that offers a collection of resource types and data sources that most often belong to a single cloud or on-premises infrastructure platform.\n\nMost of the items within the body of a data block are defined by and specific to the selected data source, and these arguments can make full use of expressions and other dynamic Terraform language features.\n\nHowever, there are some \"meta-arguments\" that are defined by Terraform itself and apply across all data sources. These arguments often have additional restrictions on what language features can be used with them, and are described in more detail in the following sections.\n\nData Resource Behavior\n\nTerraform reads data resources during the planning phase when possible, but announces in the plan when it must defer reading resources until the apply phase to preserve the order of operations. Terraform defers reading data resources in the following situations:\n\nAt least one of the given arguments is a managed resource attribute or other value that Terraform cannot predict until the apply step.\nThe data resource depends directly on a managed resource that itself has planned changes in the current plan.\nThe data resource has custom conditions and it depends directly or indirectly on a managed resource that itself has planned changes in the current plan.\n\nRefer to Data Resource Dependencies for details on what it means for a data resource to depend on other objects. Any resulting attribute of such a data resource will be unknown during planning, so it cannot be used in situations where values must be fully known.\n\nLocal-only Data Sources\n\nWhile many data sources correspond to an infrastructure object type that is accessed via a remote network API, some specialized data sources operate only within Terraform itself, calculating some results and exposing them for use elsewhere.\n\nFor example, local-only data sources exist for rendering templates, reading local files, and rendering AWS IAM policies.\n\nThe behavior of local-only data sources is the same as all other data sources, but their result data exists only temporarily during a Terraform operation, and is re-calculated each time a new plan is created.\n\nData Resource Dependencies\n\nData resources have the same dependency resolution behavior as defined for managed resources. Setting the depends_on meta-argument within data blocks defers reading of the data source until after all changes to the dependencies have been applied.\n\nIn order to ensure that data sources are accessing the most up to date information possible in a wide variety of use cases, arguments directly referencing managed resources are treated the same as if the resource was listed in depends_on. This behavior can be avoided when desired by indirectly referencing the managed resource values through a local value, unless the data resource itself has custom conditions.\n\nNOTE: In Terraform 0.12 and earlier, due to the data resource behavior of deferring the read until the apply phase when depending on values that are not yet known, using depends_on with data resources will force the read to always be deferred to the apply phase, and therefore a configuration that uses depends_on with a data resource can never converge. Due to this behavior, we do not recommend using depends_on with data resources.\n\nCustom Condition Checks\n\nYou can use precondition and postcondition blocks to specify assumptions and guarantees about how the data source operates. The following examples creates a postcondition that checks whether the AMI has the correct tags.\n\ndata \"aws_ami\" \"example\" {\n\n  id = var.aws_ami_id\n\n\n\n  lifecycle {\n\n    # The AMI ID must refer to an existing AMI that has the tag \"nomad-server\".\n\n    postcondition {\n\n      condition     = self.tags[\"Component\"] == \"nomad-server\"\n\n      error_message = \"tags[\\\"Component\\\"] must be \\\"nomad-server\\\".\"\n\n    }\n\n  }\n\n}\n\nCopy\n\nCustom conditions can help capture assumptions, helping future maintainers understand the configuration design and intent. They also return useful information about errors earlier and in context, helping consumers more easily diagnose issues in their configurations.\n\nRefer to Custom Condition Checks for more details.\n\nMultiple Resource Instances\n\nData resources support count and for_each meta-arguments as defined for managed resources, with the same syntax and behavior.\n\nAs with managed resources, when count or for_each is present it is important to distinguish the resource itself from the multiple resource instances it creates. Each instance will separately read from its data source with its own variant of the constraint arguments, producing an indexed result.\n\nSelecting a Non-default Provider Configuration\n\nData resources support the provider meta-argument as defined for managed resources, with the same syntax and behavior.\n\nLifecycle Customizations\n\nData resources do not have any customization settings available for their lifecycle. However, the lifecycle block is reserved for future versions.\n\nExample\n\nA data source configuration looks like the following:\n\n# Find the latest available AMI that is tagged with Component = web\n\ndata \"aws_ami\" \"web\" {\n\n  filter {\n\n    name   = \"state\"\n\n    values = [\"available\"]\n\n  }\n\n\n\n  filter {\n\n    name   = \"tag:Component\"\n\n    values = [\"web\"]\n\n  }\n\n\n\n  most_recent = true\n\n}\n\nCopy\nDescription\n\nThe data block creates a data instance of the given type (first block label) and name (second block label). The combination of the type and name must be unique.\n\nWithin the block (the { }) is configuration for the data instance. The configuration is dependent on the type; as with resources, each provider on the Terraform Registry has its own documentation for configuring and using the data types it provides.\n\nEach data instance will export one or more attributes, which can be used in other resources as reference expressions of the form data.<TYPE>.<NAME>.<ATTRIBUTE>. For example:\n\nresource \"aws_instance\" \"web\" {\n\n  ami           = data.aws_ami.web.id\n\n  instance_type = \"t1.micro\"\n\n}\n\nCopy\nMeta-Arguments\n\nAs data sources are essentially a read only subset of resources, they also support the same meta-arguments of resources with the exception of the lifecycle configuration block.\n\nNon-Default Provider Configurations\n\nSimilarly to resources, when a module has multiple configurations for the same provider you can specify which configuration to use with the provider meta-argument:\n\ndata \"aws_ami\" \"web\" {\n\n  provider = aws.west\n\n\n\n  # ...\n\n}\n\nCopy\n\nSee The Resource provider Meta-Argument for more information.\n\nData Source Lifecycle\n\nIf the arguments of a data instance contain no references to computed values, such as attributes of resources that have not yet been created, then the data instance will be read and its state updated during Terraform's \"refresh\" phase, which by default runs prior to creating a plan. This ensures that the retrieved data is available for use during planning and the diff will show the real values obtained.\n\nData instance arguments may refer to computed values, in which case the attributes of the instance itself cannot be resolved until all of its arguments are defined. In this case, refreshing the data instance will be deferred until the \"apply\" phase, and all interpolations of the data instance attributes will show as \"computed\" in the plan since the values are not yet known.\n\nOn this page:\n\nData Sources\nUsing Data Sources\nData Source Arguments\nData Resource Behavior\nLocal-only Data Sources\nData Resource Dependencies\nCustom Condition Checks\nMultiple Resource Instances\nSelecting a Non-default Provider Configuration\nLifecycle Customizations\nExample\nDescription\nMeta-Arguments\nData Source Lifecycle\nTheme\nDark\nLight\nSystem\nCertifications\nSystem Status\nCookie Manager\nTerms of Use\nSecurity\nPrivacy\nTrademark Policy\nTrade Controls\nGive Feedback\n(opens in new tab)\nWe use cookies & other similar technology to collect data to improve your experience on our site, as described in our Privacy Policy and Cookie Policy.\nManage PreferencesACCEPT"
  },
  {
    "title": "Data Sources - Configuration Language | Terraform | HashiCorp Developer",
    "url": "https://developer.hashicorp.com/terraform/language/v1.7.x/data-sources",
    "html": "Skip to main content\nNews\nHashiCorp to join IBM to accelerate multi-cloud automation.\nRead the blog →\nDismiss alert\nTerraform\nInstall\nTutorials\nDocumentation\nRegistry\n(opens in new tab)\nTry Cloud\n(opens in new tab)\nSearch\nCommand or control key\nK key\nTerraform Home\nConfiguration Language\nConfiguration Language\nTerraform Language\nOverview\nStyle Guide\nFiles and Directories\nSyntax\nResources\nData Sources\nProviders\nVariables and Outputs\nModules\nMoved block\nChecks\nImport\nExpressions\nFunctions\nTerraform Settings\nState\nTests\nUpgrading to Terraform v1.7\nv1.x Compatibility Promises\nTerraform Internals\nResources\nTutorial Library\nCertifications\nCommunity Forum\n(opens in new tab)\nSupport\n(opens in new tab)\nGitHub\n(opens in new tab)\nTerraform Registry\n(opens in new tab)\n\nYou are viewing documentation for version v1.7.x. View latest version.\n\nDeveloper\nTerraform\nConfiguration Language v1.7.x\nData Sources\nv1.7.x\nData Sources\n\nData sources allow Terraform to use information defined outside of Terraform, defined by another separate Terraform configuration, or modified by functions.\n\nHands-on: Try the Query Data Sources tutorial.\n\nEach provider may offer data sources alongside its set of resource types.\n\nUsing Data Sources\n\nA data source is accessed via a special kind of resource known as a data resource, declared using a data block:\n\ndata \"aws_ami\" \"example\" {\n\n  most_recent = true\n\n\n\n  owners = [\"self\"]\n\n  tags = {\n\n    Name   = \"app-server\"\n\n    Tested = \"true\"\n\n  }\n\n}\n\nCopy\n\nA data block requests that Terraform read from a given data source (\"aws_ami\") and export the result under the given local name (\"example\"). The name is used to refer to this resource from elsewhere in the same Terraform module, but has no significance outside of the scope of a module.\n\nThe data source and name together serve as an identifier for a given resource and so must be unique within a module.\n\nWithin the block body (between { and }) are query constraints defined by the data source. Most arguments in this section depend on the data source, and indeed in this example most_recent, owners and tags are all arguments defined specifically for the aws_ami data source.\n\nWhen distinguishing from data resources, the primary kind of resource (as declared by a resource block) is known as a managed resource. Both kinds of resources take arguments and export attributes for use in configuration, but while managed resources cause Terraform to create, update, and delete infrastructure objects, data resources cause Terraform only to read objects. For brevity, managed resources are often referred to just as \"resources\" when the meaning is clear from context.\n\nData Source Arguments\n\nEach data resource is associated with a single data source, which determines the kind of object (or objects) it reads and what query constraint arguments are available.\n\nEach data source in turn belongs to a provider, which is a plugin for Terraform that offers a collection of resource types and data sources that most often belong to a single cloud or on-premises infrastructure platform.\n\nMost of the items within the body of a data block are defined by and specific to the selected data source, and these arguments can make full use of expressions and other dynamic Terraform language features.\n\nHowever, there are some \"meta-arguments\" that are defined by Terraform itself and apply across all data sources. These arguments often have additional restrictions on what language features can be used with them, and are described in more detail in the following sections.\n\nData Resource Behavior\n\nTerraform reads data resources during the planning phase when possible, but announces in the plan when it must defer reading resources until the apply phase to preserve the order of operations. Terraform defers reading data resources in the following situations:\n\nAt least one of the given arguments is a managed resource attribute or other value that Terraform cannot predict until the apply step.\nThe data resource depends directly on a managed resource that itself has planned changes in the current plan.\nThe data resource has custom conditions and it depends directly or indirectly on a managed resource that itself has planned changes in the current plan.\n\nRefer to Data Resource Dependencies for details on what it means for a data resource to depend on other objects. Any resulting attribute of such a data resource will be unknown during planning, so it cannot be used in situations where values must be fully known.\n\nLocal-only Data Sources\n\nWhile many data sources correspond to an infrastructure object type that is accessed via a remote network API, some specialized data sources operate only within Terraform itself, calculating some results and exposing them for use elsewhere.\n\nFor example, local-only data sources exist for rendering templates, reading local files, and rendering AWS IAM policies.\n\nThe behavior of local-only data sources is the same as all other data sources, but their result data exists only temporarily during a Terraform operation, and is re-calculated each time a new plan is created.\n\nData Resource Dependencies\n\nData resources have the same dependency resolution behavior as defined for managed resources. Setting the depends_on meta-argument within data blocks defers reading of the data source until after all changes to the dependencies have been applied.\n\nIn order to ensure that data sources are accessing the most up to date information possible in a wide variety of use cases, arguments directly referencing managed resources are treated the same as if the resource was listed in depends_on. This behavior can be avoided when desired by indirectly referencing the managed resource values through a local value, unless the data resource itself has custom conditions.\n\nNOTE: In Terraform 0.12 and earlier, due to the data resource behavior of deferring the read until the apply phase when depending on values that are not yet known, using depends_on with data resources will force the read to always be deferred to the apply phase, and therefore a configuration that uses depends_on with a data resource can never converge. Due to this behavior, we do not recommend using depends_on with data resources.\n\nCustom Condition Checks\n\nYou can use precondition and postcondition blocks to specify assumptions and guarantees about how the data source operates. The following examples creates a postcondition that checks whether the AMI has the correct tags.\n\ndata \"aws_ami\" \"example\" {\n\n  id = var.aws_ami_id\n\n\n\n  lifecycle {\n\n    # The AMI ID must refer to an existing AMI that has the tag \"nomad-server\".\n\n    postcondition {\n\n      condition     = self.tags[\"Component\"] == \"nomad-server\"\n\n      error_message = \"tags[\\\"Component\\\"] must be \\\"nomad-server\\\".\"\n\n    }\n\n  }\n\n}\n\nCopy\n\nCustom conditions can help capture assumptions, helping future maintainers understand the configuration design and intent. They also return useful information about errors earlier and in context, helping consumers more easily diagnose issues in their configurations.\n\nRefer to Custom Condition Checks for more details.\n\nMultiple Resource Instances\n\nData resources support count and for_each meta-arguments as defined for managed resources, with the same syntax and behavior.\n\nAs with managed resources, when count or for_each is present it is important to distinguish the resource itself from the multiple resource instances it creates. Each instance will separately read from its data source with its own variant of the constraint arguments, producing an indexed result.\n\nSelecting a Non-default Provider Configuration\n\nData resources support the provider meta-argument as defined for managed resources, with the same syntax and behavior.\n\nLifecycle Customizations\n\nData resources do not have any customization settings available for their lifecycle. However, the lifecycle block is reserved for future versions.\n\nExample\n\nA data source configuration looks like the following:\n\n# Find the latest available AMI that is tagged with Component = web\n\ndata \"aws_ami\" \"web\" {\n\n  filter {\n\n    name   = \"state\"\n\n    values = [\"available\"]\n\n  }\n\n\n\n  filter {\n\n    name   = \"tag:Component\"\n\n    values = [\"web\"]\n\n  }\n\n\n\n  most_recent = true\n\n}\n\nCopy\nDescription\n\nThe data block creates a data instance of the given type (first block label) and name (second block label). The combination of the type and name must be unique.\n\nWithin the block (the { }) is configuration for the data instance. The configuration is dependent on the type; as with resources, each provider on the Terraform Registry has its own documentation for configuring and using the data types it provides.\n\nEach data instance will export one or more attributes, which can be used in other resources as reference expressions of the form data.<TYPE>.<NAME>.<ATTRIBUTE>. For example:\n\nresource \"aws_instance\" \"web\" {\n\n  ami           = data.aws_ami.web.id\n\n  instance_type = \"t1.micro\"\n\n}\n\nCopy\nMeta-Arguments\n\nAs data sources are essentially a read only subset of resources, they also support the same meta-arguments of resources with the exception of the lifecycle configuration block.\n\nNon-Default Provider Configurations\n\nSimilarly to resources, when a module has multiple configurations for the same provider you can specify which configuration to use with the provider meta-argument:\n\ndata \"aws_ami\" \"web\" {\n\n  provider = aws.west\n\n\n\n  # ...\n\n}\n\nCopy\n\nSee The Resource provider Meta-Argument for more information.\n\nData Source Lifecycle\n\nIf the arguments of a data instance contain no references to computed values, such as attributes of resources that have not yet been created, then the data instance will be read and its state updated during Terraform's \"refresh\" phase, which by default runs prior to creating a plan. This ensures that the retrieved data is available for use during planning and the diff will show the real values obtained.\n\nData instance arguments may refer to computed values, in which case the attributes of the instance itself cannot be resolved until all of its arguments are defined. In this case, refreshing the data instance will be deferred until the \"apply\" phase, and all interpolations of the data instance attributes will show as \"computed\" in the plan since the values are not yet known.\n\nOn this page:\n\nData Sources\nUsing Data Sources\nData Source Arguments\nData Resource Behavior\nLocal-only Data Sources\nData Resource Dependencies\nCustom Condition Checks\nMultiple Resource Instances\nSelecting a Non-default Provider Configuration\nLifecycle Customizations\nExample\nDescription\nMeta-Arguments\nData Source Lifecycle\nTheme\nDark\nLight\nSystem\nCertifications\nSystem Status\nCookie Manager\nTerms of Use\nSecurity\nPrivacy\nTrademark Policy\nTrade Controls\nGive Feedback\n(opens in new tab)\nWe use cookies & other similar technology to collect data to improve your experience on our site, as described in our Privacy Policy and Cookie Policy.\nManage PreferencesACCEPT"
  },
  {
    "title": "Refactoring | Terraform | HashiCorp Developer",
    "url": "https://developer.hashicorp.com/terraform/language/modules/develop/refactoring",
    "html": "Skip to main content\nNews\nHashiCorp to join IBM to accelerate multi-cloud automation.\nRead the blog →\nDismiss alert\nTerraform\nInstall\nTutorials\nDocumentation\nRegistry\n(opens in new tab)\nTry Cloud\n(opens in new tab)\nSearch\nCommand or control key\nK key\nTerraform Home\nConfiguration Language\nConfiguration Language\nStyle Guide\nFiles and Directories\nSyntax\nResources\nData Sources\nProviders\nVariables and Outputs\nModules\nOverview\nModule Blocks\nModule Sources\nMeta-Arguments\nModule Development\nOverview\nStandard Module Structure\nProviders Within Modules\nBest Practices: Module Composition\nPublishing Modules\nRefactoring Modules\nMoved block\nChecks\nImport\nExpressions\nFunctions\nTerraform Settings\nState\nTests\nUpgrading to Terraform v1.7\nv1.x Compatibility Promises\nTerraform Internals\nResources\nTutorial Library\nCertifications\nCommunity Forum\n(opens in new tab)\nSupport\n(opens in new tab)\nGitHub\n(opens in new tab)\nTerraform Registry\n(opens in new tab)\nDeveloper\nTerraform\nConfiguration Language\nModules\nModule Development\nRefactoring Modules\nv1.8.x (latest)\nRefactoring\n\nNote: Explicit refactoring declarations with moved blocks is available in Terraform v1.1 and later. For earlier Terraform versions or for refactoring actions too complex to express as moved blocks, you can use the terraform state mv CLI command as a separate step.\n\nIn shared modules and long-lived configurations, you may eventually outgrow your initial module structure and resource names. For example, you might decide that what was previously one child module makes more sense as two separate modules and move a subset of the existing resources to the new one.\n\nTerraform compares previous state with new configuration, correlating by each module or resource's unique address. Therefore by default Terraform understands moving or renaming an object as an intent to destroy the object at the old address and to create a new object at the new address.\n\nWhen you add moved blocks in your configuration to record where you've historically moved or renamed an object, Terraform treats an existing object at the old address as if it now belongs to the new address.\n\nHands On: Try the Use Configuration to Move Resources tutorial.\n\nmoved Block Syntax\n\nA moved block expects no labels and contains only from and to arguments:\n\nmoved {\n\n  from = aws_instance.a\n\n  to   = aws_instance.b\n\n}\n\nCopy\n\nThe example above records that the resource currently known as aws_instance.b was known as aws_instance.a in a previous version of this module.\n\nBefore creating a new plan for aws_instance.b, Terraform first checks whether there is an existing object for aws_instance.a recorded in the state. If there is an existing object, Terraform renames that object to aws_instance.b and then proceeds with creating a plan. The resulting plan is as if the object had originally been created at aws_instance.b, avoiding any need to destroy it during apply.\n\nThe from and to addresses both use a special addressing syntax that allows selecting modules, resources, and resources inside child modules. Below, we describe several refactoring use-cases and the appropriate addressing syntax for each situation.\n\nRenaming a Resource\nEnabling count or for_each For a Resource\nRenaming a Module Call\nEnabling count or for_each For a Module Call\nSplitting One Module into Multiple\nRemoving moved blocks\nRenaming a Resource\n\nConsider this example module with a resource configuration:\n\nresource \"aws_instance\" \"a\" {\n\n  count = 2\n\n\n\n  # (resource-type-specific configuration)\n\n}\n\nCopy\n\nApplying this configuration for the first time would cause Terraform to create aws_instance.a[0] and aws_instance.a[1].\n\nIf you later choose a different name for this resource, then you can change the name label in the resource block and record the old name inside a moved block:\n\nresource \"aws_instance\" \"b\" {\n\n  count = 2\n\n\n\n  # (resource-type-specific configuration)\n\n}\n\n\n\nmoved {\n\n  from = aws_instance.a\n\n  to   = aws_instance.b\n\n}\n\nCopy\n\nWhen creating the next plan for each configuration using this module, Terraform treats any existing objects belonging to aws_instance.a as if they had been created for aws_instance.b: aws_instance.a[0] will be treated as aws_instance.b[0], and aws_instance.a[1] as aws_instance.b[1].\n\nNew instances of the module, which never had an aws_instance.a, will ignore the moved block and propose to create aws_instance.b[0] and aws_instance.b[1] as normal.\n\nBoth of the addresses in this example referred to a resource as a whole, and so Terraform recognizes the move for all instances of the resource. That is, it covers both aws_instance.a[0] and aws_instance.a[1] without the need to identify each one separately.\n\nEach resource type has a separate schema so objects of different types are not typically compatible. You can always use the moved block to change the name of a resource, but some providers also let you change an object from one resource type to another. Refer to the provider documentation for details on which resources can move between types. You cannot use the moved block to change a managed resource (a resource block) into a data resource (a data block).\n\nEnabling count or for_each For a Resource\n\nConsider this example module containing a single-instance resource:\n\nresource \"aws_instance\" \"a\" {\n\n  # (resource-type-specific configuration)\n\n}\n\nCopy\n\nApplying this configuration would lead to Terraform creating an object bound to the address aws_instance.a.\n\nLater, you use for_each with this resource to systematically declare multiple instances. To preserve an object that was previously associated with aws_instance.a alone, you must add a moved block to specify which instance key the object will take in the new configuration:\n\nlocals {\n\n  instances = tomap({\n\n    big = {\n\n      instance_type = \"m3.large\"\n\n    }\n\n    small = {\n\n      instance_type = \"t2.medium\"\n\n    }\n\n  })\n\n}\n\n\n\nresource \"aws_instance\" \"a\" {\n\n  for_each = local.instances\n\n\n\n  instance_type = each.value.instance_type\n\n  # (other resource-type-specific configuration)\n\n}\n\n\n\nmoved {\n\n  from = aws_instance.a\n\n  to   = aws_instance.a[\"small\"]\n\n}\n\nCopy\n\nThe above will keep Terraform from planning to destroy any existing object at aws_instance.a, treating that object instead as if it were originally created as aws_instance.a[\"small\"].\n\nWhen at least one of the two addresses includes an instance key, like [\"small\"] in the above example, Terraform understands both addresses as referring to specific instances of a resource rather than the resource as a whole. That means you can use moved to switch between keys and to add and remove keys as you switch between count, for_each, or neither.\n\nThe following are some other examples of valid moved blocks that record changes to resource instance keys in a similar way:\n\n# Both old and new configuration used \"for_each\", but the\n\n# \"small\" element was renamed to \"tiny\".\n\nmoved {\n\n  from = aws_instance.b[\"small\"]\n\n  to   = aws_instance.b[\"tiny\"]\n\n}\n\n\n\n# The old configuration used \"count\" and the new configuration\n\n# uses \"for_each\", with the following mappings from\n\n# index to key:\n\nmoved {\n\n  from = aws_instance.c[0]\n\n  to   = aws_instance.c[\"small\"]\n\n}\n\nmoved {\n\n  from = aws_instance.c[1]\n\n  to   = aws_instance.c[\"tiny\"]\n\n}\n\n\n\n# The old configuration used \"count\", and the new configuration\n\n# uses neither \"count\" nor \"for_each\", and you want to keep\n\n# only the object at index 2.\n\nmoved {\n\n  from = aws_instance.d[2]\n\n  to   = aws_instance.d\n\n}\n\nCopy\n\nNote: When you add count to an existing resource that didn't use it, Terraform automatically proposes to move the original object to instance zero, unless you write an moved block explicitly mentioning that resource. However, we recommend still writing out the corresponding moved block explicitly, to make the change clearer to future readers of the module.\n\nRenaming a Module Call\n\nYou can rename a call to a module in a similar way as renaming a resource. Consider the following original module version:\n\nmodule \"a\" {\n\n  source = \"../modules/example\"\n\n\n\n  # (module arguments)\n\n}\n\nCopy\n\nWhen applying this configuration, Terraform would prefix the addresses for any resources declared in this module with the module path module.a. For example, a resource aws_instance.example would have the full address module.a.aws_instance.example.\n\nIf you later choose a better name for this module call, then you can change the name label in the module block and record the old name inside a moved block:\n\nmodule \"b\" {\n\n  source = \"../modules/example\"\n\n\n\n  # (module arguments)\n\n}\n\n\n\nmoved {\n\n  from = module.a\n\n  to   = module.b\n\n}\n\nCopy\n\nWhen creating the next plan for each configuration using this module, Terraform will treat any existing object addresses beginning with module.a as if they had instead been created in module.b. module.a.aws_instance.example would be treated as module.b.aws_instance.example.\n\nBoth of the addresses in this example referred to a module call as a whole, and so Terraform recognizes the move for all instances of the call. If this module call used count or for_each then it would apply to all of the instances, without the need to specify each one separately.\n\nEnabling count or for_each For a Module Call\n\nConsider this example of a single-instance module:\n\nmodule \"a\" {\n\n  source = \"../modules/example\"\n\n\n\n  # (module arguments)\n\n}\n\nCopy\n\nApplying this configuration would cause Terraform to create objects whose addresses begin with module.a.\n\nIn later module versions, you may need to use count with this resource to systematically declare multiple instances. To preserve an object that was previously associated with aws_instance.a alone, you can add a moved block to specify which instance key that object will take in the new configuration:\n\nmodule \"a\" {\n\n  source = \"../modules/example\"\n\n  count  = 3\n\n\n\n  # (module arguments)\n\n}\n\n\n\nmoved {\n\n  from = module.a\n\n  to   = module.a[2]\n\n}\n\nCopy\n\nThe configuration above directs Terraform to treat all objects in module.a as if they were originally created in module.a[2]. As a result, Terraform plans to create new objects only for module.a[0] and module.a[1].\n\nWhen at least one of the two addresses includes an instance key, like [2] in the above example, Terraform will understand both addresses as referring to specific instances of a module call rather than the module call as a whole. That means you can use moved to switch between keys and to add and remove keys as you switch between count, for_each, or neither.\n\nFor more examples of recording moves associated with instances, refer to the similar section Enabling count and for_each For a Resource.\n\nSplitting One Module into Multiple\n\nAs a module grows to support new requirements, it might eventually grow big enough to warrant splitting into two separate modules.\n\nConsider this example module:\n\nresource \"aws_instance\" \"a\" {\n\n  # (other resource-type-specific configuration)\n\n}\n\n\n\nresource \"aws_instance\" \"b\" {\n\n  # (other resource-type-specific configuration)\n\n}\n\n\n\nresource \"aws_instance\" \"c\" {\n\n  # (other resource-type-specific configuration)\n\n}\n\nCopy\n\nYou can split this into two modules as follows:\n\naws_instance.a now belongs to module \"x\".\naws_instance.b also belongs to module \"x\".\naws_instance.c belongs module \"y\".\n\nTo achieve this refactoring without replacing existing objects bound to the old resource addresses, you must:\n\nWrite module \"x\", copying over the two resources it should contain.\nWrite module \"y\", copying over the one resource it should contain.\nEdit the original module to no longer include any of these resources, and instead to contain only shim configuration to migrate existing users.\n\nThe new modules \"x\" and \"y\" should contain only resource blocks:\n\n# module \"x\"\n\n\n\nresource \"aws_instance\" \"a\" {\n\n  # (other resource-type-specific configuration)\n\n}\n\n\n\nresource \"aws_instance\" \"b\" {\n\n  # (other resource-type-specific configuration)\n\n}\n\nCopy\n# module \"y\"\n\n\n\nresource \"aws_instance\" \"c\" {\n\n  # (other resource-type-specific configuration)\n\n}\n\nCopy\n\nThe original module, now only a shim for backward-compatibility, calls the two new modules and indicates that the resources moved into them:\n\nmodule \"x\" {\n\n  source = \"../modules/x\"\n\n\n\n  # ...\n\n}\n\n\n\nmodule \"y\" {\n\n  source = \"../modules/y\"\n\n\n\n  # ...\n\n}\n\n\n\nmoved {\n\n  from = aws_instance.a\n\n  to   = module.x.aws_instance.a\n\n}\n\n\n\nmoved {\n\n  from = aws_instance.b\n\n  to   = module.x.aws_instance.b\n\n}\n\n\n\nmoved {\n\n  from = aws_instance.c\n\n  to   = module.y.aws_instance.c\n\n}\n\nCopy\n\nWhen an existing user of the original module upgrades to the new \"shim\" version, Terraform notices these three moved blocks and behaves as if the objects associated with the three old resource addresses were originally created inside the two new modules.\n\nNew users of this family of modules may use either the combined shim module or the two new modules separately. You may wish to communicate to your existing users that the old module is now deprecated and so they should use the two separate modules for any new needs.\n\nThe multi-module refactoring situation is unusual in that it violates the typical rule that a parent module sees its child module as a \"closed box\", unaware of exactly which resources are declared inside it. This compromise assumes that all three of these modules are maintained by the same people and distributed together in a single module package.\n\nTerraform resolves module references in moved blocks relative to the module instance they are defined in. For example, if the original module above were already a child module named module.original, the reference to module.x.aws_instance.a would resolve as module.original.module.x.aws_instance.a. A module may only make moved statements about its own objects and objects of its child modules.\n\nIf you need to refer to resources within a module that was called using count or for_each meta-arguments, you must specify a specific instance key to use in order to match with the new location of the resource configuration:\n\nmoved {\n\n  from = aws_instance.example\n\n  to   = module.new[2].aws_instance.example\n\n}\n\nCopy\nRemoving moved Blocks\n\nOver time, a long-lasting module may accumulate many moved blocks.\n\nRemoving a moved block is a generally breaking change because any configurations that refer to the old address will plan to delete that existing object instead of move it. We strongly recommend that you retain all historical moved blocks from earlier versions of your modules to preserve the upgrade path for users of any previous version.\n\nIf you do decide to remove moved blocks, proceed with caution. It can be safe to remove moved blocks when you are maintaining private modules within an organization and you are certain that all users have successfully run terraform apply with your new module version.\n\nIf you need to rename or move the same object twice, we recommend documenting the full history using chained moved blocks, where the new block refers to the existing block:\n\nmoved {\n\n  from = aws_instance.a\n\n  to   = aws_instance.b\n\n}\n\n\n\nmoved {\n\n  from = aws_instance.b\n\n  to   = aws_instance.c\n\n}\n\nCopy\n\nRecording a sequence of moves in this way allows for successful upgrades for both configurations with objects at aws_instance.a and configurations with objects at aws_instance.b. In both cases, Terraform treats the existing object as if it had been originally created as aws_instance.c.\n\nEdit this page on GitHub\n\nOn this page:\n\nRefactoring\nmoved Block Syntax\nRenaming a Resource\nEnabling count or for_each For a Resource\nRenaming a Module Call\nEnabling count or for_each For a Module Call\nSplitting One Module into Multiple\nRemoving moved Blocks\nTheme\nDark\nLight\nSystem\nCertifications\nSystem Status\nCookie Manager\nTerms of Use\nSecurity\nPrivacy\nTrademark Policy\nTrade Controls\nGive Feedback\n(opens in new tab)\nWe use cookies & other similar technology to collect data to improve your experience on our site, as described in our Privacy Policy and Cookie Policy.\nManage PreferencesACCEPT"
  },
  {
    "title": "Data Sources - Configuration Language | Terraform | HashiCorp Developer",
    "url": "https://developer.hashicorp.com/terraform/language/v1.9.x/data-sources",
    "html": "Skip to main content\nNews\nHashiCorp to join IBM to accelerate multi-cloud automation.\nRead the blog →\nDismiss alert\nTerraform\nInstall\nTutorials\nDocumentation\nRegistry\n(opens in new tab)\nTry Cloud\n(opens in new tab)\nSearch\nCommand or control key\nK key\nTerraform Home\nConfiguration Language\nConfiguration Language\nTerraform Language\nOverview\nStyle Guide\nFiles and Directories\nSyntax\nResources\nData Sources\nProviders\nVariables and Outputs\nModules\nChecks\nImport\nExpressions\nFunctions\nTerraform Settings\nState\nTests\nUpgrading to Terraform v1.7\nv1.x Compatibility Promises\nTerraform Internals\nResources\nTutorial Library\nCertifications\nCommunity Forum\n(opens in new tab)\nSupport\n(opens in new tab)\nGitHub\n(opens in new tab)\nTerraform Registry\n(opens in new tab)\n\nYou are viewing documentation for pre-release version v1.9.0 (alpha). View latest version.\n\nDeveloper\nTerraform\nConfiguration Language v1.9.x\nData Sources\nv1.9.0 (alpha)\nData Sources\n\nData sources allow Terraform to use information defined outside of Terraform, defined by another separate Terraform configuration, or modified by functions.\n\nHands-on: Try the Query Data Sources tutorial.\n\nEach provider may offer data sources alongside its set of resource types.\n\nUsing Data Sources\n\nA data source is accessed via a special kind of resource known as a data resource, declared using a data block:\n\ndata \"aws_ami\" \"example\" {\n\n  most_recent = true\n\n\n\n  owners = [\"self\"]\n\n  tags = {\n\n    Name   = \"app-server\"\n\n    Tested = \"true\"\n\n  }\n\n}\n\nCopy\n\nA data block requests that Terraform read from a given data source (\"aws_ami\") and export the result under the given local name (\"example\"). The name is used to refer to this resource from elsewhere in the same Terraform module, but has no significance outside of the scope of a module.\n\nThe data source and name together serve as an identifier for a given resource and so must be unique within a module.\n\nWithin the block body (between { and }) are query constraints defined by the data source. Most arguments in this section depend on the data source, and indeed in this example most_recent, owners and tags are all arguments defined specifically for the aws_ami data source.\n\nWhen distinguishing from data resources, the primary kind of resource (as declared by a resource block) is known as a managed resource. Both kinds of resources take arguments and export attributes for use in configuration, but while managed resources cause Terraform to create, update, and delete infrastructure objects, data resources cause Terraform only to read objects. For brevity, managed resources are often referred to just as \"resources\" when the meaning is clear from context.\n\nData Source Arguments\n\nEach data resource is associated with a single data source, which determines the kind of object (or objects) it reads and what query constraint arguments are available.\n\nEach data source in turn belongs to a provider, which is a plugin for Terraform that offers a collection of resource types and data sources that most often belong to a single cloud or on-premises infrastructure platform.\n\nMost of the items within the body of a data block are defined by and specific to the selected data source, and these arguments can make full use of expressions and other dynamic Terraform language features.\n\nHowever, there are some \"meta-arguments\" that are defined by Terraform itself and apply across all data sources. These arguments often have additional restrictions on what language features can be used with them, and are described in more detail in the following sections.\n\nData Resource Behavior\n\nTerraform reads data resources during the planning phase when possible, but announces in the plan when it must defer reading resources until the apply phase to preserve the order of operations. Terraform defers reading data resources in the following situations:\n\nAt least one of the given arguments is a managed resource attribute or other value that Terraform cannot predict until the apply step.\nThe data resource depends directly on a managed resource that itself has planned changes in the current plan.\nThe data resource has custom conditions and it depends directly or indirectly on a managed resource that itself has planned changes in the current plan.\n\nRefer to Data Resource Dependencies for details on what it means for a data resource to depend on other objects. Any resulting attribute of such a data resource will be unknown during planning, so it cannot be used in situations where values must be fully known.\n\nLocal-only Data Sources\n\nWhile many data sources correspond to an infrastructure object type that is accessed via a remote network API, some specialized data sources operate only within Terraform itself, calculating some results and exposing them for use elsewhere.\n\nFor example, local-only data sources exist for rendering templates, reading local files, and rendering AWS IAM policies.\n\nThe behavior of local-only data sources is the same as all other data sources, but their result data exists only temporarily during a Terraform operation, and is re-calculated each time a new plan is created.\n\nData Resource Dependencies\n\nData resources have the same dependency resolution behavior as defined for managed resources. Setting the depends_on meta-argument within data blocks defers reading of the data source until after all changes to the dependencies have been applied.\n\nIn order to ensure that data sources are accessing the most up to date information possible in a wide variety of use cases, arguments directly referencing managed resources are treated the same as if the resource was listed in depends_on. This behavior can be avoided when desired by indirectly referencing the managed resource values through a local value, unless the data resource itself has custom conditions.\n\nNOTE: In Terraform 0.12 and earlier, due to the data resource behavior of deferring the read until the apply phase when depending on values that are not yet known, using depends_on with data resources will force the read to always be deferred to the apply phase, and therefore a configuration that uses depends_on with a data resource can never converge. Due to this behavior, we do not recommend using depends_on with data resources.\n\nCustom Condition Checks\n\nYou can use precondition and postcondition blocks to specify assumptions and guarantees about how the data source operates. The following examples creates a postcondition that checks whether the AMI has the correct tags.\n\ndata \"aws_ami\" \"example\" {\n\n  id = var.aws_ami_id\n\n\n\n  lifecycle {\n\n    # The AMI ID must refer to an existing AMI that has the tag \"nomad-server\".\n\n    postcondition {\n\n      condition     = self.tags[\"Component\"] == \"nomad-server\"\n\n      error_message = \"tags[\\\"Component\\\"] must be \\\"nomad-server\\\".\"\n\n    }\n\n  }\n\n}\n\nCopy\n\nCustom conditions can help capture assumptions, helping future maintainers understand the configuration design and intent. They also return useful information about errors earlier and in context, helping consumers more easily diagnose issues in their configurations.\n\nRefer to Custom Condition Checks for more details.\n\nMultiple Resource Instances\n\nData resources support count and for_each meta-arguments as defined for managed resources, with the same syntax and behavior.\n\nAs with managed resources, when count or for_each is present it is important to distinguish the resource itself from the multiple resource instances it creates. Each instance will separately read from its data source with its own variant of the constraint arguments, producing an indexed result.\n\nSelecting a Non-default Provider Configuration\n\nData resources support the provider meta-argument as defined for managed resources, with the same syntax and behavior.\n\nLifecycle Customizations\n\nData resources do not have any customization settings available for their lifecycle. However, the lifecycle block is reserved for future versions.\n\nExample\n\nA data source configuration looks like the following:\n\n# Find the latest available AMI that is tagged with Component = web\n\ndata \"aws_ami\" \"web\" {\n\n  filter {\n\n    name   = \"state\"\n\n    values = [\"available\"]\n\n  }\n\n\n\n  filter {\n\n    name   = \"tag:Component\"\n\n    values = [\"web\"]\n\n  }\n\n\n\n  most_recent = true\n\n}\n\nCopy\nDescription\n\nThe data block creates a data instance of the given type (first block label) and name (second block label). The combination of the type and name must be unique.\n\nWithin the block (the { }) is configuration for the data instance. The configuration is dependent on the type; as with resources, each provider on the Terraform Registry has its own documentation for configuring and using the data types it provides.\n\nEach data instance will export one or more attributes, which can be used in other resources as reference expressions of the form data.<TYPE>.<NAME>.<ATTRIBUTE>. For example:\n\nresource \"aws_instance\" \"web\" {\n\n  ami           = data.aws_ami.web.id\n\n  instance_type = \"t1.micro\"\n\n}\n\nCopy\nMeta-Arguments\n\nAs data sources are essentially a read only subset of resources, they also support the same meta-arguments of resources with the exception of the lifecycle configuration block.\n\nNon-Default Provider Configurations\n\nSimilarly to resources, when a module has multiple configurations for the same provider you can specify which configuration to use with the provider meta-argument:\n\ndata \"aws_ami\" \"web\" {\n\n  provider = aws.west\n\n\n\n  # ...\n\n}\n\nCopy\n\nSee The Resource provider Meta-Argument for more information.\n\nData Source Lifecycle\n\nIf the arguments of a data instance contain no references to computed values, such as attributes of resources that have not yet been created, then the data instance will be read and its state updated during Terraform's \"refresh\" phase, which by default runs prior to creating a plan. This ensures that the retrieved data is available for use during planning and the diff will show the real values obtained.\n\nData instance arguments may refer to computed values, in which case the attributes of the instance itself cannot be resolved until all of its arguments are defined. In this case, refreshing the data instance will be deferred until the \"apply\" phase, and all interpolations of the data instance attributes will show as \"computed\" in the plan since the values are not yet known.\n\nOn this page:\n\nData Sources\nUsing Data Sources\nData Source Arguments\nData Resource Behavior\nLocal-only Data Sources\nData Resource Dependencies\nCustom Condition Checks\nMultiple Resource Instances\nSelecting a Non-default Provider Configuration\nLifecycle Customizations\nExample\nDescription\nMeta-Arguments\nData Source Lifecycle\nTheme\nDark\nLight\nSystem\nCertifications\nSystem Status\nCookie Manager\nTerms of Use\nSecurity\nPrivacy\nTrademark Policy\nTrade Controls\nGive Feedback\n(opens in new tab)\nWe use cookies & other similar technology to collect data to improve your experience on our site, as described in our Privacy Policy and Cookie Policy.\nManage PreferencesACCEPT"
  },
  {
    "title": "Data Sources - Configuration Language | Terraform | HashiCorp Developer",
    "url": "https://developer.hashicorp.com/terraform/language/v1.5.x/data-sources",
    "html": "Skip to main content\nNews\nHashiCorp to join IBM to accelerate multi-cloud automation.\nRead the blog →\nDismiss alert\nTerraform\nInstall\nTutorials\nDocumentation\nRegistry\n(opens in new tab)\nTry Cloud\n(opens in new tab)\nSearch\nCommand or control key\nK key\nTerraform Home\nConfiguration Language\nConfiguration Language\nTerraform Language\nOverview\nFiles and Directories\nSyntax\nResources\nData Sources\nProviders\nVariables and Outputs\nModules\nChecks\nImport\nExpressions\nFunctions\nTerraform Settings\nState\nUpgrading to Terraform v1.5\nv1.x Compatibility Promises\nTerraform Internals\nResources\nTutorial Library\nCertifications\nCommunity Forum\n(opens in new tab)\nSupport\n(opens in new tab)\nGitHub\n(opens in new tab)\nTerraform Registry\n(opens in new tab)\n\nYou are viewing documentation for version v1.5.x. View latest version.\n\nDeveloper\nTerraform\nConfiguration Language v1.5.x\nData Sources\nv1.5.x\nData Sources\n\nData sources allow Terraform to use information defined outside of Terraform, defined by another separate Terraform configuration, or modified by functions.\n\nHands-on: Try the Query Data Sources tutorial.\n\nEach provider may offer data sources alongside its set of resource types.\n\nUsing Data Sources\n\nA data source is accessed via a special kind of resource known as a data resource, declared using a data block:\n\ndata \"aws_ami\" \"example\" {\n\n  most_recent = true\n\n\n\n  owners = [\"self\"]\n\n  tags = {\n\n    Name   = \"app-server\"\n\n    Tested = \"true\"\n\n  }\n\n}\n\nCopy\n\nA data block requests that Terraform read from a given data source (\"aws_ami\") and export the result under the given local name (\"example\"). The name is used to refer to this resource from elsewhere in the same Terraform module, but has no significance outside of the scope of a module.\n\nThe data source and name together serve as an identifier for a given resource and so must be unique within a module.\n\nWithin the block body (between { and }) are query constraints defined by the data source. Most arguments in this section depend on the data source, and indeed in this example most_recent, owners and tags are all arguments defined specifically for the aws_ami data source.\n\nWhen distinguishing from data resources, the primary kind of resource (as declared by a resource block) is known as a managed resource. Both kinds of resources take arguments and export attributes for use in configuration, but while managed resources cause Terraform to create, update, and delete infrastructure objects, data resources cause Terraform only to read objects. For brevity, managed resources are often referred to just as \"resources\" when the meaning is clear from context.\n\nData Source Arguments\n\nEach data resource is associated with a single data source, which determines the kind of object (or objects) it reads and what query constraint arguments are available.\n\nEach data source in turn belongs to a provider, which is a plugin for Terraform that offers a collection of resource types and data sources that most often belong to a single cloud or on-premises infrastructure platform.\n\nMost of the items within the body of a data block are defined by and specific to the selected data source, and these arguments can make full use of expressions and other dynamic Terraform language features.\n\nHowever, there are some \"meta-arguments\" that are defined by Terraform itself and apply across all data sources. These arguments often have additional restrictions on what language features can be used with them, and are described in more detail in the following sections.\n\nData Resource Behavior\n\nTerraform reads data resources during the planning phase when possible, but announces in the plan when it must defer reading resources until the apply phase to preserve the order of operations. Terraform defers reading data resources in the following situations:\n\nAt least one of the given arguments is a managed resource attribute or other value that Terraform cannot predict until the apply step.\nThe data resource depends directly on a managed resource that itself has planned changes in the current plan.\nThe data resource has custom conditions and it depends directly or indirectly on a managed resource that itself has planned changes in the current plan.\n\nRefer to Data Resource Dependencies for details on what it means for a data resource to depend on other objects. Any resulting attribute of such a data resource will be unknown during planning, so it cannot be used in situations where values must be fully known.\n\nLocal-only Data Sources\n\nWhile many data sources correspond to an infrastructure object type that is accessed via a remote network API, some specialized data sources operate only within Terraform itself, calculating some results and exposing them for use elsewhere.\n\nFor example, local-only data sources exist for rendering templates, reading local files, and rendering AWS IAM policies.\n\nThe behavior of local-only data sources is the same as all other data sources, but their result data exists only temporarily during a Terraform operation, and is re-calculated each time a new plan is created.\n\nData Resource Dependencies\n\nData resources have the same dependency resolution behavior as defined for managed resources. Setting the depends_on meta-argument within data blocks defers reading of the data source until after all changes to the dependencies have been applied.\n\nIn order to ensure that data sources are accessing the most up to date information possible in a wide variety of use cases, arguments directly referencing managed resources are treated the same as if the resource was listed in depends_on. This behavior can be avoided when desired by indirectly referencing the managed resource values through a local value, unless the data resource itself has custom conditions.\n\nNOTE: In Terraform 0.12 and earlier, due to the data resource behavior of deferring the read until the apply phase when depending on values that are not yet known, using depends_on with data resources will force the read to always be deferred to the apply phase, and therefore a configuration that uses depends_on with a data resource can never converge. Due to this behavior, we do not recommend using depends_on with data resources.\n\nCustom Condition Checks\n\nYou can use precondition and postcondition blocks to specify assumptions and guarantees about how the data source operates. The following examples creates a postcondition that checks whether the AMI has the correct tags.\n\ndata \"aws_ami\" \"example\" {\n\n  id = var.aws_ami_id\n\n\n\n  lifecycle {\n\n    # The AMI ID must refer to an existing AMI that has the tag \"nomad-server\".\n\n    postcondition {\n\n      condition     = self.tags[\"Component\"] == \"nomad-server\"\n\n      error_message = \"tags[\\\"Component\\\"] must be \\\"nomad-server\\\".\"\n\n    }\n\n  }\n\n}\n\nCopy\n\nCustom conditions can help capture assumptions, helping future maintainers understand the configuration design and intent. They also return useful information about errors earlier and in context, helping consumers more easily diagnose issues in their configurations.\n\nRefer to Custom Condition Checks for more details.\n\nMultiple Resource Instances\n\nData resources support count and for_each meta-arguments as defined for managed resources, with the same syntax and behavior.\n\nAs with managed resources, when count or for_each is present it is important to distinguish the resource itself from the multiple resource instances it creates. Each instance will separately read from its data source with its own variant of the constraint arguments, producing an indexed result.\n\nSelecting a Non-default Provider Configuration\n\nData resources support the provider meta-argument as defined for managed resources, with the same syntax and behavior.\n\nLifecycle Customizations\n\nData resources do not have any customization settings available for their lifecycle. However, the lifecycle block is reserved for future versions.\n\nExample\n\nA data source configuration looks like the following:\n\n# Find the latest available AMI that is tagged with Component = web\n\ndata \"aws_ami\" \"web\" {\n\n  filter {\n\n    name   = \"state\"\n\n    values = [\"available\"]\n\n  }\n\n\n\n  filter {\n\n    name   = \"tag:Component\"\n\n    values = [\"web\"]\n\n  }\n\n\n\n  most_recent = true\n\n}\n\nCopy\nDescription\n\nThe data block creates a data instance of the given type (first block label) and name (second block label). The combination of the type and name must be unique.\n\nWithin the block (the { }) is configuration for the data instance. The configuration is dependent on the type; as with resources, each provider on the Terraform Registry has its own documentation for configuring and using the data types it provides.\n\nEach data instance will export one or more attributes, which can be used in other resources as reference expressions of the form data.<TYPE>.<NAME>.<ATTRIBUTE>. For example:\n\nresource \"aws_instance\" \"web\" {\n\n  ami           = data.aws_ami.web.id\n\n  instance_type = \"t1.micro\"\n\n}\n\nCopy\nMeta-Arguments\n\nAs data sources are essentially a read only subset of resources, they also support the same meta-arguments of resources with the exception of the lifecycle configuration block.\n\nNon-Default Provider Configurations\n\nSimilarly to resources, when a module has multiple configurations for the same provider you can specify which configuration to use with the provider meta-argument:\n\ndata \"aws_ami\" \"web\" {\n\n  provider = aws.west\n\n\n\n  # ...\n\n}\n\nCopy\n\nSee The Resource provider Meta-Argument for more information.\n\nData Source Lifecycle\n\nIf the arguments of a data instance contain no references to computed values, such as attributes of resources that have not yet been created, then the data instance will be read and its state updated during Terraform's \"refresh\" phase, which by default runs prior to creating a plan. This ensures that the retrieved data is available for use during planning and the diff will show the real values obtained.\n\nData instance arguments may refer to computed values, in which case the attributes of the instance itself cannot be resolved until all of its arguments are defined. In this case, refreshing the data instance will be deferred until the \"apply\" phase, and all interpolations of the data instance attributes will show as \"computed\" in the plan since the values are not yet known.\n\nOn this page:\n\nData Sources\nUsing Data Sources\nData Source Arguments\nData Resource Behavior\nLocal-only Data Sources\nData Resource Dependencies\nCustom Condition Checks\nMultiple Resource Instances\nSelecting a Non-default Provider Configuration\nLifecycle Customizations\nExample\nDescription\nMeta-Arguments\nData Source Lifecycle\nTheme\nDark\nLight\nSystem\nCertifications\nSystem Status\nCookie Manager\nTerms of Use\nSecurity\nPrivacy\nTrademark Policy\nTrade Controls\nGive Feedback\n(opens in new tab)\nWe use cookies & other similar technology to collect data to improve your experience on our site, as described in our Privacy Policy and Cookie Policy.\nManage PreferencesACCEPT"
  },
  {
    "title": "moved block configuration reference | Terraform | HashiCorp Developer",
    "url": "https://developer.hashicorp.com/terraform/language/v1.3.x/moved",
    "html": "Skip to main content\nNews\nHashiCorp to join IBM to accelerate multi-cloud automation.\nRead the blog →\nDismiss alert\nTerraform\nInstall\nTutorials\nDocumentation\nRegistry\n(opens in new tab)\nTry Cloud\n(opens in new tab)\nSearch\nCommand or control key\nK key\nTerraform Home\nConfiguration Language\nConfiguration Language\nTerraform Language\nOverview\nStyle Guide\nFiles and Directories\nSyntax\nResources\nData Sources\nProviders\nVariables and Outputs\nModules\nMoved block\nExpressions\nFunctions\nTerraform Settings\nState\nUpgrading to Terraform v1.3\nv1.x Compatibility Promises\nTerraform Internals\nResources\nTutorial Library\nCertifications\nCommunity Forum\n(opens in new tab)\nSupport\n(opens in new tab)\nGitHub\n(opens in new tab)\nTerraform Registry\n(opens in new tab)\n\nYou are viewing documentation for version v1.3.x. View latest version.\n\nDeveloper\nTerraform\nConfiguration Language v1.3.x\nMoved block\nv1.3.x\nMoved block configuration reference\n\nThis topic provides reference information for the moved block.\n\nIntroduction\n\nThe moved block programmatically changes the address of a resource. Refer to Refactoring for details about how to use the moved block in your Terraform configurations.\n\nConfiguration model\n\nThe following list outlines field hierarchy, language-specific data types, and requirements in the moved block.\n\nmoved: map\nfrom: string\nto: string\nComplete configuration\n\nWhen every field is defined, a moved block has the following form:\n\nmoved = {\n\n    from = <old address for the resource>\n\n    to = <new address for the resource>\n\n}\n\nCopy\nSpecification\n\nThis section provides details about the fields you can configure in the moved block.\n\nmoved\n\nMap that specifies addresses for the resource. The following table describes the fields you can set in the moved block.\n\nField\tDescription\tType\tRequired\nfrom\tSpecifies a resource's previous address. The syntax allows Terraform to select modules, resources, and resources inside child modules.\tstring\trequired\nto\tSpecifies the new address to relocate the resource to. The syntax allows Terraform to select modules, resources, and resources inside child modules.\tstring\trequired\n\nBefore creating a new plan for the resource specified in the to field, Terraform checks the state for an existing object at the address specified in the from field. Terraform renames existing objects to the string specified in the to field and then creates a plan. The plan directs Terraform to provision the resource specified in the from field as the resource specified in the to field. As a result, Terraform does not destroy the resource during the Terraform run.\n\nExample\n\nThe following example moves an AWS instance from address aws_instance.a to aws_instance.b:\n\nmoved {\n\n  from = aws_instance.a\n\n  to   = aws_instance.b\n\n}\n\nCopy\n\nOn this page:\n\nMoved block configuration reference\nIntroduction\nConfiguration model\nComplete configuration\nSpecification\nExample\nTheme\nDark\nLight\nSystem\nCertifications\nSystem Status\nCookie Manager\nTerms of Use\nSecurity\nPrivacy\nTrademark Policy\nTrade Controls\nGive Feedback\n(opens in new tab)\nWe use cookies & other similar technology to collect data to improve your experience on our site, as described in our Privacy Policy and Cookie Policy.\nManage PreferencesACCEPT"
  },
  {
    "title": "Data Sources - Configuration Language | Terraform | HashiCorp Developer",
    "url": "https://developer.hashicorp.com/terraform/language/v1.6.x/data-sources",
    "html": "Skip to main content\nNews\nHashiCorp to join IBM to accelerate multi-cloud automation.\nRead the blog →\nDismiss alert\nTerraform\nInstall\nTutorials\nDocumentation\nRegistry\n(opens in new tab)\nTry Cloud\n(opens in new tab)\nSearch\nCommand or control key\nK key\nTerraform Home\nConfiguration Language\nConfiguration Language\nTerraform Language\nOverview\nStyle Guide\nFiles and Directories\nSyntax\nResources\nData Sources\nProviders\nVariables and Outputs\nModules\nMoved block\nChecks\nImport\nExpressions\nFunctions\nTerraform Settings\nState\nTests\nUpgrading to Terraform v1.6\nv1.x Compatibility Promises\nTerraform Internals\nResources\nTutorial Library\nCertifications\nCommunity Forum\n(opens in new tab)\nSupport\n(opens in new tab)\nGitHub\n(opens in new tab)\nTerraform Registry\n(opens in new tab)\n\nYou are viewing documentation for version v1.6.x. View latest version.\n\nDeveloper\nTerraform\nConfiguration Language v1.6.x\nData Sources\nv1.6.x\nData Sources\n\nData sources allow Terraform to use information defined outside of Terraform, defined by another separate Terraform configuration, or modified by functions.\n\nHands-on: Try the Query Data Sources tutorial.\n\nEach provider may offer data sources alongside its set of resource types.\n\nUsing Data Sources\n\nA data source is accessed via a special kind of resource known as a data resource, declared using a data block:\n\ndata \"aws_ami\" \"example\" {\n\n  most_recent = true\n\n\n\n  owners = [\"self\"]\n\n  tags = {\n\n    Name   = \"app-server\"\n\n    Tested = \"true\"\n\n  }\n\n}\n\nCopy\n\nA data block requests that Terraform read from a given data source (\"aws_ami\") and export the result under the given local name (\"example\"). The name is used to refer to this resource from elsewhere in the same Terraform module, but has no significance outside of the scope of a module.\n\nThe data source and name together serve as an identifier for a given resource and so must be unique within a module.\n\nWithin the block body (between { and }) are query constraints defined by the data source. Most arguments in this section depend on the data source, and indeed in this example most_recent, owners and tags are all arguments defined specifically for the aws_ami data source.\n\nWhen distinguishing from data resources, the primary kind of resource (as declared by a resource block) is known as a managed resource. Both kinds of resources take arguments and export attributes for use in configuration, but while managed resources cause Terraform to create, update, and delete infrastructure objects, data resources cause Terraform only to read objects. For brevity, managed resources are often referred to just as \"resources\" when the meaning is clear from context.\n\nData Source Arguments\n\nEach data resource is associated with a single data source, which determines the kind of object (or objects) it reads and what query constraint arguments are available.\n\nEach data source in turn belongs to a provider, which is a plugin for Terraform that offers a collection of resource types and data sources that most often belong to a single cloud or on-premises infrastructure platform.\n\nMost of the items within the body of a data block are defined by and specific to the selected data source, and these arguments can make full use of expressions and other dynamic Terraform language features.\n\nHowever, there are some \"meta-arguments\" that are defined by Terraform itself and apply across all data sources. These arguments often have additional restrictions on what language features can be used with them, and are described in more detail in the following sections.\n\nData Resource Behavior\n\nTerraform reads data resources during the planning phase when possible, but announces in the plan when it must defer reading resources until the apply phase to preserve the order of operations. Terraform defers reading data resources in the following situations:\n\nAt least one of the given arguments is a managed resource attribute or other value that Terraform cannot predict until the apply step.\nThe data resource depends directly on a managed resource that itself has planned changes in the current plan.\nThe data resource has custom conditions and it depends directly or indirectly on a managed resource that itself has planned changes in the current plan.\n\nRefer to Data Resource Dependencies for details on what it means for a data resource to depend on other objects. Any resulting attribute of such a data resource will be unknown during planning, so it cannot be used in situations where values must be fully known.\n\nLocal-only Data Sources\n\nWhile many data sources correspond to an infrastructure object type that is accessed via a remote network API, some specialized data sources operate only within Terraform itself, calculating some results and exposing them for use elsewhere.\n\nFor example, local-only data sources exist for rendering templates, reading local files, and rendering AWS IAM policies.\n\nThe behavior of local-only data sources is the same as all other data sources, but their result data exists only temporarily during a Terraform operation, and is re-calculated each time a new plan is created.\n\nData Resource Dependencies\n\nData resources have the same dependency resolution behavior as defined for managed resources. Setting the depends_on meta-argument within data blocks defers reading of the data source until after all changes to the dependencies have been applied.\n\nIn order to ensure that data sources are accessing the most up to date information possible in a wide variety of use cases, arguments directly referencing managed resources are treated the same as if the resource was listed in depends_on. This behavior can be avoided when desired by indirectly referencing the managed resource values through a local value, unless the data resource itself has custom conditions.\n\nNOTE: In Terraform 0.12 and earlier, due to the data resource behavior of deferring the read until the apply phase when depending on values that are not yet known, using depends_on with data resources will force the read to always be deferred to the apply phase, and therefore a configuration that uses depends_on with a data resource can never converge. Due to this behavior, we do not recommend using depends_on with data resources.\n\nCustom Condition Checks\n\nYou can use precondition and postcondition blocks to specify assumptions and guarantees about how the data source operates. The following examples creates a postcondition that checks whether the AMI has the correct tags.\n\ndata \"aws_ami\" \"example\" {\n\n  id = var.aws_ami_id\n\n\n\n  lifecycle {\n\n    # The AMI ID must refer to an existing AMI that has the tag \"nomad-server\".\n\n    postcondition {\n\n      condition     = self.tags[\"Component\"] == \"nomad-server\"\n\n      error_message = \"tags[\\\"Component\\\"] must be \\\"nomad-server\\\".\"\n\n    }\n\n  }\n\n}\n\nCopy\n\nCustom conditions can help capture assumptions, helping future maintainers understand the configuration design and intent. They also return useful information about errors earlier and in context, helping consumers more easily diagnose issues in their configurations.\n\nRefer to Custom Condition Checks for more details.\n\nMultiple Resource Instances\n\nData resources support count and for_each meta-arguments as defined for managed resources, with the same syntax and behavior.\n\nAs with managed resources, when count or for_each is present it is important to distinguish the resource itself from the multiple resource instances it creates. Each instance will separately read from its data source with its own variant of the constraint arguments, producing an indexed result.\n\nSelecting a Non-default Provider Configuration\n\nData resources support the provider meta-argument as defined for managed resources, with the same syntax and behavior.\n\nLifecycle Customizations\n\nData resources do not have any customization settings available for their lifecycle. However, the lifecycle block is reserved for future versions.\n\nExample\n\nA data source configuration looks like the following:\n\n# Find the latest available AMI that is tagged with Component = web\n\ndata \"aws_ami\" \"web\" {\n\n  filter {\n\n    name   = \"state\"\n\n    values = [\"available\"]\n\n  }\n\n\n\n  filter {\n\n    name   = \"tag:Component\"\n\n    values = [\"web\"]\n\n  }\n\n\n\n  most_recent = true\n\n}\n\nCopy\nDescription\n\nThe data block creates a data instance of the given type (first block label) and name (second block label). The combination of the type and name must be unique.\n\nWithin the block (the { }) is configuration for the data instance. The configuration is dependent on the type; as with resources, each provider on the Terraform Registry has its own documentation for configuring and using the data types it provides.\n\nEach data instance will export one or more attributes, which can be used in other resources as reference expressions of the form data.<TYPE>.<NAME>.<ATTRIBUTE>. For example:\n\nresource \"aws_instance\" \"web\" {\n\n  ami           = data.aws_ami.web.id\n\n  instance_type = \"t1.micro\"\n\n}\n\nCopy\nMeta-Arguments\n\nAs data sources are essentially a read only subset of resources, they also support the same meta-arguments of resources with the exception of the lifecycle configuration block.\n\nNon-Default Provider Configurations\n\nSimilarly to resources, when a module has multiple configurations for the same provider you can specify which configuration to use with the provider meta-argument:\n\ndata \"aws_ami\" \"web\" {\n\n  provider = aws.west\n\n\n\n  # ...\n\n}\n\nCopy\n\nSee The Resource provider Meta-Argument for more information.\n\nData Source Lifecycle\n\nIf the arguments of a data instance contain no references to computed values, such as attributes of resources that have not yet been created, then the data instance will be read and its state updated during Terraform's \"refresh\" phase, which by default runs prior to creating a plan. This ensures that the retrieved data is available for use during planning and the diff will show the real values obtained.\n\nData instance arguments may refer to computed values, in which case the attributes of the instance itself cannot be resolved until all of its arguments are defined. In this case, refreshing the data instance will be deferred until the \"apply\" phase, and all interpolations of the data instance attributes will show as \"computed\" in the plan since the values are not yet known.\n\nOn this page:\n\nData Sources\nUsing Data Sources\nData Source Arguments\nData Resource Behavior\nLocal-only Data Sources\nData Resource Dependencies\nCustom Condition Checks\nMultiple Resource Instances\nSelecting a Non-default Provider Configuration\nLifecycle Customizations\nExample\nDescription\nMeta-Arguments\nData Source Lifecycle\nTheme\nDark\nLight\nSystem\nCertifications\nSystem Status\nCookie Manager\nTerms of Use\nSecurity\nPrivacy\nTrademark Policy\nTrade Controls\nGive Feedback\n(opens in new tab)\nWe use cookies & other similar technology to collect data to improve your experience on our site, as described in our Privacy Policy and Cookie Policy.\nManage PreferencesACCEPT"
  },
  {
    "title": "HashiCorp Developer",
    "url": "https://developer.hashicorp.com/terraform/language/v1.1.x/moved",
    "html": "News\nHashiCorp to join IBM to accelerate multi-cloud automation.\nRead the blog →\nDismiss alert\nProducts\nTutorials\nSearch\nCommand or control key\nK key\nThis page does not exist for version v1.1.x.\n\nPlease select either the most recent version or a valid version that includes the page you are looking for.\n\nGo back to Documentation\nTheme\nCertifications\nSystem Status\nCookie Manager\nTerms of Use\nSecurity\nPrivacy\nTrademark Policy\nTrade Controls\nGive Feedback\n(opens in new tab)"
  },
  {
    "title": "HashiCorp Developer",
    "url": "https://developer.hashicorp.com/terraform/language/v1.2.x/moved",
    "html": "News\nHashiCorp to join IBM to accelerate multi-cloud automation.\nRead the blog →\nDismiss alert\nProducts\nTutorials\nSearch\nCommand or control key\nK key\nThis page does not exist for version v1.2.x.\n\nPlease select either the most recent version or a valid version that includes the page you are looking for.\n\nGo back to Documentation\nTheme\nCertifications\nSystem Status\nCookie Manager\nTerms of Use\nSecurity\nPrivacy\nTrademark Policy\nTrade Controls\nGive Feedback\n(opens in new tab)"
  },
  {
    "title": "HashiCorp Developer",
    "url": "https://developer.hashicorp.com/terraform/language/v1.4.x/moved",
    "html": "News\nHashiCorp to join IBM to accelerate multi-cloud automation.\nRead the blog →\nDismiss alert\nProducts\nTutorials\nSearch\nCommand or control key\nK key\nThis page does not exist for version v1.4.x.\n\nPlease select either the most recent version or a valid version that includes the page you are looking for.\n\nGo back to Documentation\nTheme\nCertifications\nSystem Status\nCookie Manager\nTerms of Use\nSecurity\nPrivacy\nTrademark Policy\nTrade Controls\nGive Feedback\n(opens in new tab)"
  },
  {
    "title": "moved block configuration reference | Terraform | HashiCorp Developer",
    "url": "https://developer.hashicorp.com/terraform/language/v1.6.x/moved",
    "html": "Skip to main content\nNews\nHashiCorp to join IBM to accelerate multi-cloud automation.\nRead the blog →\nDismiss alert\nTerraform\nInstall\nTutorials\nDocumentation\nRegistry\n(opens in new tab)\nTry Cloud\n(opens in new tab)\nSearch\nCommand or control key\nK key\nTerraform Home\nConfiguration Language\nConfiguration Language\nTerraform Language\nOverview\nStyle Guide\nFiles and Directories\nSyntax\nResources\nData Sources\nProviders\nVariables and Outputs\nModules\nMoved block\nChecks\nImport\nExpressions\nFunctions\nTerraform Settings\nState\nTests\nUpgrading to Terraform v1.6\nv1.x Compatibility Promises\nTerraform Internals\nResources\nTutorial Library\nCertifications\nCommunity Forum\n(opens in new tab)\nSupport\n(opens in new tab)\nGitHub\n(opens in new tab)\nTerraform Registry\n(opens in new tab)\n\nYou are viewing documentation for version v1.6.x. View latest version.\n\nDeveloper\nTerraform\nConfiguration Language v1.6.x\nMoved block\nv1.6.x\nMoved block configuration reference\n\nThis topic provides reference information for the moved block.\n\nIntroduction\n\nThe moved block programmatically changes the address of a resource. Refer to Refactoring for details about how to use the moved block in your Terraform configurations.\n\nConfiguration model\n\nThe following list outlines field hierarchy, language-specific data types, and requirements in the moved block.\n\nmoved: map\nfrom: string\nto: string\nComplete configuration\n\nWhen every field is defined, a moved block has the following form:\n\nmoved = {\n\n    from = <old address for the resource>\n\n    to = <new address for the resource>\n\n}\n\nCopy\nSpecification\n\nThis section provides details about the fields you can configure in the moved block.\n\nmoved\n\nMap that specifies addresses for the resource. The following table describes the fields you can set in the moved block.\n\nField\tDescription\tType\tRequired\nfrom\tSpecifies a resource's previous address. The syntax allows Terraform to select modules, resources, and resources inside child modules.\tstring\trequired\nto\tSpecifies the new address to relocate the resource to. The syntax allows Terraform to select modules, resources, and resources inside child modules.\tstring\trequired\n\nBefore creating a new plan for the resource specified in the to field, Terraform checks the state for an existing object at the address specified in the from field. Terraform renames existing objects to the string specified in the to field and then creates a plan. The plan directs Terraform to provision the resource specified in the from field as the resource specified in the to field. As a result, Terraform does not destroy the resource during the Terraform run.\n\nExample\n\nThe following example moves an AWS instance from address aws_instance.a to aws_instance.b:\n\nmoved {\n\n  from = aws_instance.a\n\n  to   = aws_instance.b\n\n}\n\nCopy\n\nOn this page:\n\nMoved block configuration reference\nIntroduction\nConfiguration model\nComplete configuration\nSpecification\nExample\nTheme\nDark\nLight\nSystem\nCertifications\nSystem Status\nCookie Manager\nTerms of Use\nSecurity\nPrivacy\nTrademark Policy\nTrade Controls\nGive Feedback\n(opens in new tab)\nWe use cookies & other similar technology to collect data to improve your experience on our site, as described in our Privacy Policy and Cookie Policy.\nManage PreferencesACCEPT"
  },
  {
    "title": "moved block configuration reference | Terraform | HashiCorp Developer",
    "url": "https://developer.hashicorp.com/terraform/language/v1.7.x/moved",
    "html": "Skip to main content\nNews\nHashiCorp to join IBM to accelerate multi-cloud automation.\nRead the blog →\nDismiss alert\nTerraform\nInstall\nTutorials\nDocumentation\nRegistry\n(opens in new tab)\nTry Cloud\n(opens in new tab)\nSearch\nCommand or control key\nK key\nTerraform Home\nConfiguration Language\nConfiguration Language\nTerraform Language\nOverview\nStyle Guide\nFiles and Directories\nSyntax\nResources\nData Sources\nProviders\nVariables and Outputs\nModules\nMoved block\nChecks\nImport\nExpressions\nFunctions\nTerraform Settings\nState\nTests\nUpgrading to Terraform v1.7\nv1.x Compatibility Promises\nTerraform Internals\nResources\nTutorial Library\nCertifications\nCommunity Forum\n(opens in new tab)\nSupport\n(opens in new tab)\nGitHub\n(opens in new tab)\nTerraform Registry\n(opens in new tab)\n\nYou are viewing documentation for version v1.7.x. View latest version.\n\nDeveloper\nTerraform\nConfiguration Language v1.7.x\nMoved block\nv1.7.x\nMoved block configuration reference\n\nThis topic provides reference information for the moved block.\n\nIntroduction\n\nThe moved block programmatically changes the address of a resource. Refer to Refactoring for details about how to use the moved block in your Terraform configurations.\n\nConfiguration model\n\nThe following list outlines field hierarchy, language-specific data types, and requirements in the moved block.\n\nmoved: map\nfrom: string\nto: string\nComplete configuration\n\nWhen every field is defined, a moved block has the following form:\n\nmoved = {\n\n    from = <old address for the resource>\n\n    to = <new address for the resource>\n\n}\n\nCopy\nSpecification\n\nThis section provides details about the fields you can configure in the moved block.\n\nmoved\n\nMap that specifies addresses for the resource. The following table describes the fields you can set in the moved block.\n\nField\tDescription\tType\tRequired\nfrom\tSpecifies a resource's previous address. The syntax allows Terraform to select modules, resources, and resources inside child modules.\tstring\trequired\nto\tSpecifies the new address to relocate the resource to. The syntax allows Terraform to select modules, resources, and resources inside child modules.\tstring\trequired\n\nBefore creating a new plan for the resource specified in the to field, Terraform checks the state for an existing object at the address specified in the from field. Terraform renames existing objects to the string specified in the to field and then creates a plan. The plan directs Terraform to provision the resource specified in the from field as the resource specified in the to field. As a result, Terraform does not destroy the resource during the Terraform run.\n\nExample\n\nThe following example moves an AWS instance from address aws_instance.a to aws_instance.b:\n\nmoved {\n\n  from = aws_instance.a\n\n  to   = aws_instance.b\n\n}\n\nCopy\n\nOn this page:\n\nMoved block configuration reference\nIntroduction\nConfiguration model\nComplete configuration\nSpecification\nExample\nTheme\nDark\nLight\nSystem\nCertifications\nSystem Status\nCookie Manager\nTerms of Use\nSecurity\nPrivacy\nTrademark Policy\nTrade Controls\nGive Feedback\n(opens in new tab)\nWe use cookies & other similar technology to collect data to improve your experience on our site, as described in our Privacy Policy and Cookie Policy.\nManage PreferencesACCEPT"
  },
  {
    "title": "HashiCorp Developer",
    "url": "https://developer.hashicorp.com/terraform/language/v1.5.x/moved",
    "html": "News\nHashiCorp to join IBM to accelerate multi-cloud automation.\nRead the blog →\nDismiss alert\nProducts\nTutorials\nSearch\nCommand or control key\nK key\nMain Menu\nHashiCorp Developer\nTutorials\nProducts\nHashiCorp Cloud Platform\nTerraform\nPacker\nConsul\nVault\nBoundary\nNomad\nWaypoint\nVagrant\nThis page does not exist for version v1.5.x.\n\nPlease select either the most recent version or a valid version that includes the page you are looking for.\n\nGo back to Documentation\nTheme\nDark\nLight\nSystem\nCertifications\nSystem Status\nCookie Manager\nTerms of Use\nSecurity\nPrivacy\nTrademark Policy\nTrade Controls\nGive Feedback\n(opens in new tab)\nWe use cookies & other similar technology to collect data to improve your experience on our site, as described in our Privacy Policy and Cookie Policy.\nManage PreferencesACCEPT"
  },
  {
    "title": "Standard Module Structure | Terraform | HashiCorp Developer",
    "url": "https://developer.hashicorp.com/terraform/language/modules/develop/structure",
    "html": "Skip to main content\nNews\nHashiCorp to join IBM to accelerate multi-cloud automation.\nRead the blog →\nDismiss alert\nTerraform\nInstall\nTutorials\nDocumentation\nRegistry\n(opens in new tab)\nTry Cloud\n(opens in new tab)\nSearch\nCommand or control key\nK key\nTerraform Home\nConfiguration Language\nConfiguration Language\nStyle Guide\nFiles and Directories\nSyntax\nResources\nData Sources\nProviders\nVariables and Outputs\nModules\nOverview\nModule Blocks\nModule Sources\nMeta-Arguments\nModule Development\nOverview\nStandard Module Structure\nProviders Within Modules\nBest Practices: Module Composition\nPublishing Modules\nRefactoring Modules\nMoved block\nChecks\nImport\nExpressions\nFunctions\nTerraform Settings\nState\nTests\nUpgrading to Terraform v1.7\nv1.x Compatibility Promises\nTerraform Internals\nResources\nTutorial Library\nCertifications\nCommunity Forum\n(opens in new tab)\nSupport\n(opens in new tab)\nGitHub\n(opens in new tab)\nTerraform Registry\n(opens in new tab)\nDeveloper\nTerraform\nConfiguration Language\nModules\nModule Development\nStandard Module Structure\nv1.8.x (latest)\nStandard Module Structure\n\nThe standard module structure is a file and directory layout we recommend for reusable modules distributed in separate repositories. Terraform tooling is built to understand the standard module structure and use that structure to generate documentation, index modules for the module registry, and more.\n\nThe standard module structure expects the layout documented below. The list may appear long, but everything is optional except for the root module. Most modules don't need to do any extra work to follow the standard structure.\n\nRoot module. This is the only required element for the standard module structure. Terraform files must exist in the root directory of the repository. This should be the primary entrypoint for the module and is expected to be opinionated. For the Consul module the root module sets up a complete Consul cluster. It makes a lot of assumptions however, and we expect that advanced users will use specific nested modules to more carefully control what they want.\n\nREADME. The root module and any nested modules should have README files. This file should be named README or README.md. The latter will be treated as markdown. There should be a description of the module and what it should be used for. If you want to include an example for how this module can be used in combination with other resources, put it in an examples directory like this. Consider including a visual diagram depicting the infrastructure resources the module may create and their relationship.\n\nThe README doesn't need to document inputs or outputs of the module because tooling will automatically generate this. If you are linking to a file or embedding an image contained in the repository itself, use a commit-specific absolute URL so the link won't point to the wrong version of a resource in the future.\n\nLICENSE. The license under which this module is available. If you are publishing a module publicly, many organizations will not adopt a module unless a clear license is present. We recommend always having a license file, even if it is not an open source license.\n\nmain.tf, variables.tf, outputs.tf. These are the recommended filenames for a minimal module, even if they're empty. main.tf should be the primary entrypoint. For a simple module, this may be where all the resources are created. For a complex module, resource creation may be split into multiple files but any nested module calls should be in the main file. variables.tf and outputs.tf should contain the declarations for variables and outputs, respectively.\n\nVariables and outputs should have descriptions. All variables and outputs should have one or two sentence descriptions that explain their purpose. This is used for documentation. See the documentation for variable configuration and output configuration for more details.\n\nNested modules. Nested modules should exist under the modules/ subdirectory. Any nested module with a README.md is considered usable by an external user. If a README doesn't exist, it is considered for internal use only. These are purely advisory; Terraform will not actively deny usage of internal modules. Nested modules should be used to split complex behavior into multiple small modules that advanced users can carefully pick and choose. For example, the Consul module has a nested module for creating the Cluster that is separate from the module to setup necessary IAM policies. This allows a user to bring in their own IAM policy choices.\n\nIf the root module includes calls to nested modules, they should use relative paths like ./modules/consul-cluster so that Terraform will consider them to be part of the same repository or package, rather than downloading them again separately.\n\nIf a repository or package contains multiple nested modules, they should ideally be composable by the caller, rather than calling directly to each other and creating a deeply-nested tree of modules.\n\nExamples. Examples of using the module should exist under the examples/ subdirectory at the root of the repository. Each example may have a README to explain the goal and usage of the example. Examples for submodules should also be placed in the root examples/ directory.\n\nBecause examples will often be copied into other repositories for customization, any module blocks should have their source set to the address an external caller would use, not to a relative path.\n\nA minimal recommended module following the standard structure is shown below. While the root module is the only required element, we recommend the structure below as the minimum:\n\n$ tree minimal-module/\n\n.\n\n├── README.md\n\n├── main.tf\n\n├── variables.tf\n\n├── outputs.tf\n\nCopy\n\nA complete example of a module following the standard structure is shown below. This example includes all optional elements and is therefore the most complex a module can become:\n\n$ tree complete-module/\n\n.\n\n├── README.md\n\n├── main.tf\n\n├── variables.tf\n\n├── outputs.tf\n\n├── ...\n\n├── modules/\n\n│   ├── nestedA/\n\n│   │   ├── README.md\n\n│   │   ├── variables.tf\n\n│   │   ├── main.tf\n\n│   │   ├── outputs.tf\n\n│   ├── nestedB/\n\n│   ├── .../\n\n├── examples/\n\n│   ├── exampleA/\n\n│   │   ├── main.tf\n\n│   ├── exampleB/\n\n│   ├── .../\n\nCopy\nEdit this page on GitHub\nTheme\nDark\nLight\nSystem\nCertifications\nSystem Status\nCookie Manager\nTerms of Use\nSecurity\nPrivacy\nTrademark Policy\nTrade Controls\nGive Feedback\n(opens in new tab)\nWe use cookies & other similar technology to collect data to improve your experience on our site, as described in our Privacy Policy and Cookie Policy.\nManage PreferencesACCEPT"
  },
  {
    "title": "HashiCorp Developer",
    "url": "https://developer.hashicorp.com/terraform/language/v1.9.x/moved",
    "html": "News\nHashiCorp to join IBM to accelerate multi-cloud automation.\nRead the blog →\nDismiss alert\nProducts\nTutorials\nSearch\nCommand or control key\nK key\nMain Menu\nHashiCorp Developer\nTutorials\nProducts\nHashiCorp Cloud Platform\nTerraform\nPacker\nConsul\nVault\nBoundary\nNomad\nWaypoint\nVagrant\nThis page does not exist for version v1.9.x.\n\nPlease select either the most recent version or a valid version that includes the page you are looking for.\n\nGo back to Documentation\nTheme\nDark\nLight\nSystem\nCertifications\nSystem Status\nCookie Manager\nTerms of Use\nSecurity\nPrivacy\nTrademark Policy\nTrade Controls\nGive Feedback\n(opens in new tab)\nWe use cookies & other similar technology to collect data to improve your experience on our site, as described in our Privacy Policy and Cookie Policy.\nManage PreferencesACCEPT"
  },
  {
    "title": "Tests - Configuration Language | Terraform | HashiCorp Developer",
    "url": "https://developer.hashicorp.com/terraform/language/tests",
    "html": "Skip to main content\nNews\nHashiCorp to join IBM to accelerate multi-cloud automation.\nRead the blog →\nDismiss alert\nTerraform\nInstall\nTutorials\nDocumentation\nRegistry\n(opens in new tab)\nTry Cloud\n(opens in new tab)\nSearch\nCommand or control key\nK key\nTerraform Home\nConfiguration Language\nConfiguration Language\nStyle Guide\nFiles and Directories\nSyntax\nResources\nData Sources\nProviders\nVariables and Outputs\nModules\nMoved block\nChecks\nImport\nExpressions\nFunctions\nTerraform Settings\nState\nTests\nOverview\nMocks\nUpgrading to Terraform v1.7\nv1.x Compatibility Promises\nTerraform Internals\nResources\nTutorial Library\nCertifications\nCommunity Forum\n(opens in new tab)\nSupport\n(opens in new tab)\nGitHub\n(opens in new tab)\nTerraform Registry\n(opens in new tab)\nDeveloper\nTerraform\nConfiguration Language\nTests\nv1.8.x (latest)\nTests\n\nNote: This testing framework is available in Terraform v1.6.0 and later.\n\nTerraform tests let authors validate that module configuration updates do not introduce breaking changes. Tests run against test-specific, short-lived resources, preventing any risk to your existing infrastructure or state.\n\nIntegration or Unit testing\n\nBy default, tests within Terraform create real infrastructure and can run assertions and validations against that infrastructure. This is analogous to integration testing because you are testing Terraform's core functionality by executing operations and validating the infrastructure Terraform creates.\n\nYou can override the normal testing behavior by updating the command attribute within a run block (examples below). By default, each run block executes with command = apply instructing Terraform to execute a complete apply operation against your configuration. Replacing the command value with command = plan instructs Terraform to not create new infrastructure for this run block. This allows test authors to validate logical operations and custom conditions within their infrastructure in a process analogous to unit testing.\n\nTerraform v1.7.0 introduced the ability to mock data returned by the providers during a terraform test execution. This can be used to write more detailed and complete unit tests.\n\nSyntax\n\nEach Terraform test lives in a test file. Terraform discovers test files are based on their file extension: .tftest.hcl or .tftest.json.\n\nEach test file contains the following root level attributes and blocks:\n\nOne to many run blocks.\nZero to one variables block.\nZero to many provider blocks.\n\nTerraform executes run blocks in order, simulating a series of Terraform commands executing directly within the configuration directory. The order of the variables and provider blocks doesn't matter, Terraform processes all the values within these blocks at the beginning of the test operation. We recommend defining your variables and provider blocks first, at the beginning of the test file.\n\nExample\n\nThe following example demonstrates a simple Terraform configuration that creates an AWS S3 bucket, using an input variable to modify the name. We will create an example test file (below) that validates the buckets name is created as expected.\n\n# main.tf\n\n\n\nprovider \"aws\" {\n\n    region = \"eu-central-1\"\n\n}\n\n\n\nvariable \"bucket_prefix\" {\n\n  type = string\n\n}\n\n\n\nresource \"aws_s3_bucket\" \"bucket\" {\n\n  bucket = \"${var.bucket_prefix}-bucket\"\n\n}\n\n\n\noutput \"bucket_name\" {\n\n  value = aws_s3_bucket.bucket.bucket\n\n}\n\nCopy\n\nThe following test file runs a single Terraform plan command which creates the S3 bucket, and then validates the logic for calculating the name is correct by checking the actual name matches the expected name.\n\n# valid_string_concat.tftest.hcl\n\n\n\nvariables {\n\n  bucket_prefix = \"test\"\n\n}\n\n\n\nrun \"valid_string_concat\" {\n\n\n\n  command = plan\n\n\n\n  assert {\n\n    condition     = aws_s3_bucket.bucket.bucket == \"test-bucket\"\n\n    error_message = \"S3 bucket name did not match expected\"\n\n  }\n\n\n\n}\n\nCopy\nRun blocks\n\nEach run block has the following fields and blocks:\n\nField or Block Name\tDescription\tDefault Value\ncommand\tAn optional attribute, which is either apply or plan.\tapply\nplan_options.mode\tAn optional attribute, which is either normal or refresh-only.\tnormal\nplan_options.refresh\tAn optional boolean attribute.\ttrue\nplan_options.replace\tAn optional attribute containing a list of resource addresses referencing resources within the configuration under test.\t\nplan_options.target\tAn optional attribute containing a list of resource addresses referencing resources within the configuration under test.\t\nvariables\tAn optional variables block.\t\nmodule\tAn optional module block.\t\nproviders\tAn optional providers attribute.\t\nassert\tOptional assert blocks.\t\nexpect_failures\tAn optional attribute.\t\n\nThe command attribute and plan_options block tell Terraform which command and options to execute for each run block. The default operation, if you do not specify a command attribute or the plan_options block, is a normal Terraform apply operation.\n\nThe command attribute states whether the operation should be a plan or an apply operation.\n\nThe plan_options block allows test authors to customize the planning mode and options they would typically need to edit via command-line flags and options. We cover the -var and -var-file options in the Variables section.\n\nAssertions\n\nTerraform run block assertions are Custom Conditions, consisting of a condition and an error message.\n\nAt the conclusion of a Terraform test command execution, Terraform presents any failed assertions as part of a tests passed or failed status.\n\nAssertion References\n\nAssertions within tests can reference any existing named values that are available to other custom conditions within the main Terraform configuration.\n\nAdditionally, test assertions can directly reference outputs from current and previous run blocks. Pulling from the previous example, this is a valid condition: condition = output.bucket_name == \"test_bucket\".\n\nVariables\n\nYou can provide values for Input Variables within your configuration directly from your test files.\n\nThe test file syntax supports variables blocks at both the root level and within run blocks. Terraform passes all variable values from the test file into all run blocks within the file. You can override variable values for a particular run block with values provided directly within that run block.\n\nAdding to the test file from the example above:\n\n# variable_precedence.tftest.hcl\n\n\n\nvariables {\n\n  bucket_prefix = \"test\"\n\n}\n\n\n\nrun \"uses_root_level_value\" {\n\n\n\n  command = plan\n\n\n\n  assert {\n\n    condition     = aws_s3_bucket.bucket.bucket == \"test-bucket\"\n\n    error_message = \"S3 bucket name did not match expected\"\n\n  }\n\n\n\n}\n\n\n\nrun \"overrides_root_level_value\" {\n\n\n\n  command = plan\n\n\n\n  variables {\n\n    bucket_prefix = \"other\"\n\n  }\n\n\n\n  assert {\n\n    condition     = aws_s3_bucket.bucket.bucket == \"other-bucket\"\n\n    error_message = \"S3 bucket name did not match expected\"\n\n  }\n\n\n\n}\n\nCopy\n\nWe've added a second run block that specifies the bucket_prefix variable value as other, overriding the value test that is provided by the test file and used during the first run block.\n\nSpecify variables with the Command Line or definition files\n\nIn addition to specifying variable values via test files, the Terraform test command also supports the other typical mechanisms for specifying variable values.\n\nYou can specify values for variables across all tests with the Command Line and with Variable Definition Files.\n\nAs with the main configuration direction, Terraform will automatically load any variables defined in the automatic variable files within a test directory. The automatic variable files are terraform.tfvars, terraform.tfvars.json, and any files that end with .auto.tfvars or .auto.tfvars.json.\n\nNote: Variable values loaded from the automatic variable files within a test directory will only apply to tests also defined within the same test directory. Variables defined in all other ways will apply to all tests in a given test run.\n\nThis is particularly useful for using sensitive variables values and for configuring providers. Otherwise, testing files could directly expose those sensitive values.\n\nVariable definition precedence\n\nVariable Definition Precedence remains the same within tests, except for variable values that test files provide. The variables defined in test files take the highest precedence, overriding environment variables, variables files, or command-line input.\n\nFor tests defined in a test directory, any variable values defined in automatic variable files from the test directory will override values defined in automatic variable files from the main configuration directory.\n\nVariable References\n\nVariables you define within run blocks can refer to outputs from modules executed in earlier run blocks and variables defined at higher precedence levels. Variables defined within the file level variables block can only refer to global variables.\n\nFor example, the following code block shows how a variable can refer to higher precedence variables and previous run blocks:\n\nvariables {\n\n  global_value = \"some value\"\n\n}\n\n\n\nrun \"run_block_one\" {\n\n  variables {\n\n    local_value = var.global_value\n\n  }\n\n\n\n  # ...\n\n  # Some test assertions should go here.\n\n  # ...\n\n}\n\n\n\nrun \"run_block_two\" {\n\n  variables {\n\n    local_value = run.run_block_one.output_one\n\n  }\n\n\n\n  # ...\n\n  # Some test assertions should go here.\n\n  # ...\n\n}\n\nCopy\n\nAbove, the local_value in run_block_one gets its value from the global_value variable. This pattern is useful if you want to assign multiple variables the same value. You can specify a variable value once at the file level and then share it with different variables.\n\nIn comparison, local_value in run_block_two takes its value from the output value of output_one from run_block_one. This pattern is useful for passing values between run blocks, particularly if run blocks are executing different modules as detailed in the Modules section.\n\nProviders\n\nYou can set or override the required providers within the main configuration from your testing files by using provider and providers blocks and attributes.\n\nAt the root level of a Terraform testing file, you can define provider blocks as if Terraform were creating them within the main configuration. Terraform will then pass these provider blocks into its configuration as each run block executes.\n\nBy default, each provider you specify is directly available within each run block. You can customize the availability of providers within a given run block by using a providers attribute. The behavior and syntax for this block match the behavior of providers meta-argument.\n\nIf you do not provide provider configuration within a testing file, Terraform attempts to initialize any providers within its configuration using the provider's default settings. For example, any environment variables aimed at configuring providers are still available, and Terraform can use them to create default providers.\n\nBelow, we expand on our previous example to allow tests, instead of the configuration, to specify the region. In this example, we are going to test the following configuration file:\n\n# main.tf\n\n\n\nterraform {\n\n  required_providers {\n\n    aws = {\n\n      source = \"hashicorp/aws\"\n\n    }\n\n  }\n\n}\n\n\n\nvariable \"bucket_prefix\" {\n\n  type = string\n\n}\n\n\n\nresource \"aws_s3_bucket\" \"bucket\" {\n\n  bucket = \"${var.bucket_prefix}-bucket\"\n\n}\n\n\n\noutput \"bucket_name\" {\n\n  value = aws_s3_bucket.bucket.bucket\n\n}\n\nCopy\n\nWe can now define our provider blocks within the following test file:\n\n# customised_provider.tftest.hcl\n\n\n\nprovider \"aws\" {\n\n    region = \"eu-central-1\"\n\n}\n\n\n\nvariables {\n\n  bucket_prefix = \"test\"\n\n}\n\n\n\nrun \"valid_string_concat\" {\n\n\n\n  command = plan\n\n\n\n  assert {\n\n    condition     = aws_s3_bucket.bucket.bucket == \"test-bucket\"\n\n    error_message = \"S3 bucket name did not match expected\"\n\n  }\n\n\n\n}\n\nCopy\n\nWe can also create a more complex example configuration, that makes use of multiple providers and aliases:\n\n# main.tf\n\n\n\nterraform {\n\n  required_providers {\n\n    aws = {\n\n      source                = \"hashicorp/aws\"\n\n      configuration_aliases = [aws.secondary]\n\n    }\n\n  }\n\n}\n\n\n\nvariable \"bucket_prefix\" {\n\n  default = \"test\"\n\n  type    = string\n\n}\n\n\n\nresource \"aws_s3_bucket\" \"primary_bucket\" {\n\n  bucket = \"${var.bucket_prefix}-primary\"\n\n}\n\n\n\nresource \"aws_s3_bucket\" \"secondary_bucket\" {\n\n  provider = aws.secondary\n\n  bucket   = \"${var.bucket_prefix}-secondary\"\n\n}\n\nCopy\n\nWithin our test file we can specify multiple providers:\n\n# customised_providers.tftest.hcl\n\n\n\nprovider \"aws\" {\n\n  region = \"us-east-1\"\n\n}\n\n\n\nprovider \"aws\" {\n\n  alias  = \"secondary\"\n\n  region = \"eu-central-1\"\n\n}\n\n\n\nrun \"providers\" {\n\n\n\n  command = plan\n\n\n\n  assert {\n\n    condition     = aws_s3_bucket.primary_bucket.bucket == \"test-primary\"\n\n    error_message = \"invalid value for primary S3 bucket\"\n\n  }\n\n\n\n  assert {\n\n    condition     = aws_s3_bucket.secondary_bucket.bucket == \"test-secondary\"\n\n    error_message = \"invalid value for secondary S3 bucket\"\n\n  }\n\n}\n\nCopy\n\nIt is also possible to define specific providers you want to use in specific run blocks:\n\n# main.tf\n\n\n\nterraform {\n\n  required_providers {\n\n    aws = {\n\n      source                = \"hashicorp/aws\"\n\n      configuration_aliases = [aws.secondary]\n\n    }\n\n  }\n\n}\n\n\n\ndata \"aws_region\" \"primary\" {}\n\n\n\ndata \"aws_region\" \"secondary\" {\n\n  provider = aws.secondary\n\n}\n\n\n\nvariable \"bucket_prefix\" {\n\n  default = \"test\"\n\n  type    = string\n\n}\n\n\n\nresource \"aws_s3_bucket\" \"primary_bucket\" {\n\n  bucket = \"${var.bucket_prefix}-${data.aws_region.primary.name}-primary\"\n\n}\n\n\n\nresource \"aws_s3_bucket\" \"secondary_bucket\" {\n\n  provider = aws.secondary\n\n  bucket   = \"${var.bucket_prefix}-${data.aws_region.secondary.name}-secondary\"\n\n}\n\nCopy\n\nOur test file can pass in specific providers for each different run block:\n\n# customised_providers.tftest.hcl\n\n\n\nprovider \"aws\" {\n\n  region = \"us-east-1\"\n\n}\n\n\n\nprovider \"aws\" {\n\n  alias  = \"secondary\"\n\n  region = \"eu-central-1\"\n\n}\n\n\n\nprovider \"aws\" {\n\n  alias  = \"tertiary\"\n\n  region = \"eu-west-2\"\n\n}\n\n\n\nrun \"default_providers\" {\n\n\n\n  command = plan\n\n\n\n  assert {\n\n    condition     = aws_s3_bucket.primary_bucket.bucket == \"test-us-east-1-primary\"\n\n    error_message = \"invalid value for primary S3 bucket\"\n\n  }\n\n\n\n  assert {\n\n    condition     = aws_s3_bucket.secondary_bucket.bucket == \"test-eu-central-1-secondary\"\n\n    error_message = \"invalid value for secondary S3 bucket\"\n\n  }\n\n}\n\n\n\nrun \"customised_providers\" {\n\n\n\n  command = plan\n\n\n\n  providers = {\n\n    aws           = aws\n\n    aws.secondary = aws.tertiary\n\n  }\n\n\n\n  assert {\n\n    condition     = aws_s3_bucket.primary_bucket.bucket == \"test-us-east-1-primary\"\n\n    error_message = \"invalid value for primary S3 bucket\"\n\n  }\n\n\n\n  assert {\n\n    condition     = aws_s3_bucket.secondary_bucket.bucket == \"test-eu-west-2-secondary\"\n\n    error_message = \"invalid value for secondary S3 bucket\"\n\n  }\n\n}\n\nCopy\n\nNote: When running tests with command = apply, switching providers between run blocks can result in failed operations and tests because resources created by one provider definition will be unusable when modified by a second.\n\nFrom Terraform v1.7.0, provider blocks can also reference test file variables and run block outputs. This means the testing framework can retrieve credentials and other setup information from one provider and use this when initializing a second.\n\nIn the following example, the vault provider is initialized first, and then used within a setup module to extract credentials for the aws provider. For more information on setup modules, see Modules.\n\n\n\nprovider \"vault\" {\n\n  # ... vault configuration ...\n\n}\n\n\n\nprovider \"aws\" {\n\n  region     = \"us-east-1\"\n\n\n\n  # The `aws` provider can reference the outputs of the \"vault_setup\" run block.\n\n  access_key = run.vault_setup.aws_access_key\n\n  secret_key = run.vault_setup.aws_secret_key\n\n}\n\n\n\nrun \"vault_setup\" {\n\n  module {\n\n    # This module should only include reference to the Vault provider. Terraform\n\n    # will automatically work out which providers to supply based on the module\n\n    # configuration. The tests will error if a run block requires access to a\n\n    # provider that references outputs from a run block that has not executed.\n\n    source = \"./testing/vault-setup\"\n\n  }\n\n}\n\n\n\nrun \"use_aws_provider\" {\n\n  # This run block can then use both the `aws` and `vault` providers, as the\n\n  # previous run block provided all the data required for the `aws` provider.\n\n}\n\nCopy\nModules\n\nYou can modify the module that a given run block executes.\n\nBy default, Terraform executes the given command against the configuration being tested for each run block. Terraform tests the configuration within the directory you execute the terraform test command from (or the directory you point to with the -chdir argument). Each run block also allows the user to change the targeted configuration using the module block.\n\nUnlike the traditional module block, the module block within test files only supports the source attribute and the version attribute. The remaining attributes that are typically supplied via the traditional module block should be supplied by the alternate attributes and blocks within the run block.\n\nNote: Terraform test files only support local and registry modules within the source attribute.\n\nAll other blocks and attributes within the run block are supported when executing an alternate module, with assert blocks executing against values from the alternate module. This is discussed more in Modules State.\n\nTwo example use cases for the modules block within a testing file are:\n\nA setup module that creates the infrastructure the main configuration requires for testing.\nA loading module to load and validate secondary infrastructure (such as data sources) that are not created directly by the main configuration being tested.\n\nThe following examples demonstrate both of these use cases.\n\nFirst, we have a module that will create and load several files into an already created S3 bucket. This is the configuration we want to test.\n\n# main.tf\n\n\n\nvariable \"bucket\" {\n\n  type = string\n\n}\n\n\n\nvariable \"files\" {\n\n  type = map(string)\n\n}\n\n\n\ndata \"aws_s3_bucket\" \"bucket\" {\n\n  bucket = var.bucket\n\n}\n\n\n\nresource \"aws_s3_object\" \"object\" {\n\n  for_each = var.files\n\n\n\n  bucket = data.aws_s3_bucket.bucket.id\n\n  key = each.key\n\n  source = each.value\n\n\n\n  etag = filemd5(each.value)\n\n}\n\nCopy\n\nSecond, we have a setup module that will create the S3 bucket, so it is available to the configuration under test.\n\n# testing/setup/main.tf\n\n\n\nvariable \"bucket\" {\n\n  type = string\n\n}\n\n\n\nresource \"aws_s3_bucket\" \"bucket\" {\n\n  bucket = var.bucket\n\n}\n\nCopy\n\nThird, we have a loading module, that will load the files in the s3 bucket. This is a fairly contrived example, as it is definitely possible just to validate the files directly when they are created in the module under test. It is, however, good for demonstrating the use case.\n\n# testing/loader/main.tf\n\n\n\nvariable \"bucket\" {\n\n  type = string\n\n}\n\n\n\ndata \"aws_s3_objects\" \"objects\" {\n\n  bucket = var.bucket\n\n}\n\nCopy\n\nFinally, we have the test file itself which configures everything and calls out to the various helper modules we have created.\n\n# file_count.tftest.hcl\n\n\n\nvariables {\n\n  bucket = \"my_test_bucket\"\n\n  files = {\n\n    \"file-one.txt\": \"data/files/file_one.txt\"\n\n    \"file-two.txt\": \"data/files/file_two.txt\"\n\n  }\n\n}\n\n\n\nprovider \"aws\" {\n\n  region = \"us-east-1\"\n\n}\n\n\n\nrun \"setup\" {\n\n  # Create the S3 bucket we will use later.\n\n\n\n  module {\n\n    source = \"./testing/setup\"\n\n  }\n\n}\n\n\n\nrun \"execute\" {\n\n  # This is empty, we just run the configuration under test using all the default settings.\n\n}\n\n\n\nrun \"verify\" {\n\n  # Load and count the objects created in the \"execute\" run block.\n\n\n\n  module {\n\n    source = \"./testing/loader\"\n\n  }\n\n\n\n  assert {\n\n    condition = length(data.aws_s3_objects.objects.keys) == 2\n\n    error_message = \"created the wrong number of s3 objects\"\n\n  }\n\n}\n\nCopy\nModules state\n\nWhile Terraform executes a terraform test command, Terraform maintains at least one, but possibly many, state files within memory for each test file.\n\nThere is always at least one state file that maintains the state of the main configuration under test. This state file is shared by all run blocks that do not have a module block specifying an alternate module to load.\n\nAdditionally, there is one state file per alternate module that Terraform loads. An alternate module state file is shared by all run blocks that execute the given module.\n\nThe Terraform team is interested in any use cases requiring manual state management or the ability to execute different configurations against the same state within the test command. If you have a use case, please file an issue and share it with us.\n\nThe following example uses comments to explain where the state files for each run block originate. In the below example Terraform creates and manages a total of three state files. The first state file is for the main configuration under test, the second for the setup module, and the third for the loader module.\n\nrun \"setup\" {\n\n\n\n  # This run block references an alternate module and is the first run block\n\n  # to reference this particular alternate module. Therefore, Terraform creates\n\n  # and populates a new empty state file for this run block.\n\n\n\n  module {\n\n    source = \"./testing/setup\"\n\n  }\n\n}\n\n\n\nrun \"init\" {\n\n\n\n  # This run block does not reference an alternate module, so it uses the main\n\n  # state file for the configuration under test. As this is the first run block\n\n  # to reference the main configuration, the previously empty state file now\n\n  # contains the resources created by this run block.\n\n\n\n  assert {\n\n    # In practice we'd do some interesting checks and tests here but the\n\n    # assertions aren't important for this example.\n\n  }\n\n\n\n  # ... more assertions ...\n\n}\n\n\n\nrun \"update_setup\" {\n\n\n\n  # We've now re-referenced the setup module, so the state file that was created\n\n  # for the first \"setup\" run block will be reused. It will contain any\n\n  # resources that were created as part of the other run block before this run\n\n  # block executes and will be updated with any changes made by this run block\n\n  # after.\n\n\n\n  module {\n\n    source = \"./testing/setup\"\n\n  }\n\n\n\n  variables {\n\n    # In practice, we'd likely make some changes to the module compared to the\n\n    # first run block here. Otherwise, there would be no point recalling the\n\n    # module.\n\n  }\n\n}\n\n\n\nrun \"update\" {\n\n\n\n  # As with the \"init\" run block, we are executing against the main configuration\n\n  # again. This means we'd load the main state file that was initially populated\n\n  # by the \"init\" run block, and any changes made by this \"run\" block will be\n\n  # carried forward to any future run blocks that execute against the main\n\n  # configuration.\n\n\n\n  # ... updated variables ...\n\n\n\n  # ... assertions ...\n\n}\n\n\n\nrun \"loader\" {\n\n\n\n  # This run block is now referencing our second alternate module so will create\n\n  # our third and final state file. The other two state files are managing\n\n  # resources from the main configuration and resources from the setup module.\n\n  # We are getting a new state file for this run block as the loader module has\n\n  # not previously been referenced by any run blocks.\n\n\n\n  module {\n\n    source = \"./testing/loader\"\n\n  }\n\n}\n\n\n\nCopy\nModules Cleanup\n\nAt the conclusion of a test file, Terraform attempts to destroy every resource it created during the execution of that test file. When Terraform loads alternate modules, the order in which Terraform destroys those objects in is important. For example, in the first Modules example, Terraform could not destroy the resources created in the \"setup\" run block before the objects created in the \"execute\" run block, because the S3 bucket we created in the \"setup\" step can not be destroyed while it contains objects.\n\nTerraform destroys resources in reverse run block order. In the most recent example, there are three state files. One for the main state, one for the ./testing/loader module, and one for the ./testing/setup module. The ./testing/loader state file would be destroyed first as it was referenced most recently by the last run block. The main state file would be destroyed second as it was referenced by the \"update\" run block. The ./testing/setup state file would then be destroyed last.\n\nNote, that the first two run blocks \"setup\" and \"init\", do nothing during the destroy operations as their state files are used by later run blocks and have already been destroyed.\n\nIf you use a single setup module as an alternate module, and it executes first, or you use no alternate modules, then the order of destruction does not affect you. Anything more complex may require careful consideration to make sure the destruction of resources can complete automatically.\n\nExpecting failures\n\nBy default, if any Custom Conditions, including check block assertions, fail during the execution of a Terraform test file then the overall command reports the test as a failure.\n\nHowever, it is a common testing paradigm to want to test failure cases. Terraform supports the expect_failures attribute for this use case.\n\nIn each run block the expect_failures attribute can provide a list of checkable objects (resources, data sources, check blocks, input variables, and outputs) that should fail their custom conditions. The test passes if the checkable objects you specify report an issue, and the test fails overall if they do not.\n\nYou can still write assertions alongside an expect_failures block, but you should be mindful that all custom conditions, except check block assertions, halt the execution of Terraform. This still applies during test execution, so your assertions should only consider values that you are sure will be computed before the checkable object is due to fail. You can manage this using references, or the depends_on meta-argument within your main configuration.\n\nThis also means that, with the exception of check blocks, you can only reliably include a single checkable object. We support a list of checkable objects within the expect_failures attribute purely for check blocks.\n\nA quick example below demonstrates testing the validation block on an input variable. The configuration file accepts a single input variable that must be even number.\n\n# main.tf\n\n\n\nvariable \"input\" {\n\n  type = number\n\n\n\n  validation {\n\n    condition = var.input % 2 == 0\n\n    error_message = \"must be even number\"\n\n  }\n\n}\n\nCopy\n\nThe test file contains two run blocks. One that validates that our custom condition passes on an even number and one that validates our custom condition fails on an odd number.\n\n# input_validation.tftest.hcl\n\n\n\nvariables {\n\n  input = 0\n\n}\n\n\n\nrun \"zero\" {\n\n  # The variable defined above is even, so we expect the validation to pass.\n\n\n\n  command = plan\n\n}\n\n\n\nrun \"one\" {\n\n  # This time we set the variable is odd, so we expect the validation to fail.\n\n\n\n  command = plan\n\n\n\n  variables {\n\n    input = 1\n\n  }\n\n\n\n  expect_failures = [\n\n    var.input,\n\n  ]\n\n}\n\nCopy\n\nNote: Terraform only expects failures in the operation specified by the command attribute of the run block.\n\nBe careful when using expect_failures in run blocks with command = apply. A run block with command = apply that expects a custom condition failure will fail overall if that custom condition fails during the plan.\n\nThis is logically consistent, as the run block is expecting to be able to run an apply operation but can not because the plan failed. It is also potentially confusing, as you will see the failure in the diagnostics as the reason the test failed, even though that failure was marked as being expected.\n\nThere are instances when Terraform does not execute a custom condition during the planning stage, because that condition is relying on computed attributes that are only available after Terraform creates the referenced resource. In these cases, you could use an expect_failures block alongside a command = apply attribute and value. However, in most cases we recommend only using expect_failures alongside command = plan operations.\n\nNote: Expected failures only apply to user-defined custom conditions.\n\nOther kinds of failure besides the specified expected failures in the checkable object still result in the overall test failing. For example, a variable that expects a boolean value as input fails the surrounding test if Terraform provides the wrong kind of value, even if that variable is included in an expect_failures attribute.\n\nThe expect_failures attribute is included to allow authors to test their configuration and any logic defined within. A type mismatch, as in the previous example, is not something Terraform authors should have to worry about testing as Terraform itself will handle enforce type constraints. As such, you can only expect_failures in custom conditions.\n\nEdit this page on GitHub\n\nOn this page:\n\nTests\nIntegration or Unit testing\nSyntax\nRun blocks\nVariables\nProviders\nModules\nExpecting failures\nTheme\nDark\nLight\nSystem\nCertifications\nSystem Status\nCookie Manager\nTerms of Use\nSecurity\nPrivacy\nTrademark Policy\nTrade Controls\nGive Feedback\n(opens in new tab)\nWe use cookies & other similar technology to collect data to improve your experience on our site, as described in our Privacy Policy and Cookie Policy.\nManage PreferencesACCEPT"
  },
  {
    "title": "State: Locking | Terraform | HashiCorp Developer",
    "url": "https://developer.hashicorp.com/terraform/language/state/locking",
    "html": "Skip to main content\nNews\nHashiCorp to join IBM to accelerate multi-cloud automation.\nRead the blog →\nDismiss alert\nTerraform\nInstall\nTutorials\nDocumentation\nRegistry\n(opens in new tab)\nTry Cloud\n(opens in new tab)\nSearch\nCommand or control key\nK key\nTerraform Home\nConfiguration Language\nConfiguration Language\nStyle Guide\nFiles and Directories\nSyntax\nResources\nData Sources\nProviders\nVariables and Outputs\nModules\nMoved block\nChecks\nImport\nExpressions\nFunctions\nTerraform Settings\nState\nOverview\nPurpose\nThe terraform_remote_state Data Source\nBackends: State Storage and Locking\nImport Existing Resources\nLocking\nWorkspaces\nRemote State\nSensitive Data\nTests\nUpgrading to Terraform v1.7\nv1.x Compatibility Promises\nTerraform Internals\nResources\nTutorial Library\nCertifications\nCommunity Forum\n(opens in new tab)\nSupport\n(opens in new tab)\nGitHub\n(opens in new tab)\nTerraform Registry\n(opens in new tab)\nDeveloper\nTerraform\nConfiguration Language\nState\nLocking\nv1.8.x (latest)\nState Locking\n\nIf supported by your backend, Terraform will lock your state for all operations that could write state. This prevents others from acquiring the lock and potentially corrupting your state.\n\nState locking happens automatically on all operations that could write state. You won't see any message that it is happening. If state locking fails, Terraform will not continue. You can disable state locking for most commands with the -lock flag but it is not recommended.\n\nIf acquiring the lock is taking longer than expected, Terraform will output a status message. If Terraform doesn't output a message, state locking is still occurring if your backend supports it.\n\nNot all backends support locking. The documentation for each backend includes details on whether it supports locking or not.\n\nForce Unlock\n\nTerraform has a force-unlock command to manually unlock the state if unlocking failed.\n\nBe very careful with this command. If you unlock the state when someone else is holding the lock it could cause multiple writers. Force unlock should only be used to unlock your own lock in the situation where automatic unlocking failed.\n\nTo protect you, the force-unlock command requires a unique lock ID. Terraform will output this lock ID if unlocking fails. This lock ID acts as a nonce, ensuring that locks and unlocks target the correct lock.\n\nEdit this page on GitHub\n\nOn this page:\n\nState Locking\nForce Unlock\nTheme\nDark\nLight\nSystem\nCertifications\nSystem Status\nCookie Manager\nTerms of Use\nSecurity\nPrivacy\nTrademark Policy\nTrade Controls\nGive Feedback\n(opens in new tab)\nWe use cookies & other similar technology to collect data to improve your experience on our site, as described in our Privacy Policy and Cookie Policy.\nManage PreferencesACCEPT"
  },
  {
    "title": "The count Meta-Argument - Configuration Language | Terraform | HashiCorp Developer",
    "url": "https://developer.hashicorp.com/terraform/language/meta-arguments/count",
    "html": "Skip to main content\nNews\nHashiCorp to join IBM to accelerate multi-cloud automation.\nRead the blog →\nDismiss alert\nTerraform\nInstall\nTutorials\nDocumentation\nRegistry\n(opens in new tab)\nTry Cloud\n(opens in new tab)\nSearch\nCommand or control key\nK key\nTerraform Home\nConfiguration Language\nConfiguration Language\nStyle Guide\nFiles and Directories\nSyntax\nResources\nOverview\nResource Blocks\nResource Behavior\nMeta-Arguments\ndepends_on\ncount\nfor_each\nprovider\nlifecycle\nProvisioners\nThe terraform_data Resource Type\nData Sources\nProviders\nVariables and Outputs\nModules\nMoved block\nChecks\nImport\nExpressions\nFunctions\nTerraform Settings\nState\nTests\nUpgrading to Terraform v1.7\nv1.x Compatibility Promises\nTerraform Internals\nResources\nTutorial Library\nCertifications\nCommunity Forum\n(opens in new tab)\nSupport\n(opens in new tab)\nGitHub\n(opens in new tab)\nTerraform Registry\n(opens in new tab)\nDeveloper\nTerraform\nConfiguration Language\nMeta-Arguments\ncount\nv1.8.x (latest)\nThe count Meta-Argument\n\nVersion note: Module support for count was added in Terraform 0.13, and previous versions can only use it with resources.\n\nNote: A given resource or module block cannot use both count and for_each.\n\nHands-on: Try the Manage Similar Resources With Count tutorial.\n\nBy default, a resource block configures one real infrastructure object. (Similarly, a module block includes a child module's contents into the configuration one time.) However, sometimes you want to manage several similar objects (like a fixed pool of compute instances) without writing a separate block for each one. Terraform has two ways to do this: count and for_each.\n\nIf a resource or module block includes a count argument whose value is a whole number, Terraform will create that many instances.\n\nBasic Syntax\n\ncount is a meta-argument defined by the Terraform language. It can be used with modules and with every resource type.\n\nThe count meta-argument accepts a whole number, and creates that many instances of the resource or module. Each instance has a distinct infrastructure object associated with it, and each is separately created, updated, or destroyed when the configuration is applied.\n\nresource \"aws_instance\" \"server\" {\n\n  count = 4 # create four similar EC2 instances\n\n\n\n  ami           = \"ami-a1b2c3d4\"\n\n  instance_type = \"t2.micro\"\n\n\n\n  tags = {\n\n    Name = \"Server ${count.index}\"\n\n  }\n\n}\n\nCopy\nThe count Object\n\nIn blocks where count is set, an additional count object is available in expressions, so you can modify the configuration of each instance. This object has one attribute:\n\ncount.index — The distinct index number (starting with 0) corresponding to this instance.\nUsing Expressions in count\n\nThe count meta-argument accepts numeric expressions. However, unlike most arguments, the count value must be known before Terraform performs any remote resource actions. This means count can't refer to any resource attributes that aren't known until after a configuration is applied (such as a unique ID generated by the remote API when an object is created).\n\nReferring to Instances\n\nWhen count is set, Terraform distinguishes between the block itself and the multiple resource or module instances associated with it. Instances are identified by an index number, starting with 0.\n\n<TYPE>.<NAME> or module.<NAME> (for example, aws_instance.server) refers to the resource block.\n<TYPE>.<NAME>[<INDEX>] or module.<NAME>[<INDEX>] (for example, aws_instance.server[0], aws_instance.server[1], etc.) refers to individual instances.\n\nThis is different from resources and modules without count or for_each, which can be referenced without an index or key.\n\nSimilarly, resources from child modules with multiple instances are prefixed with module.<NAME>[<KEY>] when displayed in plan output and elsewhere in the UI. For a module without count or for_each, the address will not contain the module index as the module's name suffices to reference the module.\n\nNote: Within nested provisioner or connection blocks, the special self object refers to the current resource instance, not the resource block as a whole.\n\nWhen to Use for_each Instead of count\n\nIf your instances are almost identical, count is appropriate. If some of their arguments need distinct values that can't be directly derived from an integer, it's safer to use for_each.\n\nBefore for_each was available, it was common to derive count from the length of a list and use count.index to look up the original list value:\n\nvariable \"subnet_ids\" {\n\n  type = list(string)\n\n}\n\n\n\nresource \"aws_instance\" \"server\" {\n\n  # Create one instance for each subnet\n\n  count = length(var.subnet_ids)\n\n\n\n  ami           = \"ami-a1b2c3d4\"\n\n  instance_type = \"t2.micro\"\n\n  subnet_id     = var.subnet_ids[count.index]\n\n\n\n  tags = {\n\n    Name = \"Server ${count.index}\"\n\n  }\n\n}\n\nCopy\n\nThis was fragile, because the resource instances were still identified by their index instead of the string values in the list. If an element was removed from the middle of the list, every instance after that element would see its subnet_id value change, resulting in more remote object changes than intended. Using for_each gives the same flexibility without the extra churn.\n\nEdit this page on GitHub\n\nOn this page:\n\nThe count Meta-Argument\nBasic Syntax\nThe count Object\nUsing Expressions in count\nReferring to Instances\nWhen to Use for_each Instead of count\nTheme\nDark\nLight\nSystem\nCertifications\nSystem Status\nCookie Manager\nTerms of Use\nSecurity\nPrivacy\nTrademark Policy\nTrade Controls\nGive Feedback\n(opens in new tab)\nWe use cookies & other similar technology to collect data to improve your experience on our site, as described in our Privacy Policy and Cookie Policy.\nManage PreferencesACCEPT"
  },
  {
    "title": "Conditional Expressions - Configuration Language | Terraform | HashiCorp Developer",
    "url": "https://developer.hashicorp.com/terraform/language/expressions/conditionals",
    "html": "Skip to main content\nNews\nHashiCorp to join IBM to accelerate multi-cloud automation.\nRead the blog →\nDismiss alert\nTerraform\nInstall\nTutorials\nDocumentation\nRegistry\n(opens in new tab)\nTry Cloud\n(opens in new tab)\nSearch\nCommand or control key\nK key\nTerraform Home\nConfiguration Language\nConfiguration Language\nStyle Guide\nFiles and Directories\nSyntax\nResources\nData Sources\nProviders\nVariables and Outputs\nModules\nMoved block\nChecks\nImport\nExpressions\nOverview\nTypes and Values\nStrings and Templates\nReferences to Values\nOperators\nFunction Calls\nConditional Expressions\nFor Expressions\nSplat Expressions\nDynamic Blocks\nCustom Conditions\nType Constraints\nVersion Constraints\nFunctions\nTerraform Settings\nState\nTests\nUpgrading to Terraform v1.7\nv1.x Compatibility Promises\nTerraform Internals\nResources\nTutorial Library\nCertifications\nCommunity Forum\n(opens in new tab)\nSupport\n(opens in new tab)\nGitHub\n(opens in new tab)\nTerraform Registry\n(opens in new tab)\nDeveloper\nTerraform\nConfiguration Language\nExpressions\nConditional Expressions\nv1.8.x (latest)\nConditional Expressions\n\nA conditional expression uses the value of a boolean expression to select one of two values.\n\nHands-on: Try the Create Dynamic Expressions tutorial.\n\nSyntax\n\nThe syntax of a conditional expression is as follows:\n\ncondition ? true_val : false_val\n\nCopy\n\nIf condition is true then the result is true_val. If condition is false then the result is false_val.\n\nA common use of conditional expressions is to define defaults to replace invalid values:\n\nvar.a != \"\" ? var.a : \"default-a\"\n\nCopy\n\nIf var.a is an empty string then the result is \"default-a\", but otherwise it is the actual value of var.a.\n\nConditions\n\nThe condition can be any expression that resolves to a boolean value. This will usually be an expression that uses the equality, comparison, or logical operators.\n\nCustom Condition Checks\n\nYou can create conditions that produce custom error messages for several types of objects in a configuration. For example, you can add a condition to an input variable that checks whether incoming image IDs are formatted properly.\n\nCustom conditions can help capture assumptions, helping future maintainers understand the configuration design and intent. They also return useful information about errors earlier and in context, helping consumers more easily diagnose issues in their configurations.\n\nRefer to Custom Condition Checks for details.\n\nResult Types\n\nThe two result values may be of any type, but they must both be of the same type so that Terraform can determine what type the whole conditional expression will return without knowing the condition value.\n\nIf the two result expressions don't produce the same type then Terraform will attempt to find a type that they can both convert to, and make those conversions automatically if so.\n\nFor example, the following expression is valid and will always return a string, because in Terraform all numbers can convert automatically to a string using decimal digits:\n\nvar.example ? 12 : \"hello\"\n\nCopy\n\nRelying on this automatic conversion behavior can be confusing for those who are not familiar with Terraform's conversion rules though, so we recommend being explicit using type conversion functions in any situation where there may be some uncertainty about the expected result type.\n\nThe following example is contrived because it would be easier to write the constant \"12\" instead of the type conversion in this case, but shows how to use tostring to explicitly convert a number to a string.\n\nvar.example ? tostring(12) : \"hello\"\n\nCopy\nEdit this page on GitHub\n\nOn this page:\n\nConditional Expressions\nSyntax\nConditions\nResult Types\nTheme\nDark\nLight\nSystem\nCertifications\nSystem Status\nCookie Manager\nTerms of Use\nSecurity\nPrivacy\nTrademark Policy\nTrade Controls\nGive Feedback\n(opens in new tab)\nWe use cookies & other similar technology to collect data to improve your experience on our site, as described in our Privacy Policy and Cookie Policy.\nManage PreferencesACCEPT"
  },
  {
    "title": "The for_each Meta-Argument - Configuration Language | Terraform | HashiCorp Developer",
    "url": "https://developer.hashicorp.com/terraform/language/meta-arguments/for_each",
    "html": "News\nHashiCorp to join IBM to accelerate multi-cloud automation.\nRead the blog →\nDismiss alert\nTerraform\nInstall\nTutorials\nDocumentation\nRegistry\n(opens in new tab)\nTry Cloud\n(opens in new tab)\nSearch\nCommand or control key\nK key\nTerraform Home\nConfiguration Language\nConfiguration Language\nStyle Guide\nFiles and Directories\nSyntax\nResources\nOverview\nResource Blocks\nResource Behavior\nMeta-Arguments\ndepends_on\ncount\nfor_each\nprovider\nlifecycle\nProvisioners\nThe terraform_data Resource Type\nData Sources\nProviders\nVariables and Outputs\nModules\nMoved block\nChecks\nImport\nExpressions\nFunctions\nTerraform Settings\nState\nTests\nUpgrading to Terraform v1.7\nv1.x Compatibility Promises\nTerraform Internals\nResources\nTutorial Library\nCertifications\nCommunity Forum\n(opens in new tab)\nSupport\n(opens in new tab)\nGitHub\n(opens in new tab)\nTerraform Registry\n(opens in new tab)\nDeveloper\nTerraform\nConfiguration Language\nMeta-Arguments\nfor_each\nv1.8.x (latest)\nThe for_each Meta-Argument\n\nBy default, a resource block configures one real infrastructure object (and similarly, a module block includes a child module's contents into the configuration one time). However, sometimes you want to manage several similar objects (like a fixed pool of compute instances) without writing a separate block for each one. Terraform has two ways to do this: count and for_each.\n\nHands-on: Try the Manage Similar Resources With For Each tutorial.\n\nIf a resource or module block includes a for_each argument whose value is a map or a set of strings, Terraform creates one instance for each member of that map or set.\n\nVersion note: for_each was added in Terraform 0.12.6. Module support for for_each was added in Terraform 0.13; previous versions can only use it with resources.\n\nNote: A given resource or module block cannot use both count and for_each.\n\nBasic Syntax\n\nfor_each is a meta-argument defined by the Terraform language. It can be used with modules and with every resource type.\n\nThe for_each meta-argument accepts a map or a set of strings, and creates an instance for each item in that map or set. Each instance has a distinct infrastructure object associated with it, and each is separately created, updated, or destroyed when the configuration is applied.\n\nMap:\n\nresource \"azurerm_resource_group\" \"rg\" {\n\n  for_each = tomap({\n\n    a_group       = \"eastus\"\n\n    another_group = \"westus2\"\n\n  })\n\n  name     = each.key\n\n  location = each.value\n\n}\n\nCopy\n\nSet of strings:\n\nresource \"aws_iam_user\" \"the-accounts\" {\n\n  for_each = toset([\"Todd\", \"James\", \"Alice\", \"Dottie\"])\n\n  name     = each.key\n\n}\n\nCopy\n\nChild module:\n\n# my_buckets.tf\n\nmodule \"bucket\" {\n\n  for_each = toset([\"assets\", \"media\"])\n\n  source   = \"./publish_bucket\"\n\n  name     = \"${each.key}_bucket\"\n\n}\n\nCopy\n# publish_bucket/bucket-and-cloudfront.tf\n\nvariable \"name\" {} # this is the input parameter of the module\n\n\n\nresource \"aws_s3_bucket\" \"example\" {\n\n  # Because var.name includes each.key in the calling\n\n  # module block, its value will be different for\n\n  # each instance of this module.\n\n  bucket = var.name\n\n\n\n  # ...\n\n}\n\n\n\nresource \"aws_iam_user\" \"deploy_user\" {\n\n  # ...\n\n}\n\nCopy\nThe each Object\n\nIn blocks where for_each is set, an additional each object is available in expressions, so you can modify the configuration of each instance. This object has two attributes:\n\neach.key — The map key (or set member) corresponding to this instance.\neach.value — The map value corresponding to this instance. (If a set was provided, this is the same as each.key.)\nLimitations on values used in for_each\n\nThe keys of the map (or all the values in the case of a set of strings) must be known values, or you will get an error message that for_each has dependencies that cannot be determined before apply, and a -target may be needed.\n\nfor_each keys cannot be the result (or rely on the result of) of impure functions, including uuid, bcrypt, or timestamp, as their evaluation is deferred during the main evaluation step.\n\nSensitive values, such as sensitive input variables, sensitive outputs, or sensitive resource attributes, cannot be used as arguments to for_each. The value used in for_each is used to identify the resource instance and will always be disclosed in UI output, which is why sensitive values are not allowed. Attempts to use sensitive values as for_each arguments will result in an error.\n\nIf you transform a value containing sensitive data into an argument to be used in for_each, be aware that most functions in Terraform will return a sensitive result if given an argument with any sensitive content. In many cases, you can achieve similar results to a function used for this purpose by using a for expression. For example, if you would like to call keys(local.map), where local.map is an object with sensitive values (but non-sensitive keys), you can create a value to pass to for_each with toset([for k,v in local.map : k]).\n\nUsing Expressions in for_each\n\nThe for_each meta-argument accepts map or set expressions. However, unlike most arguments, the for_each value must be known before Terraform performs any remote resource actions. This means for_each can't refer to any resource attributes that aren't known until after a configuration is applied (such as a unique ID generated by the remote API when an object is created).\n\nThe for_each value must be a map or set with one element per desired resource instance. To use a sequence as the for_each value, you must use an expression that explicitly returns a set value, like the toset function. To prevent unwanted surprises during conversion, the for_each argument does not implicitly convert lists or tuples to sets. If you need to declare resource instances based on a nested data structure or combinations of elements from multiple data structures you can use Terraform expressions and functions to derive a suitable value. For example:\n\nTransform a multi-level nested structure into a flat list by using nested for expressions with the flatten function.\nProduce an exhaustive list of combinations of elements from two or more collections by using the setproduct function inside a for expression.\nChaining for_each Between Resources\n\nBecause a resource using for_each appears as a map of objects when used in expressions elsewhere, you can directly use one resource as the for_each of another in situations where there is a one-to-one relationship between two sets of objects.\n\nFor example, in AWS an aws_vpc object is commonly associated with a number of other objects that provide additional services to that VPC, such as an \"internet gateway\". If you are declaring multiple VPC instances using for_each then you can chain that for_each into another resource to declare an internet gateway for each VPC:\n\nvariable \"vpcs\" {\n\n  type = map(object({\n\n    cidr_block = string\n\n  }))\n\n}\n\n\n\nresource \"aws_vpc\" \"example\" {\n\n  # One VPC for each element of var.vpcs\n\n  for_each = var.vpcs\n\n\n\n  # each.value here is a value from var.vpcs\n\n  cidr_block = each.value.cidr_block\n\n}\n\n\n\nresource \"aws_internet_gateway\" \"example\" {\n\n  # One Internet Gateway per VPC\n\n  for_each = aws_vpc.example\n\n\n\n  # each.value here is a full aws_vpc object\n\n  vpc_id = each.value.id\n\n}\n\n\n\noutput \"vpc_ids\" {\n\n  value = {\n\n    for k, v in aws_vpc.example : k => v.id\n\n  }\n\n\n\n  # The VPCs aren't fully functional until their\n\n  # internet gateways are running.\n\n  depends_on = [aws_internet_gateway.example]\n\n}\n\nCopy\n\nThis chaining pattern explicitly and concisely declares the relationship between the internet gateway instances and the VPC instances, which tells Terraform to expect the instance keys for both to always change together, and typically also makes the configuration easier to understand for human maintainers.\n\nReferring to Instances\n\nWhen for_each is set, Terraform distinguishes between the block itself and the multiple resource or module instances associated with it. Instances are identified by a map key (or set member) from the value provided to for_each.\n\n<TYPE>.<NAME> or module.<NAME> (for example, azurerm_resource_group.rg) refers to the block.\n<TYPE>.<NAME>[<KEY>] or module.<NAME>[<KEY>] (for example, azurerm_resource_group.rg[\"a_group\"], azurerm_resource_group.rg[\"another_group\"], etc.) refers to individual instances.\n\nThis is different from resources and modules without count or for_each, which can be referenced without an index or key.\n\nSimilarly, resources from child modules with multiple instances are prefixed with module.<NAME>[<KEY>] when displayed in plan output and elsewhere in the UI. For a module without count or for_each, the address will not contain the module index as the module's name suffices to reference the module.\n\nNote: Within nested provisioner or connection blocks, the special self object refers to the current resource instance, not the resource block as a whole.\n\nUsing Sets\n\nThe Terraform language doesn't have a literal syntax for set values, but you can use the toset function to explicitly convert a list of strings to a set:\n\nlocals {\n\n  subnet_ids = toset([\n\n    \"subnet-abcdef\",\n\n    \"subnet-012345\",\n\n  ])\n\n}\n\n\n\nresource \"aws_instance\" \"server\" {\n\n  for_each = local.subnet_ids\n\n\n\n  ami           = \"ami-a1b2c3d4\"\n\n  instance_type = \"t2.micro\"\n\n  subnet_id     = each.key # note: each.key and each.value are the same for a set\n\n\n\n  tags = {\n\n    Name = \"Server ${each.key}\"\n\n  }\n\n}\n\nCopy\n\nConversion from list to set discards the ordering of the items in the list and removes any duplicate elements. toset([\"b\", \"a\", \"b\"]) will produce a set containing only \"a\" and \"b\" in no particular order; the second \"b\" is discarded.\n\nIf you are writing a module with an input variable that will be used as a set of strings for for_each, you can set its type to set(string) to avoid the need for an explicit type conversion:\n\nvariable \"subnet_ids\" {\n\n  type = set(string)\n\n}\n\n\n\nresource \"aws_instance\" \"server\" {\n\n  for_each = var.subnet_ids\n\n\n\n  # (and the other arguments as above)\n\n}\n\nCopy\nEdit this page on GitHub\n\nOn this page:\n\nThe for_each Meta-Argument\nBasic Syntax\nThe each Object\nLimitations on values used in for_each\nUsing Expressions in for_each\nReferring to Instances\nUsing Sets\nTheme\nCertifications\nSystem Status\nCookie Manager\nTerms of Use\nSecurity\nPrivacy\nTrademark Policy\nTrade Controls\nGive Feedback\n(opens in new tab)"
  },
  {
    "title": "For Expressions - Configuration Language | Terraform | HashiCorp Developer",
    "url": "https://developer.hashicorp.com/terraform/language/expressions/for",
    "html": "Skip to main content\nNews\nHashiCorp to join IBM to accelerate multi-cloud automation.\nRead the blog →\nDismiss alert\nTerraform\nInstall\nTutorials\nDocumentation\nRegistry\n(opens in new tab)\nTry Cloud\n(opens in new tab)\nSearch\nCommand or control key\nK key\nTerraform Home\nConfiguration Language\nConfiguration Language\nStyle Guide\nFiles and Directories\nSyntax\nResources\nData Sources\nProviders\nVariables and Outputs\nModules\nMoved block\nChecks\nImport\nExpressions\nOverview\nTypes and Values\nStrings and Templates\nReferences to Values\nOperators\nFunction Calls\nConditional Expressions\nFor Expressions\nSplat Expressions\nDynamic Blocks\nCustom Conditions\nType Constraints\nVersion Constraints\nFunctions\nTerraform Settings\nState\nTests\nUpgrading to Terraform v1.7\nv1.x Compatibility Promises\nTerraform Internals\nResources\nTutorial Library\nCertifications\nCommunity Forum\n(opens in new tab)\nSupport\n(opens in new tab)\nGitHub\n(opens in new tab)\nTerraform Registry\n(opens in new tab)\nDeveloper\nTerraform\nConfiguration Language\nExpressions\nFor Expressions\nv1.8.x (latest)\nfor Expressions\n\nA for expression creates a complex type value by transforming another complex type value. Each element in the input value can correspond to either one or zero values in the result, and an arbitrary expression can be used to transform each input element into an output element.\n\nFor example, if var.list were a list of strings, then the following expression would produce a tuple of strings with all-uppercase letters:\n\n[for s in var.list : upper(s)]\n\nCopy\n\nThis for expression iterates over each element of var.list, and then evaluates the expression upper(s) with s set to each respective element. It then builds a new tuple value with all of the results of executing that expression in the same order.\n\nInput Types\n\nA for expression's input (given after the in keyword) can be a list, a set, a tuple, a map, or an object.\n\nThe above example showed a for expression with only a single temporary symbol s, but a for expression can optionally declare a pair of temporary symbols in order to use the key or index of each item too:\n\n[for k, v in var.map : length(k) + length(v)]\n\nCopy\n\nFor a map or object type, like above, the k symbol refers to the key or attribute name of the current element. You can also use the two-symbol form with lists and tuples, in which case the additional symbol is the index of each element starting from zero, which conventionally has the symbol name i or idx unless it's helpful to choose a more specific name:\n\n[for i, v in var.list : \"${i} is ${v}\"]\n\nCopy\n\nThe index or key symbol is always optional. If you specify only a single symbol after the for keyword then that symbol will always represent the value of each element of the input collection.\n\nResult Types\n\nThe type of brackets around the for expression decide what type of result it produces.\n\nThe above example uses [ and ], which produces a tuple. If you use { and } instead, the result is an object and you must provide two result expressions that are separated by the => symbol:\n\n{for s in var.list : s => upper(s)}\n\nCopy\n\nThis expression produces an object whose attributes are the original elements from var.list and their corresponding values are the uppercase versions. For example, the resulting value might be as follows:\n\n{\n\n  foo = \"FOO\"\n\n  bar = \"BAR\"\n\n  baz = \"BAZ\"\n\n}\n\nCopy\n\nA for expression alone can only produce either an object value or a tuple value, but Terraform's automatic type conversion rules mean that you can typically use the results in locations where lists, maps, and sets are expected.\n\nFiltering Elements\n\nA for expression can also include an optional if clause to filter elements from the source collection, producing a value with fewer elements than the source value:\n\n[for s in var.list : upper(s) if s != \"\"]\n\nCopy\n\nOne common reason for filtering collections in for expressions is to split a single source collection into two separate collections based on some criteria. For example, if the input var.users is a map of objects where the objects each have an attribute is_admin then you may wish to produce separate maps with admin vs non-admin objects:\n\nvariable \"users\" {\n\n  type = map(object({\n\n    is_admin = bool\n\n  }))\n\n}\n\n\n\nlocals {\n\n  admin_users = {\n\n    for name, user in var.users : name => user\n\n    if user.is_admin\n\n  }\n\n  regular_users = {\n\n    for name, user in var.users : name => user\n\n    if !user.is_admin\n\n  }\n\n}\n\nCopy\nElement Ordering\n\nBecause for expressions can convert from unordered types (maps, objects, sets) to ordered types (lists, tuples), Terraform must choose an implied ordering for the elements of an unordered collection.\n\nFor maps and objects, Terraform sorts the elements by key or attribute name, using lexical sorting.\n\nFor sets of strings, Terraform sorts the elements by their value, using lexical sorting.\n\nFor sets of other types, Terraform uses an arbitrary ordering that may change in future versions. We recommend converting the expression result into a set to make it clear elsewhere in the configuration that the result is unordered. You can use the toset function to concisely convert a for expression result to be of a set type.\n\ntoset([for e in var.set : e.example])\n\nCopy\nGrouping Results\n\nIf the result type is an object (using { and } delimiters) then normally the given key expression must be unique across all elements in the result, or Terraform will return an error.\n\nSometimes the resulting keys are not unique, and so to support that situation Terraform supports a special grouping mode which changes the result to support multiple elements per key.\n\nTo activate grouping mode, add the symbol ... after the value expression. For example:\n\nvariable \"users\" {\n\n  type = map(object({\n\n    role = string\n\n  }))\n\n}\n\n\n\nlocals {\n\n  users_by_role = {\n\n    for name, user in var.users : user.role => name...\n\n  }\n\n}\n\nCopy\n\nThe above represents a situation where a module expects a map describing various users who each have a single \"role\", where the map keys are usernames. The usernames are guaranteed unique because they are map keys in the input, but many users may all share a single role name.\n\nThe local.users_by_role expression inverts the input map so that the keys are the role names and the values are usernames, but the expression is in grouping mode (due to the ... after name) and so the result will be a map of lists of strings, such as the following:\n\n{\n\n  \"admin\": [\n\n    \"ps\",\n\n  ],\n\n  \"maintainer\": [\n\n    \"am\",\n\n    \"jb\",\n\n    \"kl\",\n\n    \"ma\",\n\n  ],\n\n  \"viewer\": [\n\n    \"st\",\n\n    \"zq\",\n\n  ],\n\n}\n\nCopy\n\nDue to the element ordering rules, Terraform will sort the users lexically by username as part of evaluating the for expression, and so the usernames associated with each role will be lexically sorted after grouping.\n\nRepeated Configuration Blocks\n\nThe for expressions mechanism is for constructing collection values from other collection values within expressions, which you can then assign to individual resource arguments that expect complex values.\n\nSome resource types also define nested block types, which typically represent separate objects that belong to the containing resource in some way. You can't dynamically generate nested blocks using for expressions, but you can generate nested blocks for a resource dynamically using dynamic blocks.\n\nEdit this page on GitHub\n\nOn this page:\n\nfor Expressions\nInput Types\nResult Types\nFiltering Elements\nElement Ordering\nGrouping Results\nRepeated Configuration Blocks\nTheme\nDark\nLight\nSystem\nCertifications\nSystem Status\nCookie Manager\nTerms of Use\nSecurity\nPrivacy\nTrademark Policy\nTrade Controls\nGive Feedback\n(opens in new tab)\nWe use cookies & other similar technology to collect data to improve your experience on our site, as described in our Privacy Policy and Cookie Policy.\nManage PreferencesACCEPT"
  },
  {
    "title": "Local Values - Configuration Language | Terraform | HashiCorp Developer",
    "url": "https://developer.hashicorp.com/terraform/language/values/locals",
    "html": "Skip to main content\nNews\nHashiCorp to join IBM to accelerate multi-cloud automation.\nRead the blog →\nDismiss alert\nTerraform\nInstall\nTutorials\nDocumentation\nRegistry\n(opens in new tab)\nTry Cloud\n(opens in new tab)\nSearch\nCommand or control key\nK key\nTerraform Home\nConfiguration Language\nConfiguration Language\nStyle Guide\nFiles and Directories\nSyntax\nResources\nData Sources\nProviders\nVariables and Outputs\nOverview\nInput Variables\nOutput Values\nLocal Values\nModules\nMoved block\nChecks\nImport\nExpressions\nFunctions\nTerraform Settings\nState\nTests\nUpgrading to Terraform v1.7\nv1.x Compatibility Promises\nTerraform Internals\nResources\nTutorial Library\nCertifications\nCommunity Forum\n(opens in new tab)\nSupport\n(opens in new tab)\nGitHub\n(opens in new tab)\nTerraform Registry\n(opens in new tab)\nDeveloper\nTerraform\nConfiguration Language\nVariables and Outputs\nLocal Values\nv1.8.x (latest)\nLocal Values\n\nHands-on: Try the Simplify Terraform Configuration with Locals tutorial.\n\nA local value assigns a name to an expression, so you can use the name multiple times within a module instead of repeating the expression.\n\nIf you're familiar with traditional programming languages, it can be useful to compare Terraform modules to function definitions:\n\nInput variables are like function arguments.\nOutput values are like function return values.\nLocal values are like a function's temporary local variables.\n\nNote: For brevity, local values are often referred to as just \"locals\" when the meaning is clear from context.\n\nDeclaring a Local Value\n\nA set of related local values can be declared together in a single locals block:\n\nlocals {\n\n  service_name = \"forum\"\n\n  owner        = \"Community Team\"\n\n}\n\nCopy\n\nThe expressions in local values are not limited to literal constants; they can also reference other values in the module in order to transform or combine them, including variables, resource attributes, or other local values:\n\nlocals {\n\n  # Ids for multiple sets of EC2 instances, merged together\n\n  instance_ids = concat(aws_instance.blue.*.id, aws_instance.green.*.id)\n\n}\n\n\n\nlocals {\n\n  # Common tags to be assigned to all resources\n\n  common_tags = {\n\n    Service = local.service_name\n\n    Owner   = local.owner\n\n  }\n\n}\n\nCopy\nUsing Local Values\n\nOnce a local value is declared, you can reference it in expressions as local.<NAME>.\n\nNote: Local values are created by a locals block (plural), but you reference them as attributes on an object named local (singular). Make sure to leave off the \"s\" when referencing a local value!\n\nresource \"aws_instance\" \"example\" {\n\n  # ...\n\n\n\n  tags = local.common_tags\n\n}\n\nCopy\n\nA local value can only be accessed in expressions within the module where it was declared.\n\nWhen To Use Local Values\n\nLocal values can be helpful to avoid repeating the same values or expressions multiple times in a configuration, but if overused they can also make a configuration hard to read by future maintainers by hiding the actual values used.\n\nUse local values only in moderation, in situations where a single value or result is used in many places and that value is likely to be changed in future. The ability to easily change the value in a central place is the key advantage of local values.\n\nEdit this page on GitHub\n\nOn this page:\n\nLocal Values\nDeclaring a Local Value\nUsing Local Values\nWhen To Use Local Values\nTheme\nDark\nLight\nSystem\nCertifications\nSystem Status\nCookie Manager\nTerms of Use\nSecurity\nPrivacy\nTrademark Policy\nTrade Controls\nGive Feedback\n(opens in new tab)\nWe use cookies & other similar technology to collect data to improve your experience on our site, as described in our Privacy Policy and Cookie Policy.\nManage PreferencesACCEPT"
  },
  {
    "title": "Input Variables - Configuration Language | Terraform | HashiCorp Developer",
    "url": "https://developer.hashicorp.com/terraform/language/values/variables#custom-validation-rules",
    "html": "Skip to main content\nNews\nHashiCorp to join IBM to accelerate multi-cloud automation.\nRead the blog →\nDismiss alert\nTerraform\nInstall\nTutorials\nDocumentation\nRegistry\n(opens in new tab)\nTry Cloud\n(opens in new tab)\nSearch\nCommand or control key\nK key\nTerraform Home\nConfiguration Language\nConfiguration Language\nStyle Guide\nFiles and Directories\nSyntax\nResources\nData Sources\nProviders\nVariables and Outputs\nOverview\nInput Variables\nOutput Values\nLocal Values\nModules\nMoved block\nChecks\nImport\nExpressions\nFunctions\nTerraform Settings\nState\nTests\nUpgrading to Terraform v1.7\nv1.x Compatibility Promises\nTerraform Internals\nResources\nTutorial Library\nCertifications\nCommunity Forum\n(opens in new tab)\nSupport\n(opens in new tab)\nGitHub\n(opens in new tab)\nTerraform Registry\n(opens in new tab)\nDeveloper\nTerraform\nConfiguration Language\nVariables and Outputs\nInput Variables\nv1.8.x (latest)\nInput Variables\n\nHands-on: Try the Customize Terraform Configuration with Variables tutorial.\n\nInput variables let you customize aspects of Terraform modules without altering the module's own source code. This functionality allows you to share modules across different Terraform configurations, making your module composable and reusable.\n\nWhen you declare variables in the root module of your configuration, you can set their values using CLI options and environment variables. When you declare them in child modules, the calling module should pass values in the module block.\n\nIf you're familiar with traditional programming languages, it can be useful to compare Terraform modules to function definitions:\n\nInput variables are like function arguments.\nOutput values are like function return values.\nLocal values are like a function's temporary local variables.\n\nNote: For brevity, input variables are often referred to as just \"variables\" or \"Terraform variables\" when it is clear from context what sort of variable is being discussed. Other kinds of variables in Terraform include environment variables (set by the shell where Terraform runs) and expression variables (used to indirectly represent a value in an expression).\n\nDeclaring an Input Variable\n\nEach input variable accepted by a module must be declared using a variable block:\n\nvariable \"image_id\" {\n\n  type = string\n\n}\n\n\n\nvariable \"availability_zone_names\" {\n\n  type    = list(string)\n\n  default = [\"us-west-1a\"]\n\n}\n\n\n\nvariable \"docker_ports\" {\n\n  type = list(object({\n\n    internal = number\n\n    external = number\n\n    protocol = string\n\n  }))\n\n  default = [\n\n    {\n\n      internal = 8300\n\n      external = 8300\n\n      protocol = \"tcp\"\n\n    }\n\n  ]\n\n}\n\nCopy\n\nThe label after the variable keyword is a name for the variable, which must be unique among all variables in the same module. This name is used to assign a value to the variable from outside and to reference the variable's value from within the module.\n\nThe name of a variable can be any valid identifier except the following: source, version, providers, count, for_each, lifecycle, depends_on, locals.\n\nThese names are reserved for meta-arguments in module configuration blocks, and cannot be declared as variable names.\n\nArguments\n\nTerraform CLI defines the following optional arguments for variable declarations:\n\ndefault - A default value which then makes the variable optional.\ntype - This argument specifies what value types are accepted for the variable.\ndescription - This specifies the input variable's documentation.\nvalidation - A block to define validation rules, usually in addition to type constraints.\nsensitive - Limits Terraform UI output when the variable is used in configuration.\nnullable - Specify if the variable can be null within the module.\nDefault values\n\nThe variable declaration can also include a default argument. If present, the variable is considered to be optional and the default value will be used if no value is set when calling the module or running Terraform. The default argument requires a literal value and cannot reference other objects in the configuration.\n\nType Constraints\n\nThe type argument in a variable block allows you to restrict the type of value that will be accepted as the value for a variable. If no type constraint is set then a value of any type is accepted.\n\nWhile type constraints are optional, we recommend specifying them; they can serve as helpful reminders for users of the module, and they allow Terraform to return a helpful error message if the wrong type is used.\n\nType constraints are created from a mixture of type keywords and type constructors. The supported type keywords are:\n\nstring\nnumber\nbool\n\nThe type constructors allow you to specify complex types such as collections:\n\nlist(<TYPE>)\nset(<TYPE>)\nmap(<TYPE>)\nobject({<ATTR NAME> = <TYPE>, ... })\ntuple([<TYPE>, ...])\n\nThe keyword any may be used to indicate that any type is acceptable. For more information on the meaning and behavior of these different types, as well as detailed information about automatic conversion of complex types, see Type Constraints.\n\nIf both the type and default arguments are specified, the given default value must be convertible to the specified type.\n\nInput Variable Documentation\n\nBecause the input variables of a module are part of its user interface, you can briefly describe the purpose of each variable using the optional description argument:\n\nvariable \"image_id\" {\n\n  type        = string\n\n  description = \"The id of the machine image (AMI) to use for the server.\"\n\n}\n\nCopy\n\nThe description should concisely explain the purpose of the variable and what kind of value is expected. This description string might be included in documentation about the module, and so it should be written from the perspective of the user of the module rather than its maintainer. For commentary for module maintainers, use comments.\n\nCustom Validation Rules\n\nThis feature was introduced in Terraform CLI v0.13.0.\n\nYou can specify custom validation rules for a particular variable by adding a validation block within the corresponding variable block. The example below checks whether the AMI ID has the correct syntax.\n\nvariable \"image_id\" {\n\n  type        = string\n\n  description = \"The id of the machine image (AMI) to use for the server.\"\n\n\n\n  validation {\n\n    condition     = length(var.image_id) > 4 && substr(var.image_id, 0, 4) == \"ami-\"\n\n    error_message = \"The image_id value must be a valid AMI id, starting with \\\"ami-\\\".\"\n\n  }\n\n}\n\nCopy\n\nRefer to Custom Condition Checks for more details.\n\nSuppressing Values in CLI Output\n\nThis feature was introduced in Terraform v0.14.0.\n\nHands-on: Try the Protect Sensitive Input Variables tutorial.\n\nSetting a variable as sensitive prevents Terraform from showing its value in the plan or apply output, when you use that variable elsewhere in your configuration.\n\nTerraform will still record sensitive values in the state, and so anyone who can access the state data will have access to the sensitive values in cleartext. For more information, see Sensitive Data in State.\n\nDeclare a variable as sensitive by setting the sensitive argument to true:\n\nvariable \"user_information\" {\n\n  type = object({\n\n    name    = string\n\n    address = string\n\n  })\n\n  sensitive = true\n\n}\n\n\n\nresource \"some_resource\" \"a\" {\n\n  name    = var.user_information.name\n\n  address = var.user_information.address\n\n}\n\nCopy\n\nAny expressions whose result depends on the sensitive variable will be treated as sensitive themselves, and so in the above example the two arguments of resource \"some_resource\" \"a\" will also be hidden in the plan output:\n\nTerraform will perform the following actions:\n\n\n\n  # some_resource.a will be created\n\n  + resource \"some_resource\" \"a\" {\n\n      + name    = (sensitive value)\n\n      + address = (sensitive value)\n\n    }\n\n\n\nPlan: 1 to add, 0 to change, 0 to destroy.\n\nCopy\n\nIn some cases where you use a sensitive variable inside a nested block, Terraform may treat the entire block as redacted. This happens for resource types where all of the blocks of a particular type are required to be unique, and so disclosing the content of one block might imply the content of a sibling block.\n\n  # some_resource.a will be updated in-place\n\n  ~ resource \"some_resource\" \"a\" {\n\n      ~ nested_block {\n\n          # At least one attribute in this block is (or was) sensitive,\n\n          # so its contents will not be displayed.\n\n        }\n\n    }\n\nCopy\n\nA provider can also declare an attribute as sensitive, which will cause Terraform to hide it from regular output regardless of how you assign it a value. For more information, see Sensitive Resource Attributes.\n\nIf you use a sensitive value as part of an output value then Terraform will require you to also mark the output value itself as sensitive, to confirm that you intended to export it.\n\nCases where Terraform may disclose a sensitive variable\n\nA sensitive variable is a configuration-centered concept, and values are sent to providers without any obfuscation. A provider error could disclose a value if that value is included in the error message. For example, a provider might return the following error even if \"foo\" is a sensitive value: \"Invalid value 'foo' for field\"\n\nIf a resource attribute is used as, or part of, the provider-defined resource id, an apply will disclose the value. In the example below, the prefix attribute has been set to a sensitive variable, but then that value (\"jae\") is later disclosed as part of the resource id:\n\n  # random_pet.animal will be created\n\n  + resource \"random_pet\" \"animal\" {\n\n      + id        = (known after apply)\n\n      + length    = 2\n\n      + prefix    = (sensitive value)\n\n      + separator = \"-\"\n\n    }\n\n\n\nPlan: 1 to add, 0 to change, 0 to destroy.\n\n\n\n...\n\n\n\nrandom_pet.animal: Creating...\n\nrandom_pet.animal: Creation complete after 0s [id=jae-known-mongoose]\n\nCopy\nDisallowing Null Input Values\n\nThis feature is available in Terraform v1.1.0 and later.\n\nThe nullable argument in a variable block controls whether the module caller may assign the value null to the variable.\n\nvariable \"example\" {\n\n  type     = string\n\n  nullable = false\n\n}\n\nCopy\n\nThe default value for nullable is true. When nullable is true, null is a valid value for the variable, and the module configuration must always account for the possibility of the variable value being null. Passing a null value as a module input argument will override any default value.\n\nSetting nullable to false ensures that the variable value will never be null within the module. If nullable is false and the variable has a default value, then Terraform uses the default when a module input argument is null.\n\nThe nullable argument only controls where the direct value of the variable may be null. For variables of collection or structural types, such as lists or objects, the caller may still use null in nested elements or attributes, as long as the collection or structure itself is not null.\n\nUsing Input Variable Values\n\nWithin the module that declared a variable, its value can be accessed from within expressions as var.<NAME>, where <NAME> matches the label given in the declaration block:\n\nNote: Input variables are created by a variable block, but you reference them as attributes on an object named var.\n\nresource \"aws_instance\" \"example\" {\n\n  instance_type = \"t2.micro\"\n\n  ami           = var.image_id\n\n}\n\nCopy\n\nThe value assigned to a variable can only be accessed in expressions within the module where it was declared.\n\nAssigning Values to Root Module Variables\n\nWhen variables are declared in the root module of your configuration, they can be set in a number of ways:\n\nIn an HCP Terraform workspace.\nIndividually, with the -var command line option.\nIn variable definitions (.tfvars) files, either specified on the command line or automatically loaded.\nAs environment variables.\n\nThe following sections describe these options in more detail. This section does not apply to child modules, where values for input variables are instead assigned in the configuration of their parent module, as described in Modules.\n\nVariables on the Command Line\n\nTo specify individual variables on the command line, use the -var option when running the terraform plan and terraform apply commands:\n\nterraform apply -var=\"image_id=ami-abc123\"\n\nterraform apply -var='image_id_list=[\"ami-abc123\",\"ami-def456\"]' -var=\"instance_type=t2.micro\"\n\nterraform apply -var='image_id_map={\"us-east-1\":\"ami-abc123\",\"us-east-2\":\"ami-def456\"}'\n\nCopy\n\nThe above examples show appropriate syntax for Unix-style shells, such as on Linux or macOS. For more information on shell quoting, including additional examples for Windows Command Prompt, see Input Variables on the Command Line.\n\nYou can use the -var option multiple times in a single command to set several different variables.\n\nVariable Definitions (.tfvars) Files\n\nTo set lots of variables, it is more convenient to specify their values in a variable definitions file (with a filename ending in either .tfvars or .tfvars.json) and then specify that file on the command line with -var-file:\n\nLinux, Mac OS, and UNIX:\n\nterraform apply -var-file=\"testing.tfvars\"\n\nCopy\n\nPowerShell:\n\nterraform apply -var-file='testing.tfvars'\n\nCopy\n\nWindows cmd.exe:\n\nterraform apply -var-file=\"testing.tfvars\"\n\nCopy\n\nNote: This is how HCP Terraform passes workspace variables to Terraform.\n\nA variable definitions file uses the same basic syntax as Terraform language files, but consists only of variable name assignments:\n\nimage_id = \"ami-abc123\"\n\navailability_zone_names = [\n\n  \"us-east-1a\",\n\n  \"us-west-1c\",\n\n]\n\nCopy\n\nTerraform also automatically loads a number of variable definitions files if they are present:\n\nFiles named exactly terraform.tfvars or terraform.tfvars.json.\nAny files with names ending in .auto.tfvars or .auto.tfvars.json.\n\nFiles whose names end with .json are parsed instead as JSON objects, with the root object properties corresponding to variable names:\n\n{\n\n  \"image_id\": \"ami-abc123\",\n\n  \"availability_zone_names\": [\"us-west-1a\", \"us-west-1c\"]\n\n}\n\nCopy\nEnvironment Variables\n\nAs a fallback for the other ways of defining variables, Terraform searches the environment of its own process for environment variables named TF_VAR_ followed by the name of a declared variable.\n\nThis can be useful when running Terraform in automation, or when running a sequence of Terraform commands in succession with the same variables. For example, at a bash prompt on a Unix system:\n\n$ export TF_VAR_image_id=ami-abc123\n\n$ terraform plan\n\n...\n\nCopy\n\nOn operating systems where environment variable names are case-sensitive, Terraform matches the variable name exactly as given in configuration, and so the required environment variable name will usually have a mix of upper and lower case letters as in the above example.\n\nComplex-typed Values\n\nWhen variable values are provided in a variable definitions file, you can use Terraform's usual syntax for literal expressions to assign complex-typed values, like lists and maps.\n\nSome special rules apply to the -var command line option and to environment variables. For convenience, Terraform defaults to interpreting -var and environment variable values as literal strings, which need only shell quoting, and no special quoting for Terraform. For example, in a Unix-style shell:\n\n$ export TF_VAR_image_id='ami-abc123'\n\nCopy\n\nHowever, if a root module variable uses a type constraint to require a complex value (list, set, map, object, or tuple), Terraform will instead attempt to parse its value using the same syntax used within variable definitions files, which requires careful attention to the string escaping rules in your shell:\n\n$ export TF_VAR_availability_zone_names='[\"us-west-1b\",\"us-west-1d\"]'\n\nCopy\n\nFor readability, and to avoid the need to worry about shell escaping, we recommend always setting complex variable values via variable definitions files. For more information on quoting and escaping for -var arguments, see Input Variables on the Command Line.\n\nValues for Undeclared Variables\n\nIf you have defined a variable value, but not its corresponding variable {} definition, you may get an error or warning depending on how you have provided that value.\n\nIf you provide values for undeclared variables defined as environment variables you will not get an error or warning. This is because environment variables may be declared but not used in all configurations that might be run.\n\nIf you provide values for undeclared variables defined in a file you will get a warning. This is to help in cases where you have provided a variable value meant for a variable declaration, but perhaps there is a mistake in the value definition. For example, the following configuration:\n\nvariable \"moose\" {\n\n  type = string\n\n}\n\nCopy\n\nAnd the following .tfvars file:\n\nmosse = \"Moose\"\n\nCopy\n\nWill cause Terraform to warn you that there is no variable declared \"mosse\", which can help you spot this mistake.\n\nIf you use .tfvars files across multiple configurations and expect to continue to see this warning, you can use the -compact-warnings option to simplify your output.\n\nIf you provide values for undeclared variables on the command line, Terraform will return an error. To avoid this error, either declare a variable block for the value, or remove the variable value from your Terraform call.\n\nVariable Definition Precedence\n\nThe above mechanisms for setting variables can be used together in any combination. If the same variable is assigned multiple values, Terraform uses the last value it finds, overriding any previous values. Note that the same variable cannot be assigned multiple values within a single source.\n\nTerraform loads variables in the following order, with later sources taking precedence over earlier ones:\n\nEnvironment variables\nThe terraform.tfvars file, if present.\nThe terraform.tfvars.json file, if present.\nAny *.auto.tfvars or *.auto.tfvars.json files, processed in lexical order of their filenames.\nAny -var and -var-file options on the command line, in the order they are provided. (This includes variables set by an HCP Terraform workspace.)\n\nImportant: In Terraform 0.12 and later, variables with map and object values behave the same way as other variables: the last value found overrides the previous values. This is a change from previous versions of Terraform, which would merge map values instead of overriding them.\n\nVariable precedence within Terraform tests\n\nWithin Terraform test files, you can specify variable values within variables blocks, either nested within run blocks or defined directly within the file.\n\nVariables defined in this way take precedence over all other mechanisms during test execution, with variables defined within run blocks taking precedence over those defined within the file.\n\nEdit this page on GitHub\n\nOn this page:\n\nInput Variables\nDeclaring an Input Variable\nArguments\nUsing Input Variable Values\nAssigning Values to Root Module Variables\nTheme\nDark\nLight\nSystem\nCertifications\nSystem Status\nCookie Manager\nTerms of Use\nSecurity\nPrivacy\nTrademark Policy\nTrade Controls\nGive Feedback\n(opens in new tab)\nWe use cookies & other similar technology to collect data to improve your experience on our site, as described in our Privacy Policy and Cookie Policy.\nManage PreferencesACCEPT"
  },
  {
    "title": "Expressions - Configuration Language | Terraform | HashiCorp Developer",
    "url": "https://developer.hashicorp.com/terraform/language/expressions",
    "html": "Skip to main content\nNews\nHashiCorp to join IBM to accelerate multi-cloud automation.\nRead the blog →\nDismiss alert\nTerraform\nInstall\nTutorials\nDocumentation\nRegistry\n(opens in new tab)\nTry Cloud\n(opens in new tab)\nSearch\nCommand or control key\nK key\nTerraform Home\nConfiguration Language\nConfiguration Language\nStyle Guide\nFiles and Directories\nSyntax\nResources\nData Sources\nProviders\nVariables and Outputs\nModules\nMoved block\nChecks\nImport\nExpressions\nOverview\nTypes and Values\nStrings and Templates\nReferences to Values\nOperators\nFunction Calls\nConditional Expressions\nFor Expressions\nSplat Expressions\nDynamic Blocks\nCustom Conditions\nType Constraints\nVersion Constraints\nFunctions\nTerraform Settings\nState\nTests\nUpgrading to Terraform v1.7\nv1.x Compatibility Promises\nTerraform Internals\nResources\nTutorial Library\nCertifications\nCommunity Forum\n(opens in new tab)\nSupport\n(opens in new tab)\nGitHub\n(opens in new tab)\nTerraform Registry\n(opens in new tab)\nDeveloper\nTerraform\nConfiguration Language\nExpressions\nv1.8.x (latest)\nExpressions\n\nHands-on: Try the Create Dynamic Expressions tutorial.\n\nExpressions refer to or compute values within a configuration. The simplest expressions are just literal values, like \"hello\" or 5, but the Terraform language also allows more complex expressions such as references to data exported by resources, arithmetic, conditional evaluation, and a number of built-in functions.\n\nExpressions can be used in a number of places in the Terraform language, but some contexts limit which expression constructs are allowed, such as requiring a literal value of a particular type or forbidding references to resource attributes. Each language feature's documentation describes any restrictions it places on expressions.\n\nYou can experiment with the behavior of Terraform's expressions from the Terraform expression console, by running the terraform console command.\n\nThe other pages in this section describe the features of Terraform's expression syntax.\n\nTypes and Values documents the data types that Terraform expressions can resolve to, and the literal syntaxes for values of those types.\n\nStrings and Templates documents the syntaxes for string literals, including interpolation sequences and template directives.\n\nReferences to Values documents how to refer to named values like variables and resource attributes.\n\nOperators documents the arithmetic, comparison, and logical operators.\n\nFunction Calls documents the syntax for calling Terraform's built-in functions.\n\nConditional Expressions documents the <CONDITION> ? <TRUE VAL> : <FALSE VAL> expression, which chooses between two values based on a bool condition.\n\nFor Expressions documents expressions like [for s in var.list : upper(s)], which can transform a complex type value into another complex type value.\n\nSplat Expressions documents expressions like var.list[*].id, which can extract simpler collections from more complicated expressions.\n\nDynamic Blocks documents a way to create multiple repeatable nested blocks within a resource or other construct.\n\nType Constraints documents the syntax for referring to a type, rather than a value of that type. Input variables expect this syntax in their type argument.\n\nVersion Constraints documents the syntax of special strings that define a set of allowed software versions. Terraform uses version constraints in several places.\n\nEdit this page on GitHub\nTheme\nDark\nLight\nSystem\nCertifications\nSystem Status\nCookie Manager\nTerms of Use\nSecurity\nPrivacy\nTrademark Policy\nTrade Controls\nGive Feedback\n(opens in new tab)\nWe use cookies & other similar technology to collect data to improve your experience on our site, as described in our Privacy Policy and Cookie Policy.\nManage PreferencesACCEPT"
  },
  {
    "title": "Style Guide - Configuration Language | Terraform | HashiCorp Developer",
    "url": "https://developer.hashicorp.com/terraform/language/v1.2.x/style",
    "html": "Skip to main content\nNews\nHashiCorp to join IBM to accelerate multi-cloud automation.\nRead the blog →\nDismiss alert\nTerraform\nInstall\nTutorials\nDocumentation\nRegistry\n(opens in new tab)\nTry Cloud\n(opens in new tab)\nSearch\nCommand or control key\nK key\nTerraform Home\nConfiguration Language\nConfiguration Language\nTerraform Language\nOverview\nStyle Guide\nFiles and Directories\nSyntax\nResources\nData Sources\nProviders\nVariables and Outputs\nModules\nExpressions\nFunctions\nTerraform Settings\nState\nUpgrading to Terraform v1.2\nv1.x Compatibility Promises\nTerraform Internals\nResources\nTutorial Library\nCertifications\nCommunity Forum\n(opens in new tab)\nSupport\n(opens in new tab)\nGitHub\n(opens in new tab)\nTerraform Registry\n(opens in new tab)\n\nYou are viewing documentation for version v1.2.x. View latest version.\n\nDeveloper\nTerraform\nConfiguration Language v1.2.x\nStyle Guide\nv1.2.x\nStyle Guide\n\nThe flexibility of Terraform's configuration language gives you many options to choose from as you write your code, structure your directories, and test your configuration. While some design decisions depend on your organization's needs or preferences, there are some common patterns that we suggest you adopt. Adopting and adhering to a style guide keeps your Terraform code legible, scalable, and maintainable.\n\nThis article discusses best practices and some considerations to keep in mind as you develop your organization's style guide. The article is split into two sections. The first section covers code style recommendations, such as formatting and resource organization. The second section covers operations and workflow recommendations, such as lifecycle management through meta-arguments, versioning, and sensitive data management.\n\nCode style\n\nWriting Terraform code in a consistent style makes it easier to read and maintain. The following sections discuss code style recommendations, including the following:\n\nRun terraform fmt and terraform validate before committing your code to version control.\nUse a linter such as TFLint to enforce your organization's own coding best practices.\nUse # for single and multi-line comments.\nUse nouns for resource names and do not include the resource type in the name.\nUse underscores to separate multiple words in names. Wrap the resource type and name in double quotes in your resource definition.\nLet your code build on itself: define dependent resources after the resources that reference them.\nInclude a type and description for every variable.\nInclude a description for every output.\nAvoid overuse of variables and local values.\nAlways include a default provider configuration.\nUse count and for_each sparingly.\nCode formatting\n\nThe Terraform parser allows you some flexibility in how you lay out the elements in your configuration files, but the Terraform language also has some idiomatic style conventions which we recommend users always follow for consistency between files and modules written by different teams.\n\nIndent two spaces for each nesting level\n\nWhen multiple arguments with single-line values appear on consecutive lines at the same nesting level, align their equals signs:\n\nami           = \"abc123\"\n\ninstance_type = \"t2.micro\"\n\n\nWhen both arguments and blocks appear together inside a block body, place all of the arguments together at the top and then place nested blocks below them. Use one blank line to separate the arguments from the blocks.\n\nUse empty lines to separate logical groups of arguments within a block.\n\nFor blocks that contain both arguments and \"meta-arguments\" (as defined by the Terraform language semantics), list meta-arguments first and separate them from other arguments with one blank line. Place meta-argument blocks last and separate them from other blocks with one blank line. Refer to dynamic resource count for more information on meta-arguments.\n\nresource \"aws_instance\" \"example\" {\n\n  # meta-argument first\n\n  count = 2\n\n\n\n  ami           = \"abc123\"\n\n  instance_type = \"t2.micro\"\n\n\n\n  network_interface {\n\n    # ...\n\n  }\n\n\n\n  # meta-argument block last\n\n  lifecycle {\n\n    create_before_destroy = true\n\n  }\n\n}\n\n\nTop-level blocks should always be separated from one another by one blank line. Nested blocks should also be separated by blank lines, except when grouping together related blocks of the same type (like multiple provisioner blocks in a resource).\n\nAvoid grouping multiple blocks of the same type with other blocks of a different type, unless the block types are defined by semantics to form a family. (For example: root_block_device, ebs_block_device and ephemeral_block_device on aws_instance form a family of block types describing AWS block devices, and can therefore be grouped together and mixed.)\n\nThe terraform fmt command formats your Terraform configuration to a subset of the above recommendations. By default, the terraform fmt command will only modify your Terraform code in the directory that you execute it in, but you can include the -recursive flag to modify code in all subdirectories as well.\n\nWe recommend that you run terraform fmt before each commit to version control. You can use mechanisms such as Git pre-commit hooks to automatically run this command each time you commit your code.\n\nIf you use Microsoft VS Code, use the Terraform VS Code extension to enable features such as syntax highlighting and validation, automatic code formatting, and integration with Terraform Cloud. If your development environment or text editor supports the Language Server Protocol, you can use the Terraform Language Server to access most of the VS Code extension features.\n\nCode validation\n\nThe terraform validate command checks that your configuration is syntactically valid and internally consistent. The validate command does not check if argument values are valid for a specific provider, but it will verify that they are the correct type. It does not evaluate any existing state.\n\nThe terraform validate command is safe to run automatically and frequently. You can configure your text editor to run this command as a post-save check, define it as a pre-commit hook in a Git repository, or run it as a step in a CI/CD pipeline.\n\nFor more information, refer to the Terraform validate documentation.\n\nFile names\n\nWe recommend the following file naming conventions:\n\nA backend.tf file that contains your backend configuration. You can define multiple terraform blocks in your configuration to separate your backend configuration from your Terraform and provider versioning configuration.\nA main.tf file that contains all resource and data source blocks.\nA outputs.tf file that contains all output blocks in alphabetical order.\nA providers.tf file that contains all provider blocks and configuration.\nA terraform.tf file that contains a single terraform block which defines your required_version and required_providers.\nA variables.tf file that contains all variable blocks in alphabetical order.\nA locals.tf file that contains local values. Refer to local values for more information.\nA override.tf file that contains override definitions for your configuration. Terraform loads this and all files ending with _override.tf last. Use them sparingly and add comments to the original resource definitions, as these overrides make your code harder to reason about. Refer to the override files documentation for more information.\n\nAs your codebase grows, limiting it to just these files can become difficult to maintain. If your code becomes hard to navigate due to its size, we recommend that you organize resources and data sources in separate files by logical groups. For example, if your web application requires networking, storage, and compute resources, you might create the following files:\n\nA network.tf file that contains your VPC, subnets, load balancers, and all other networking resources.\nA storage.tf file that contains your object storage and related permissions configuration.\nA compute.tf file that contains your compute instances.\n\nNo matter how you decide to split your code, it should be immediately clear where a maintainer can find a specific resource or data source definition.\n\nAs your configuration grows, you may need to separate it into multiple state files. The HashiCorp Well-Architected Framework provides more guidance about configuration structure and scope.\n\nLinting and static code analysis\n\nTerraform does not have a built-in linter, but many organizations rely on a third party linting tool such as TFLint to enforce code standards. A linter uses static code analysis to compare your Terraform code against a set of rules. Most linters ship with a default set of rules, but also let you write your own.\n\nComments\n\nWrite your code so it is easy to understand. Only when necessary, use comments to clarify complexity for other maintainers.\n\nUse # for both single- and multi-line comments. The // and /* */ comment syntaxes are not considered idiomatic, but Terraform supports them to remain backwards-compatible with earlier versions of HCL.\n\n# Each tunnel is responsible for encrypting and decrypting traffic exiting\n\n# and leaving its associated gateway.\n\nresource \"google_compute_vpn_tunnel\" \"tunnel1\" {\n\n  ## ...\n\nResource naming\n\nEvery resource within a configuration must have a unique name. For consistency and readability, use a descriptive noun and separate words with underscores. Do not include the resource type in the resource identifier since the resource address already includes it. Wrap the resource type and name in double quotes.\n\n❌ Bad:\n\nresource aws_instance webAPI-aws-instance {...}\n\n\n✅ Good:\n\nresource \"aws_instance\" \"web_api\" {...}\n\nResource order\n\nThe order of the resources and data sources in your code does not affect how Terraform builds them, so organize your resources for readability. Terraform determines the creation order based on cross-resource dependencies.\n\nHow you order your resources largely depends on the size and complexity of your code, but we recommend defining data sources alongside the resources that reference them. For readability, your Terraform code should “build on itself” — you should define a data source before the resource that references it.\n\nThe following example defines an aws_instance that relies on two data sources, aws_ami and aws_availability_zone. For readability and continuity, it defines the data sources before the aws_instance resource.\n\ndata \"aws_ami\" \"web\" {\n\n  ##...\n\n}\n\n\n\ndata \"aws_availability_zones\" \"available\" {\n\n  ##...\n\n}\n\n\n\nresource \"aws_instance\" \"web\" {\n\n  ami               = data.aws_ami.web.id\n\n  availability_zone = data.aws_availability_zones.available.names[0]\n\n  ##...\n\n}\n\n\nWe recommend following a consistent order for resource parameters:\n\nIf present, The count or for_each meta-argument.\nResource-specific non-block parameters.\nResource-specific block parameters.\nIf required, a lifecycle block.\nIf required, the depends_on parameter.\nVariables\n\nWhile variables make your modules more flexible, overusing variables can make code difficult to understand. When deciding whether to expose a variable for a resource setting, consider whether that parameter will change between deployments.\n\nDefine a type and a description for every variable.\n\nIf the variable is optional, define a reasonable default.\n\nFor sensitive variables, such as passwords and private keys, set the sensitive parameter to true. Remember that Terraform will still store this value in plain text in its state, but it will not display it when you run terraform plan or terraform apply. Refer to secrets management for more information on how to securely handle sensitive values.\n\nUse input variable validation to create additional rules for your variable values in addition to Terraform's type validation. Only use variable validation when your variable values have uniquely restrictive requirements. For example, if your Terraform configuration requires two web instances, add a validation block to enforce it:\n\nvariable \"web_instance_count\" {\n\n  type        = number\n\n  description = \"Number of web instances to deploy. This application requires at least two instances.\"\n\n\n\n  validation {\n\n    condition     = var.web_instance_count > 1\n\n    error_message = \"This application requires at least two web instances.\"\n\n  }\n\n}\n\n\nWe recommend following a consistent order for variable parameters:\n\nType\nDescription\nDefault (optional)\nSensitive (optional)\nValidation blocks\nOutputs\n\nOutput values let you expose data about your infrastructure on the command line and make it easy to reference in other Terraform configurations. Like you would for variables, provide a description for each output.\n\nWe recommend that you use the following order for your output parameters:\n\nDescription\nValue\nSensitive (optional)\n\nEvery variable and output requires a unique name. For consistency and readability, we recommend that you use a descriptive noun and separate words with underscores.\n\nvariable \"db_disk_size\" {\n\n type        = number\n\n description = \"Disk size for the API database\"\n\n default     = 100\n\n}\n\n\n\nvariable \"db_password\" {\n\n type        = string\n\n description = \"Database password\"\n\n sensitive   = true\n\n}\n\n\n\noutput \"web_public_ip\" {\n\n description = \"Public IP of the web instance\"\n\n value       = aws_instance.web.public_ip\n\n}\n\nLocal values\n\nLocal values let you reference an expression or value multiple times. Use local values sparingly, as overuse can make your code harder to understand.\n\nFor example, you can use a local value to create a suffix for the region and environment (for example, development or test), and append it to multiple resources.\n\nlocals {\n\n  name_suffix = \"${var.region}-${var.environment}\"\n\n}\n\n\n\nresource \"aws_instance\" \"web\" {\n\n  ami           = data.aws_ami.ubuntu.id\n\n  instance_type = \"t3.micro\"\n\n\n\n  tags = {\n\n    Name = \"web-${local.name_suffix}\"\n\n  }\n\n}\n\n\nDefine local values in one of two places:\n\nIf you reference the local value in multiple files, define it in a file named locals.tf.\nIf the local is specific to a file, define it at the top of that file.\n\nAs for other Terraform objects, use descriptive nouns for local value names and underscores to separate multiple words.\n\nFor more information, refer to the local values documentation and the Simplify Terraform configuration with locals tutorial.\n\nProvider aliasing\n\nProvider aliasing lets you define multiple provider blocks for the same Terraform provider. Potential use cases for aliases include provisioning resources in multiple regions within a single configuration. The provider meta-argument for resources and the providers meta-argument for modules specifies which provider to use.\n\nproviders.tf\nprovider \"aws\" {\n\n  region = \"us-east-1\"\n\n}\n\n\n\nprovider \"aws\" {\n\n  alias  = \"west\"\n\n  region = \"us-west-2\"\n\n}\n\nmain.tf\nresource \"aws_instance\" \"example\" {\n\n  provider = aws.west\n\n  # ...\n\n}\n\n\n\nmodule \"aws_vpc\" {\n\n  source = \"./aws_vpc\"\n\n  providers = {\n\n    aws = aws.west\n\n  }\n\n}\n\nAny provider block that does not define the alias parameter is the default provider configuration.\nAlways include a default provider configuration and define all of your providers in the same file.\nIf you define multiple instances of a provider, define the default first.\nFor non-default providers, define the alias as the first parameter of the provider block.\nDynamic resource count\n\nThe for_each and count meta-arguments let you create multiple resources from a single resource block depending on run-time conditions. You can use these meta-arguments to make your code flexible and reduce duplicate resource blocks. If the resources are almost identical, use count. If some of arguments need distinct values that you cannot derive from an integer, use for_each.\n\nThe for_each meta-argument accepts a map or set value, and Terraform will create an instance of that resource for each element in the value you provide. In the following example, Terraform creates an aws_instance for each of the strings defined in the web_instances variable: \"ui\", \"api\", \"db\" and \"metrics\". The example uses each.key to give each instance a unique name. The web_private_ips output uses a for expression to create a map of instance names and their private IP addresses, while the web_ui_public_ip output addresses the instance with the key \"ui\" directly.\n\nvariable \"web_instances\" {\n\n  type        = list(string)\n\n  description = \"A list of instances for the web application\"\n\n  default = [\n\n    \"ui\",\n\n    \"api\",\n\n    \"db\",\n\n    \"metrics\"\n\n  ]\n\n}\n\nresource \"aws_instance\" \"web\" {\n\n  for_each = toset(var.web_instances)\n\n  ami           = data.aws_ami.webapp.id\n\n  instance_type = \"t3.micro\"\n\n  tags = {\n\n    Name = \"web_${each.key}\"\n\n  }\n\n}\n\noutput \"web_private_ips\" {\n\n  description = \"Private IPs of the web instances\"\n\n  value = {\n\n    for k, v in aws_instance.web : k => v.private_ip\n\n  }\n\n}\n\noutput \"web_ui_public_ip\" {\n\n  description = \"Public IP of the web UI instance\"\n\n  value       = aws_instance.web[\"ui\"].public_ip\n\n}\n\n\nThe above example will create the following output:\n\nweb_private_ips = {\n\n  \"api\" = \"172.31.25.29\"\n\n  \"db\" = \"172.31.18.33\"\n\n  \"metrics\" = \"172.31.26.112\"\n\n  \"ui\" = \"172.31.20.142\"\n\n}\n\nweb_ui_public_ip = \"18.216.208.182\"\n\n\nRefer to the for_each meta-argument documentation for more examples.\n\nThe count meta-argument lets you create multiple instances of a resource from a single resource block. Refer to the count meta-argument documentation for examples.\n\nA common practice to conditionally create resources is to use the count meta-argument with a conditional expression. In the following example, Terraform will only create the aws_instance if var.enable_metrics is true.\n\nvariable \"enable_metrics\" {\n\n  description = \"True if the metrics server should be deployed\"\n\n  type        = bool\n\n  default     = true\n\n}\n\n\n\nresource \"aws_instance\" \"web\" {\n\n  count = var.enable_metrics ? 1 : 0\n\n\n\n  ami           = data.aws_ami.webapp.id\n\n  instance_type = \"t3.micro\"\n\n  ##...\n\n}\n\n\nMeta-arguments simplify your code but add complexity, so use them in moderation. If the effect of the meta-argument is not immediately obvious, use a comment for clarification.\n\nTo learn more about these meta-arguments, refer to the for_each and count documentation.\n\n.gitignore\n\nDefine a .gitignore file for your repository to exclude files that you should not publish to version control, such as your state file.\n\nDo not commit:\n\nYour terraform.tfstate state file, including terraform.tfstate.* backup state files.\nYour .terraform.tfstate.lock.info file. Terraform creates and deletes this file automatically when you run a terraform apply command and contains info about your state lock\nYour .terraform directory, where Terraform downloads providers and child modules. Saved plan files that you create when you include the -out flat when you run terraform plan.\nAny .tfvars files that contain sensitive information.\n\nAlways commit:\n\nAll Terraform code files\nYour .terraform.lock.hcl dependency lock file\nA .gitignore file that excludes the files listed below\nA README.md to describe the code, input variables, and outputs\n\nFor an example, refer to GitHub's Terraform .gitignore file.\n\nWorkflow style\n\nThis section reviews standards that enable predictable and secure Terraform workflows, such as:\n\nPin your Terraform, provider, and module versions.\nName your module repositories using this three-part name terraform-<PROVIDER>-<NAME> when using the Terraform Cloud registry.\nStore local modules at ./modules/<module_name>.\nUse the tfe_outputs data source or provider-specific data sources to share state between two state files.\nProtect credentials by using dynamic provider credentials or a secrets manager such as HashiCorp Vault.\nWrite tests for your modules.\nUse policy enforcement on Terraform Cloud to set guardrails for infrastructure operations.\nVersion pinning\n\nTo prevent providers and modules upgrades from introducing unintentional changes to your infrastructure, use version pinning.\n\nSpecify provider versions using the required_providers block. Terraform version constraints support a range of accepted versions.\n\nPin modules to a specific major and minor version as shown in the example below to ensure stability. You can use looser restrictions if you are certain that the module does not introduce breaking changes outside of major version updates.\n\nWe also recommend that you set a minimum required version of the Terraform binary using the required_version in your terraform block. This requires all operators to use a Terraform version that has all of your configuration's required features.\n\nterraform {\n\n  required_providers {\n\n    aws = {\n\n      source  = \"hashicorp/aws\"\n\n      version = \"5.34.0\"\n\n    }\n\n  }\n\n\n\n  required_version = \">= 1.7\"\n\n}\n\n\nThe above example pins the version of the hashicorp/aws provider to version 5.34.0, and requires that operators use Terraform 1.7 or newer.\n\nFor modules sourced from a registry, use the version parameter in the module block to pin the version. For local modules, Terraform ignores the version parameter.\n\nmodule \"vault_starter\" {\n\n  source  = \"hashicorp/vault-starter/aws\"\n\n  version = \"1.0.0\"\n\n  ##...\n\n}\n\nModule repository names\n\nThe Terraform registry requires that repositories match a naming convention for all modules that you publish to the registry. Module repositories must use this three-part name terraform-<PROVIDER>-<NAME>, where <NAME> reflects the type of infrastructure the module manages and <PROVIDER> is the main provider the module uses. The <NAME> segment can contain additional hyphens, for example, terraform-google-vault or terraform-aws-ec2-instance.\n\nModule structure\n\nTerraform modules define self-contained, reusable pieces of infrastructure-as-code.\n\nUse modules to group together logically related resources that you need to provision together. For example:\n\nA networking module that defines a VPC, along with its subnets, gateway, and security groups.\nAn application module defining all resources required for each deployment. This stack could include web servers, databases, storage, and supported networking.\n\nReview the module creation recommended pattern documentation and standard module structure for guidance on how to structure your modules.\n\nLocal modules\n\nLocal modules are sourced from local disk rather than a remote module registry. We recommend publishing your modules to a module registry, such as the Terraform Cloud private module registry, to easily version, share, and reuse modules across your organization. If you cannot use a module registry, using local modules can simplify maintaining and updating your code.\n\nWe recommend that you define child modules in the ./modules/<module_name> directory.\n\nRepository structure\n\nHow you structure your modules and Terraform configuration in version control significantly impacts versioning and operations. We recommend that you store your actual infrastructure configuration separately from your module code.\n\nStore each module in an individual repository. This lets you independently version each module and makes it easier to publish your modules in the private Terraform registry.\n\nOrganize your infrastructure configuration in repositories that group together logically-related resources. For example, a single repository for a web application that requires compute, networking, and database resources . By separating your resources into groups, you limit the number of resources that may be impacted by failures for any operation.\n\nAnother approach is to group all modules and infrastructure configuration into a single monolithic repository, or monorepo. For example, a monorepo may define a collection of local modules for each component of the infrastructure stack, and deploy them in the root module.\n\n.\n\n├── modules\n\n│   ├── function\n\n│   │   ├── main.tf      # contains aws_iam_role, aws_lambda_function\n\n│   │   ├── outputs.tf\n\n│   │   └── variables.tf\n\n│   ├── queue\n\n│   │   ├── main.tf      # contains aws_sqs_queue\n\n│   │   ├── outputs.tf\n\n│   │   └── variables.tf\n\n│   └── vpc\n\n│       ├── main.tf      # contains aws_vpc, aws_subnet\n\n│       ├── outputs.tf\n\n│       └── variables.tf\n\n├── main.tf\n\n├── outputs.tf\n\n└── variables.tf\n\n\nThe advantage of monolithic repositories is having a single source of truth that tracks every infrastructure change. However, monolithic repositories can complicate your CI/CD automation: since any code change triggers a deployment that operates on your entire repository, your workflow must target only the modified directories. You also lose the granular access control, since anyone with repository access can modify any file in it.\n\nIf your organization requires a monolithic approach, Terraform Cloud and Terraform Enterprise let you scope a workspace to a specific directory in a repository, simplifying your workflows.\n\nBranching strategy\n\nTo collaborate on your Terraform code, we recommend using the GitHub flow. This approach uses short-lived branches to help your team quickly review, test, and merge changes to your code. To make changes to your code, you would:\n\nCreate a new branch from your main branch\nWrite, commit, and push your changes to the new branch\nCreate a pull request\nReview the changes with your team\nMerge the pull request\nDelete the branch\n\nTerraform Cloud and Terraform Enterprise can run speculative plans for pull requests. These speculative plans run automatically when you create or update a pull request, and you can use them to see the effect that your changes will have on your infrastructure before you merge them to your main branch. When you merge your pull request, Terraform Cloud will start a new run to apply these changes.\n\nMultiple environments\n\nWe recommend that your repository's main branch be the source of truth for all environments. For Terraform Cloud and Terraform Enterprise users, we recommend that you use separate workspaces for each environment. For larger codebases, we recommend that you split your resources across multiple workspaces to prevent large state files and limit unintended consequences from changes. For example, you could structure your code as follows:\n\n.\n\n├── compute\n\n│   ├── main.tf\n\n│   ├── outputs.tf\n\n│   └── variables.tf\n\n├── database\n\n│   ├── main.tf\n\n│   ├── outputs.tf\n\n│   └── variables.tf\n\n└── networking\n\n    ├── main.tf\n\n    ├── outputs.tf\n\n    └── variables.tf\n\n\nIn this scenario, you would create three workspaces per environment. For example, your production environment would have a prod-compute, prod-database, and prod-networking workspace. Read more about Terraform workspace and project best practices.\n\nIf you do not use Terraform Cloud or Terraform Enterprise, we recommend that you use modules to encapsulate your configuration, and use a directory for each environment so that each one has a separate state file. The configuration in each of these directories would call the local modules, each with parameters specific to their environment. This also lets you maintain separate variable and backend configurations for each environment.\n\n├── modules\n\n│   ├── compute\n\n│   │   └── main.tf\n\n│   ├── database\n\n│   │   └── main.tf\n\n│   └── network\n\n│       └── main.tf\n\n├── dev\n\n│   ├── backend.tf\n\n│   ├── main.tf\n\n│   └── variables.tf\n\n├── prod\n\n│   ├── backend.tf\n\n│   ├── main.tf\n\n│   └── variables.tf\n\n└── staging\n\n    ├── backend.tf\n\n    ├── main.tf\n\n    └── variables.tf\n\nState sharing\n\nSince your state contains sensitive information, avoid sharing full state files when possible.\n\nIf you use Terraform Cloud or Terraform Enterprise and need to reference resources across workspaces, use the tfe_outputs data source.\n\nIf you do not use Terraform Cloud or Terraform Enterprise but still need to reference data about other infrastructure resources, use data sources to query the provider. For example, you can use the aws_instance data source to look up an AWS EC2 instance by its ID or tags.\n\nSecrets management\n\nIf you do not configure remote state storage, the Terraform CLI stores the entire state in plaintext on the local disk. State can include sensitive data, such as passwords and private keys. Terraform Cloud and Terraform Enterprise provide state encryption through HashiCorp Vault.\n\nIf you use Terraform Cloud or Terraform Enterprise, we recommend the following:\n\nWhen using Terraform Enterprise, define and enforce a Sentinel policy to prevent use of the local_exec provisioner or external data sources.\nWhen using Terraform Cloud or Terraform Enterprise, use dynamic provider credentials to avoid using long-lived static credentials.\n\nIf you use Terraform Community Edition, we recommend the following:\n\nConfigure provider credentials using provider-specific environment variables.\nAccess secrets from a secrets management system such as HashiCorp Vault with the Terraform Vault provider. Be aware that Terraform will still write these values in plaintext to your state file.\n\nIf you use a custom CI/CD pipeline, review your CI/CD tool's best practices for managing sensitive values. Most tools let you access sensitive values as environment variables. For more information, refer to your CI/CD documentation.\n\nUsing secrets in GitHub Actions\nGitlab pipeline security\nIntegrate Vault into your CI/CD pipeline\nIntegration and unit testing\n\nTerraform tests let you validate your modules and catch breaking changes. We recommend that you write tests for your Terraform modules and run them just as you run your tests for your application code, such as pre-merge check in your pull requests or as a prerequisite step in your automated CI/CD pipeline.\n\nTests differ from validation methods such as variable validation, preconditions, postconditions, and check blocks. These features focus on verifying the infrastructure deployed by your code, while tests validate the behavior and logic of your code itself. For more information, refer to the Terraform test documentation and the Write Terraform tests tutorial.\n\nPolicy\n\nPolicies are rules that Terraform Cloud enforces on Terraform runs. You can use policies to validate that the Terraform plan complies with your organization's best practices. For example, you can write policies that:\n\nLimit the size of a web instance\nCheck for required resource tags\nBlock deployments on Fridays\nEnforce security configuration and cost management\n\nWe recommend that you store policies in a separate VCS repository from your Terraform code.\n\nFor more information, refer to the policy enforcement documentation, as well as the enforce policy with Sential and detect infrastructure drift and enforce OPA policies tutorials.\n\nNext steps\n\nThis article introduces some considerations to keep in mind as you standardize your organization's Terraform style guidelines. Enforcing a standard way of writing and organizing your Terraform code across your organization ensures that it is readable, maintainable, and shareable.\n\nThe HashiCorp Well-Architected Framework provides more guidance on adapting your Terraform workflows for scale.\n\nOn this page:\n\nStyle Guide\nCode style\nCode formatting\nCode validation\nFile names\nLinting and static code analysis\nComments\nResource naming\nResource order\nVariables\nOutputs\nLocal values\nProvider aliasing\nDynamic resource count\n.gitignore\nWorkflow style\nVersion pinning\nModule repository names\nModule structure\nLocal modules\nRepository structure\nBranching strategy\nMultiple environments\nState sharing\nSecrets management\nIntegration and unit testing\nPolicy\nNext steps\nTheme\nDark\nLight\nSystem\nCertifications\nSystem Status\nCookie Manager\nTerms of Use\nSecurity\nPrivacy\nTrademark Policy\nTrade Controls\nGive Feedback\n(opens in new tab)\nWe use cookies & other similar technology to collect data to improve your experience on our site, as described in our Privacy Policy and Cookie Policy.\nManage PreferencesACCEPT"
  },
  {
    "title": "Backend Configuration - Configuration Language | Terraform | HashiCorp Developer",
    "url": "https://developer.hashicorp.com/terraform/language/settings/backends/configuration",
    "html": "Skip to main content\nNews\nHashiCorp to join IBM to accelerate multi-cloud automation.\nRead the blog →\nDismiss alert\nTerraform\nInstall\nTutorials\nDocumentation\nRegistry\n(opens in new tab)\nTry Cloud\n(opens in new tab)\nSearch\nCommand or control key\nK key\nTerraform Home\nConfiguration Language\nConfiguration Language\nStyle Guide\nFiles and Directories\nSyntax\nResources\nData Sources\nProviders\nVariables and Outputs\nModules\nMoved block\nChecks\nImport\nExpressions\nFunctions\nTerraform Settings\nOverview\nHCP Terraform\nBackends\nBackend Configuration\nAvailable Backends\nState\nTests\nUpgrading to Terraform v1.7\nv1.x Compatibility Promises\nTerraform Internals\nResources\nTutorial Library\nCertifications\nCommunity Forum\n(opens in new tab)\nSupport\n(opens in new tab)\nGitHub\n(opens in new tab)\nTerraform Registry\n(opens in new tab)\nDeveloper\nTerraform\nConfiguration Language\nTerraform Settings\nBackends\nBackend Configuration\nv1.8.x (latest)\nBackend Configuration\n\nA backend defines where Terraform stores its state data files.\n\nTerraform uses persisted state data to keep track of the resources it manages. Most non-trivial Terraform configurations either integrate with HCP Terraform or use a backend to store state remotely. This lets multiple people access the state data and work together on that collection of infrastructure resources.\n\nThis page describes how to configure a backend by adding the backend block to your configuration.\n\nNote: In Terraform versions before 1.1.0, we classified backends as standard or enhanced. The enhanced label differentiated the remote backend, which could both store state and perform Terraform operations. This classification has been removed. Refer to Using HCP Terraform for details about storing state, executing remote operations, and using HCP Terraform directly from Terraform.\n\nAvailable Backends\n\nBy default, Terraform uses a backend called local, which stores state as a local file on disk. You can also configure one of the built-in backends included in this documentation.\n\nSome of these backends act like plain remote disks for state files, while others support locking the state while operations are being performed. This helps prevent conflicts and inconsistencies. The built-in backends listed are the only backends. You cannot load additional backends as plugins.\n\nNote: We removed the artifactory, etcd, etcdv3, manta, and swift backends in Terraform v1.3. Information about their behavior in older versions is still available in the Terraform v1.2 documentation. For migration paths from these removed backends, refer to Upgrading to Terraform v1.3.\n\nUsing a Backend Block\n\nYou do not need to configure a backend when using HCP Terraform because HCP Terraform automatically manages state in the workspaces associated with your configuration. If your configuration includes a cloud block, it cannot include a backend block.\n\nTo configure a backend, add a nested backend block within the top-level terraform block. The following example configures the remote backend.\n\nterraform {\n\n  backend \"remote\" {\n\n    organization = \"example_corp\"\n\n\n\n    workspaces {\n\n      name = \"my-app-prod\"\n\n    }\n\n  }\n\n}\n\nCopy\n\nThere are some important limitations on backend configuration:\n\nA configuration can only provide one backend block.\nA backend block cannot refer to named values (like input variables, locals, or data source attributes).\nYou cannot reference values declared within backend blocks elsewhere in the configuration. Refer to References to Resource Attributes for more details.\nCredentials and Sensitive Data\n\nBackends store state in a remote service, which allows multiple people to access it. Accessing remote state generally requires access credentials, since state data contains extremely sensitive information.\n\nWarning: We recommend using environment variables to supply credentials and other sensitive data. If you use -backend-config or hardcode these values directly in your configuration, Terraform will include these values in both the .terraform subdirectory and in plan files. This can leak sensitive credentials.\n\nTerraform writes the backend configuration in plain text in two separate files.\n\nThe .terraform/terraform.tfstate file contains the backend configuration for the current working directory.\nAll plan files capture the information in .terraform/terraform.tfstate at the time the plan was created. This helps ensure Terraform is applying the plan to correct set of infrastructure.\n\nWhen applying a plan that you previously saved to a file, Terraform uses the backend configuration stored in that file instead of the current backend settings. If that configuration contains time-limited credentials, they may expire before you finish applying the plan. Use environment variables to pass credentials when you need to use different values between the plan and apply steps.\n\nBackend Types\n\nThe block label of the backend block (\"remote\", in the example above) indicates which backend type to use. Terraform has a built-in selection of backends, and the configured backend must be available in the version of Terraform you are using.\n\nThe arguments used in the block's body are specific to the chosen backend type; they configure where and how the backend will store the configuration's state, and in some cases configure other behavior.\n\nSome backends allow providing access credentials directly as part of the configuration for use in unusual situations, for pragmatic reasons. However, in normal use, we do not recommend including access credentials as part of the backend configuration. Instead, leave those arguments completely unset and provide credentials using the credentials files or environment variables that are conventional for the target system, as described in the documentation for each backend.\n\nRefer to the page for each backend type for full details and that type's configuration arguments.\n\nDefault Backend\n\nIf a configuration includes no backend block, Terraform defaults to using the local backend, which stores state as a plain file in the current working directory.\n\nInitialization\n\nWhen you change a backend's configuration, you must run terraform init again to validate and configure the backend before you can perform any plans, applies, or state operations.\n\nAfter you initialize, Terraform creates a .terraform/ directory locally. This directory contains the most recent backend configuration, including any authentication parameters you provided to the Terraform CLI. Do not check this directory into Git, as it may contain sensitive credentials for your remote backend.\n\nThe local backend configuration is different and entirely separate from the terraform.tfstate file that contains state data about your real-world infrastruture. Terraform stores the terraform.tfstate file in your remote backend.\n\nWhen you change backends, Terraform gives you the option to migrate your state to the new backend. This lets you adopt backends without losing any existing state.\n\nImportant: Before migrating to a new backend, we strongly recommend manually backing up your state by copying your terraform.tfstate file to another location.\n\nPartial Configuration\n\nYou do not need to specify every required argument in the backend configuration. Omitting certain arguments may be desirable if some arguments are provided automatically by an automation script running Terraform. When some or all of the arguments are omitted, we call this a partial configuration.\n\nWith a partial configuration, the remaining configuration arguments must be provided as part of the initialization process.\n\nThere are several ways to supply the remaining arguments:\n\nFile: A configuration file may be specified via the init command line. To specify a file, use the -backend-config=PATH option when running terraform init. If the file contains secrets it may be kept in a secure data store, such as Vault, in which case it must be downloaded to the local disk before running Terraform.\n\nCommand-line key/value pairs: Key/value pairs can be specified via the init command line. Note that many shells retain command-line flags in a history file, so this isn't recommended for secrets. To specify a single key/value pair, use the -backend-config=\"KEY=VALUE\" option when running terraform init.\n\nInteractively: Terraform will interactively ask you for the required values, unless interactive input is disabled. Terraform will not prompt for optional values.\n\nIf backend settings are provided in multiple locations, the top-level settings are merged such that any command-line options override the settings in the main configuration and then the command-line options are processed in order, with later options overriding values set by earlier options.\n\nThe final, merged configuration is stored on disk in the .terraform directory, which should be ignored from version control. This means that sensitive information can be omitted from version control, but it will be present in plain text on local disk when running Terraform.\n\nWhen using partial configuration, Terraform requires at a minimum that an empty backend configuration is specified in one of the root Terraform configuration files, to specify the backend type. For example:\n\nterraform {\n\n  backend \"consul\" {}\n\n}\n\nCopy\nFile\n\nA backend configuration file has the contents of the backend block as top-level attributes, without the need to wrap it in another terraform or backend block:\n\naddress = \"demo.consul.io\"\n\npath    = \"example_app/terraform_state\"\n\nscheme  = \"https\"\n\nCopy\n\n*.backendname.tfbackend (e.g. config.consul.tfbackend) is the recommended naming pattern. Terraform will not prevent you from using other names but following this convention will help your editor understand the content and likely provide better editing experience as a result.\n\nCommand-line key/value pairs\n\nThe same settings can alternatively be specified on the command line as follows:\n\n$ terraform init \\\n\n    -backend-config=\"address=demo.consul.io\" \\\n\n    -backend-config=\"path=example_app/terraform_state\" \\\n\n    -backend-config=\"scheme=https\"\n\nCopy\n\nThe Consul backend also requires a Consul access token. Per the recommendation above of omitting credentials from the configuration and using other mechanisms, the Consul token would be provided by setting either the CONSUL_HTTP_TOKEN or CONSUL_HTTP_AUTH environment variables. See the documentation of your chosen backend to learn how to provide credentials to it outside of its main configuration.\n\nChanging Configuration\n\nYou can change your backend configuration at any time. You can change both the configuration itself as well as the type of backend (for example from \"consul\" to \"s3\").\n\nTerraform will automatically detect any changes in your configuration and request a reinitialization. As part of the reinitialization process, Terraform will ask if you'd like to migrate your existing state to the new configuration. This allows you to easily switch from one backend to another.\n\nIf you're using multiple workspaces, Terraform can copy all workspaces to the destination. If Terraform detects you have multiple workspaces, it will ask if this is what you want to do.\n\nIf you're just reconfiguring the same backend, Terraform will still ask if you want to migrate your state. You can respond \"no\" in this scenario.\n\nUnconfiguring a Backend\n\nIf you no longer want to use any backend, you can simply remove the configuration from the file. Terraform will detect this like any other change and prompt you to reinitialize.\n\nAs part of the reinitialization, Terraform will ask if you'd like to migrate your state back down to normal local state. Once this is complete then Terraform is back to behaving as it does by default.\n\nEdit this page on GitHub\n\nOn this page:\n\nBackend Configuration\nAvailable Backends\nUsing a Backend Block\nInitialization\nPartial Configuration\nChanging Configuration\nUnconfiguring a Backend\nTheme\nDark\nLight\nSystem\nCertifications\nSystem Status\nCookie Manager\nTerms of Use\nSecurity\nPrivacy\nTrademark Policy\nTrade Controls\nGive Feedback\n(opens in new tab)\nWe use cookies & other similar technology to collect data to improve your experience on our site, as described in our Privacy Policy and Cookie Policy.\nManage PreferencesACCEPT"
  },
  {
    "title": "Override Files - Configuration Language | Terraform | HashiCorp Developer",
    "url": "https://developer.hashicorp.com/terraform/language/files/override",
    "html": "Skip to main content\nNews\nHashiCorp to join IBM to accelerate multi-cloud automation.\nRead the blog →\nDismiss alert\nTerraform\nInstall\nTutorials\nDocumentation\nRegistry\n(opens in new tab)\nTry Cloud\n(opens in new tab)\nSearch\nCommand or control key\nK key\nTerraform Home\nConfiguration Language\nConfiguration Language\nStyle Guide\nFiles and Directories\nOverview\nOverride Files\nDependency Lock File\nTest Files\nSyntax\nResources\nData Sources\nProviders\nVariables and Outputs\nModules\nMoved block\nChecks\nImport\nExpressions\nFunctions\nTerraform Settings\nState\nTests\nUpgrading to Terraform v1.7\nv1.x Compatibility Promises\nTerraform Internals\nResources\nTutorial Library\nCertifications\nCommunity Forum\n(opens in new tab)\nSupport\n(opens in new tab)\nGitHub\n(opens in new tab)\nTerraform Registry\n(opens in new tab)\nDeveloper\nTerraform\nConfiguration Language\nFiles and Directories\nOverride Files\nv1.8.x (latest)\nOverride Files\n\nTerraform normally loads all of the .tf and .tf.json files within a directory and expects each one to define a distinct set of configuration objects. If two files attempt to define the same object, Terraform returns an error.\n\nIn some rare cases, it is convenient to be able to override specific portions of an existing configuration object in a separate file. For example, a human-edited configuration file in the Terraform language native syntax could be partially overridden using a programmatically-generated file in JSON syntax.\n\nFor these rare situations, Terraform has special handling of any configuration file whose name ends in _override.tf or _override.tf.json. This special handling also applies to a file named literally override.tf or override.tf.json.\n\nTerraform initially skips these override files when loading configuration, and then afterwards processes each one in turn (in lexicographical order). For each top-level block defined in an override file, Terraform attempts to find an already-defined object corresponding to that block and then merges the override block contents into the existing object.\n\nUse override files only in special circumstances. Over-use of override files hurts readability, since a reader looking only at the original files cannot easily see that some portions of those files have been overridden without consulting all of the override files that are present. When using override files, use comments in the original files to warn future readers about which override files apply changes to each block.\n\nExample\n\nIf you have a Terraform configuration example.tf with the following contents:\n\nresource \"aws_instance\" \"web\" {\n\n  instance_type = \"t2.micro\"\n\n  ami           = \"ami-408c7f28\"\n\n}\n\nCopy\n\n...and you created a file override.tf containing the following:\n\nresource \"aws_instance\" \"web\" {\n\n  ami = \"foo\"\n\n}\n\nCopy\n\nTerraform will merge the latter into the former, behaving as if the original configuration had been as follows:\n\nresource \"aws_instance\" \"web\" {\n\n  instance_type = \"t2.micro\"\n\n  ami           = \"foo\"\n\n}\n\nCopy\nMerging Behavior\n\nThe merging behavior is slightly different for each block type, and some special constructs within certain blocks are merged in a special way.\n\nThe general rule, which applies in most cases, is:\n\nA top-level block in an override file merges with a block in a normal configuration file that has the same block header. The block header is the block type and any quoted labels that follow it.\n\nWithin a top-level block, an attribute argument within an override block replaces any argument of the same name in the original block.\n\nWithin a top-level block, any nested blocks within an override block replace all blocks of the same type in the original block. Any block types that do not appear in the override block remain from the original block.\n\nThe contents of nested configuration blocks are not merged.\n\nThe resulting merged block must still comply with any validation rules that apply to the given block type.\n\nIf more than one override file defines the same top-level block, the overriding effect is compounded, with later blocks taking precedence over earlier blocks. Overrides are processed in order first by filename (in lexicographical order) and then by position in each file.\n\nThe following sections describe the special merging behaviors that apply to specific arguments within certain top-level block types.\n\nMerging resource and data blocks\n\nWithin a resource block, the contents of any lifecycle nested block are merged on an argument-by-argument basis. For example, if an override block sets only the create_before_destroy argument then any ignore_changes argument in the original block will be preserved.\n\nIf an overriding resource block contains one or more provisioner blocks then any provisioner blocks in the original block are ignored.\n\nIf an overriding resource block contains a connection block then it completely overrides any connection block present in the original block.\n\nThe depends_on meta-argument may not be used in override blocks, and will produce an error.\n\nMerging variable blocks\n\nThe arguments within a variable block are merged in the standard way described above, but some special considerations apply due to the interactions between the type and default arguments.\n\nIf the original block defines a default value and an override block changes the variable's type, Terraform attempts to convert the default value to the overridden type, producing an error if this conversion is not possible.\n\nConversely, if the original block defines a type and an override block changes the default, the overridden default value must be compatible with the original type specification.\n\nMerging output blocks\n\nThe depends_on meta-argument may not be used in override blocks, and will produce an error.\n\nMerging locals blocks\n\nEach locals block defines a number of named values. Overrides are applied on a value-by-value basis, ignoring which locals block they are defined in.\n\nMerging terraform blocks\n\nThe settings within terraform blocks are considered individually when merging.\n\nIf the required_providers argument is set, its value is merged on an element-by-element basis, which allows an override block to adjust the constraint for a single provider without affecting the constraints for other providers.\n\nIn both the required_version and required_providers settings, each override constraint entirely replaces the constraints for the same component in the original block. If both the base block and the override block both set required_version then the constraints in the base block are entirely ignored.\n\nThe presence of a block defining a backend (either cloud or backend) in an override file always takes precedence over a block defining a backend in the original configuration. That is, if a cloud block is set within the original configuration and a backend block is set in the override file, Terraform will use the backend block specified in the override file upon merging. Similarly, if a backend block is set within the original configuration and a cloud block is set in the override file, Terraform will use the cloud block specified in the override file upon merging.\n\nEdit this page on GitHub\n\nOn this page:\n\nOverride Files\nExample\nMerging Behavior\nTheme\nDark\nLight\nSystem\nCertifications\nSystem Status\nCookie Manager\nTerms of Use\nSecurity\nPrivacy\nTrademark Policy\nTrade Controls\nGive Feedback\n(opens in new tab)\nWe use cookies & other similar technology to collect data to improve your experience on our site, as described in our Privacy Policy and Cookie Policy.\nManage PreferencesACCEPT"
  },
  {
    "title": "HashiCorp Developer",
    "url": "https://developer.hashicorp.com/terraform/language/v1.1.x/style",
    "html": "News\nHashiCorp to join IBM to accelerate multi-cloud automation.\nRead the blog →\nDismiss alert\nProducts\nTutorials\nSearch\nCommand or control key\nK key\nThis page does not exist for version v1.1.x.\n\nPlease select either the most recent version or a valid version that includes the page you are looking for.\n\nGo back to Documentation\nTheme\nCertifications\nSystem Status\nCookie Manager\nTerms of Use\nSecurity\nPrivacy\nTrademark Policy\nTrade Controls\nGive Feedback\n(opens in new tab)"
  },
  {
    "title": "Style Guide - Configuration Language | Terraform | HashiCorp Developer",
    "url": "https://developer.hashicorp.com/terraform/language/v1.3.x/style",
    "html": "Skip to main content\nNews\nHashiCorp to join IBM to accelerate multi-cloud automation.\nRead the blog →\nDismiss alert\nTerraform\nInstall\nTutorials\nDocumentation\nRegistry\n(opens in new tab)\nTry Cloud\n(opens in new tab)\nSearch\nCommand or control key\nK key\nTerraform Home\nConfiguration Language\nConfiguration Language\nTerraform Language\nOverview\nStyle Guide\nFiles and Directories\nSyntax\nResources\nData Sources\nProviders\nVariables and Outputs\nModules\nMoved block\nExpressions\nFunctions\nTerraform Settings\nState\nUpgrading to Terraform v1.3\nv1.x Compatibility Promises\nTerraform Internals\nResources\nTutorial Library\nCertifications\nCommunity Forum\n(opens in new tab)\nSupport\n(opens in new tab)\nGitHub\n(opens in new tab)\nTerraform Registry\n(opens in new tab)\n\nYou are viewing documentation for version v1.3.x. View latest version.\n\nDeveloper\nTerraform\nConfiguration Language v1.3.x\nStyle Guide\nv1.3.x\nStyle Guide\n\nThe flexibility of Terraform's configuration language gives you many options to choose from as you write your code, structure your directories, and test your configuration. While some design decisions depend on your organization's needs or preferences, there are some common patterns that we suggest you adopt. Adopting and adhering to a style guide keeps your Terraform code legible, scalable, and maintainable.\n\nThis article discusses best practices and some considerations to keep in mind as you develop your organization's style guide. The article is split into two sections. The first section covers code style recommendations, such as formatting and resource organization. The second section covers operations and workflow recommendations, such as lifecycle management through meta-arguments, versioning, and sensitive data management.\n\nCode style\n\nWriting Terraform code in a consistent style makes it easier to read and maintain. The following sections discuss code style recommendations, including the following:\n\nRun terraform fmt and terraform validate before committing your code to version control.\nUse a linter such as TFLint to enforce your organization's own coding best practices.\nUse # for single and multi-line comments.\nUse nouns for resource names and do not include the resource type in the name.\nUse underscores to separate multiple words in names. Wrap the resource type and name in double quotes in your resource definition.\nLet your code build on itself: define dependent resources after the resources that reference them.\nInclude a type and description for every variable.\nInclude a description for every output.\nAvoid overuse of variables and local values.\nAlways include a default provider configuration.\nUse count and for_each sparingly.\nCode formatting\n\nThe Terraform parser allows you some flexibility in how you lay out the elements in your configuration files, but the Terraform language also has some idiomatic style conventions which we recommend users always follow for consistency between files and modules written by different teams.\n\nIndent two spaces for each nesting level\n\nWhen multiple arguments with single-line values appear on consecutive lines at the same nesting level, align their equals signs:\n\nami           = \"abc123\"\n\ninstance_type = \"t2.micro\"\n\n\nWhen both arguments and blocks appear together inside a block body, place all of the arguments together at the top and then place nested blocks below them. Use one blank line to separate the arguments from the blocks.\n\nUse empty lines to separate logical groups of arguments within a block.\n\nFor blocks that contain both arguments and \"meta-arguments\" (as defined by the Terraform language semantics), list meta-arguments first and separate them from other arguments with one blank line. Place meta-argument blocks last and separate them from other blocks with one blank line. Refer to dynamic resource count for more information on meta-arguments.\n\nresource \"aws_instance\" \"example\" {\n\n  # meta-argument first\n\n  count = 2\n\n\n\n  ami           = \"abc123\"\n\n  instance_type = \"t2.micro\"\n\n\n\n  network_interface {\n\n    # ...\n\n  }\n\n\n\n  # meta-argument block last\n\n  lifecycle {\n\n    create_before_destroy = true\n\n  }\n\n}\n\n\nTop-level blocks should always be separated from one another by one blank line. Nested blocks should also be separated by blank lines, except when grouping together related blocks of the same type (like multiple provisioner blocks in a resource).\n\nAvoid grouping multiple blocks of the same type with other blocks of a different type, unless the block types are defined by semantics to form a family. (For example: root_block_device, ebs_block_device and ephemeral_block_device on aws_instance form a family of block types describing AWS block devices, and can therefore be grouped together and mixed.)\n\nThe terraform fmt command formats your Terraform configuration to a subset of the above recommendations. By default, the terraform fmt command will only modify your Terraform code in the directory that you execute it in, but you can include the -recursive flag to modify code in all subdirectories as well.\n\nWe recommend that you run terraform fmt before each commit to version control. You can use mechanisms such as Git pre-commit hooks to automatically run this command each time you commit your code.\n\nIf you use Microsoft VS Code, use the Terraform VS Code extension to enable features such as syntax highlighting and validation, automatic code formatting, and integration with Terraform Cloud. If your development environment or text editor supports the Language Server Protocol, you can use the Terraform Language Server to access most of the VS Code extension features.\n\nCode validation\n\nThe terraform validate command checks that your configuration is syntactically valid and internally consistent. The validate command does not check if argument values are valid for a specific provider, but it will verify that they are the correct type. It does not evaluate any existing state.\n\nThe terraform validate command is safe to run automatically and frequently. You can configure your text editor to run this command as a post-save check, define it as a pre-commit hook in a Git repository, or run it as a step in a CI/CD pipeline.\n\nFor more information, refer to the Terraform validate documentation.\n\nFile names\n\nWe recommend the following file naming conventions:\n\nA backend.tf file that contains your backend configuration. You can define multiple terraform blocks in your configuration to separate your backend configuration from your Terraform and provider versioning configuration.\nA main.tf file that contains all resource and data source blocks.\nA outputs.tf file that contains all output blocks in alphabetical order.\nA providers.tf file that contains all provider blocks and configuration.\nA terraform.tf file that contains a single terraform block which defines your required_version and required_providers.\nA variables.tf file that contains all variable blocks in alphabetical order.\nA locals.tf file that contains local values. Refer to local values for more information.\nA override.tf file that contains override definitions for your configuration. Terraform loads this and all files ending with _override.tf last. Use them sparingly and add comments to the original resource definitions, as these overrides make your code harder to reason about. Refer to the override files documentation for more information.\n\nAs your codebase grows, limiting it to just these files can become difficult to maintain. If your code becomes hard to navigate due to its size, we recommend that you organize resources and data sources in separate files by logical groups. For example, if your web application requires networking, storage, and compute resources, you might create the following files:\n\nA network.tf file that contains your VPC, subnets, load balancers, and all other networking resources.\nA storage.tf file that contains your object storage and related permissions configuration.\nA compute.tf file that contains your compute instances.\n\nNo matter how you decide to split your code, it should be immediately clear where a maintainer can find a specific resource or data source definition.\n\nAs your configuration grows, you may need to separate it into multiple state files. The HashiCorp Well-Architected Framework provides more guidance about configuration structure and scope.\n\nLinting and static code analysis\n\nTerraform does not have a built-in linter, but many organizations rely on a third party linting tool such as TFLint to enforce code standards. A linter uses static code analysis to compare your Terraform code against a set of rules. Most linters ship with a default set of rules, but also let you write your own.\n\nComments\n\nWrite your code so it is easy to understand. Only when necessary, use comments to clarify complexity for other maintainers.\n\nUse # for both single- and multi-line comments. The // and /* */ comment syntaxes are not considered idiomatic, but Terraform supports them to remain backwards-compatible with earlier versions of HCL.\n\n# Each tunnel is responsible for encrypting and decrypting traffic exiting\n\n# and leaving its associated gateway.\n\nresource \"google_compute_vpn_tunnel\" \"tunnel1\" {\n\n  ## ...\n\nResource naming\n\nEvery resource within a configuration must have a unique name. For consistency and readability, use a descriptive noun and separate words with underscores. Do not include the resource type in the resource identifier since the resource address already includes it. Wrap the resource type and name in double quotes.\n\n❌ Bad:\n\nresource aws_instance webAPI-aws-instance {...}\n\n\n✅ Good:\n\nresource \"aws_instance\" \"web_api\" {...}\n\nResource order\n\nThe order of the resources and data sources in your code does not affect how Terraform builds them, so organize your resources for readability. Terraform determines the creation order based on cross-resource dependencies.\n\nHow you order your resources largely depends on the size and complexity of your code, but we recommend defining data sources alongside the resources that reference them. For readability, your Terraform code should “build on itself” — you should define a data source before the resource that references it.\n\nThe following example defines an aws_instance that relies on two data sources, aws_ami and aws_availability_zone. For readability and continuity, it defines the data sources before the aws_instance resource.\n\ndata \"aws_ami\" \"web\" {\n\n  ##...\n\n}\n\n\n\ndata \"aws_availability_zones\" \"available\" {\n\n  ##...\n\n}\n\n\n\nresource \"aws_instance\" \"web\" {\n\n  ami               = data.aws_ami.web.id\n\n  availability_zone = data.aws_availability_zones.available.names[0]\n\n  ##...\n\n}\n\n\nWe recommend following a consistent order for resource parameters:\n\nIf present, The count or for_each meta-argument.\nResource-specific non-block parameters.\nResource-specific block parameters.\nIf required, a lifecycle block.\nIf required, the depends_on parameter.\nVariables\n\nWhile variables make your modules more flexible, overusing variables can make code difficult to understand. When deciding whether to expose a variable for a resource setting, consider whether that parameter will change between deployments.\n\nDefine a type and a description for every variable.\n\nIf the variable is optional, define a reasonable default.\n\nFor sensitive variables, such as passwords and private keys, set the sensitive parameter to true. Remember that Terraform will still store this value in plain text in its state, but it will not display it when you run terraform plan or terraform apply. Refer to secrets management for more information on how to securely handle sensitive values.\n\nUse input variable validation to create additional rules for your variable values in addition to Terraform's type validation. Only use variable validation when your variable values have uniquely restrictive requirements. For example, if your Terraform configuration requires two web instances, add a validation block to enforce it:\n\nvariable \"web_instance_count\" {\n\n  type        = number\n\n  description = \"Number of web instances to deploy. This application requires at least two instances.\"\n\n\n\n  validation {\n\n    condition     = var.web_instance_count > 1\n\n    error_message = \"This application requires at least two web instances.\"\n\n  }\n\n}\n\n\nWe recommend following a consistent order for variable parameters:\n\nType\nDescription\nDefault (optional)\nSensitive (optional)\nValidation blocks\nOutputs\n\nOutput values let you expose data about your infrastructure on the command line and make it easy to reference in other Terraform configurations. Like you would for variables, provide a description for each output.\n\nWe recommend that you use the following order for your output parameters:\n\nDescription\nValue\nSensitive (optional)\n\nEvery variable and output requires a unique name. For consistency and readability, we recommend that you use a descriptive noun and separate words with underscores.\n\nvariable \"db_disk_size\" {\n\n type        = number\n\n description = \"Disk size for the API database\"\n\n default     = 100\n\n}\n\n\n\nvariable \"db_password\" {\n\n type        = string\n\n description = \"Database password\"\n\n sensitive   = true\n\n}\n\n\n\noutput \"web_public_ip\" {\n\n description = \"Public IP of the web instance\"\n\n value       = aws_instance.web.public_ip\n\n}\n\nLocal values\n\nLocal values let you reference an expression or value multiple times. Use local values sparingly, as overuse can make your code harder to understand.\n\nFor example, you can use a local value to create a suffix for the region and environment (for example, development or test), and append it to multiple resources.\n\nlocals {\n\n  name_suffix = \"${var.region}-${var.environment}\"\n\n}\n\n\n\nresource \"aws_instance\" \"web\" {\n\n  ami           = data.aws_ami.ubuntu.id\n\n  instance_type = \"t3.micro\"\n\n\n\n  tags = {\n\n    Name = \"web-${local.name_suffix}\"\n\n  }\n\n}\n\n\nDefine local values in one of two places:\n\nIf you reference the local value in multiple files, define it in a file named locals.tf.\nIf the local is specific to a file, define it at the top of that file.\n\nAs for other Terraform objects, use descriptive nouns for local value names and underscores to separate multiple words.\n\nFor more information, refer to the local values documentation and the Simplify Terraform configuration with locals tutorial.\n\nProvider aliasing\n\nProvider aliasing lets you define multiple provider blocks for the same Terraform provider. Potential use cases for aliases include provisioning resources in multiple regions within a single configuration. The provider meta-argument for resources and the providers meta-argument for modules specifies which provider to use.\n\nproviders.tf\nprovider \"aws\" {\n\n  region = \"us-east-1\"\n\n}\n\n\n\nprovider \"aws\" {\n\n  alias  = \"west\"\n\n  region = \"us-west-2\"\n\n}\n\nmain.tf\nresource \"aws_instance\" \"example\" {\n\n  provider = aws.west\n\n  # ...\n\n}\n\n\n\nmodule \"aws_vpc\" {\n\n  source = \"./aws_vpc\"\n\n  providers = {\n\n    aws = aws.west\n\n  }\n\n}\n\nAny provider block that does not define the alias parameter is the default provider configuration.\nAlways include a default provider configuration and define all of your providers in the same file.\nIf you define multiple instances of a provider, define the default first.\nFor non-default providers, define the alias as the first parameter of the provider block.\nDynamic resource count\n\nThe for_each and count meta-arguments let you create multiple resources from a single resource block depending on run-time conditions. You can use these meta-arguments to make your code flexible and reduce duplicate resource blocks. If the resources are almost identical, use count. If some of arguments need distinct values that you cannot derive from an integer, use for_each.\n\nThe for_each meta-argument accepts a map or set value, and Terraform will create an instance of that resource for each element in the value you provide. In the following example, Terraform creates an aws_instance for each of the strings defined in the web_instances variable: \"ui\", \"api\", \"db\" and \"metrics\". The example uses each.key to give each instance a unique name. The web_private_ips output uses a for expression to create a map of instance names and their private IP addresses, while the web_ui_public_ip output addresses the instance with the key \"ui\" directly.\n\nvariable \"web_instances\" {\n\n  type        = list(string)\n\n  description = \"A list of instances for the web application\"\n\n  default = [\n\n    \"ui\",\n\n    \"api\",\n\n    \"db\",\n\n    \"metrics\"\n\n  ]\n\n}\n\nresource \"aws_instance\" \"web\" {\n\n  for_each = toset(var.web_instances)\n\n  ami           = data.aws_ami.webapp.id\n\n  instance_type = \"t3.micro\"\n\n  tags = {\n\n    Name = \"web_${each.key}\"\n\n  }\n\n}\n\noutput \"web_private_ips\" {\n\n  description = \"Private IPs of the web instances\"\n\n  value = {\n\n    for k, v in aws_instance.web : k => v.private_ip\n\n  }\n\n}\n\noutput \"web_ui_public_ip\" {\n\n  description = \"Public IP of the web UI instance\"\n\n  value       = aws_instance.web[\"ui\"].public_ip\n\n}\n\n\nThe above example will create the following output:\n\nweb_private_ips = {\n\n  \"api\" = \"172.31.25.29\"\n\n  \"db\" = \"172.31.18.33\"\n\n  \"metrics\" = \"172.31.26.112\"\n\n  \"ui\" = \"172.31.20.142\"\n\n}\n\nweb_ui_public_ip = \"18.216.208.182\"\n\n\nRefer to the for_each meta-argument documentation for more examples.\n\nThe count meta-argument lets you create multiple instances of a resource from a single resource block. Refer to the count meta-argument documentation for examples.\n\nA common practice to conditionally create resources is to use the count meta-argument with a conditional expression. In the following example, Terraform will only create the aws_instance if var.enable_metrics is true.\n\nvariable \"enable_metrics\" {\n\n  description = \"True if the metrics server should be deployed\"\n\n  type        = bool\n\n  default     = true\n\n}\n\n\n\nresource \"aws_instance\" \"web\" {\n\n  count = var.enable_metrics ? 1 : 0\n\n\n\n  ami           = data.aws_ami.webapp.id\n\n  instance_type = \"t3.micro\"\n\n  ##...\n\n}\n\n\nMeta-arguments simplify your code but add complexity, so use them in moderation. If the effect of the meta-argument is not immediately obvious, use a comment for clarification.\n\nTo learn more about these meta-arguments, refer to the for_each and count documentation.\n\n.gitignore\n\nDefine a .gitignore file for your repository to exclude files that you should not publish to version control, such as your state file.\n\nDo not commit:\n\nYour terraform.tfstate state file, including terraform.tfstate.* backup state files.\nYour .terraform.tfstate.lock.info file. Terraform creates and deletes this file automatically when you run a terraform apply command and contains info about your state lock\nYour .terraform directory, where Terraform downloads providers and child modules. Saved plan files that you create when you include the -out flat when you run terraform plan.\nAny .tfvars files that contain sensitive information.\n\nAlways commit:\n\nAll Terraform code files\nYour .terraform.lock.hcl dependency lock file\nA .gitignore file that excludes the files listed below\nA README.md to describe the code, input variables, and outputs\n\nFor an example, refer to GitHub's Terraform .gitignore file.\n\nWorkflow style\n\nThis section reviews standards that enable predictable and secure Terraform workflows, such as:\n\nPin your Terraform, provider, and module versions.\nName your module repositories using this three-part name terraform-<PROVIDER>-<NAME> when using the Terraform Cloud registry.\nStore local modules at ./modules/<module_name>.\nUse the tfe_outputs data source or provider-specific data sources to share state between two state files.\nProtect credentials by using dynamic provider credentials or a secrets manager such as HashiCorp Vault.\nWrite tests for your modules.\nUse policy enforcement on Terraform Cloud to set guardrails for infrastructure operations.\nVersion pinning\n\nTo prevent providers and modules upgrades from introducing unintentional changes to your infrastructure, use version pinning.\n\nSpecify provider versions using the required_providers block. Terraform version constraints support a range of accepted versions.\n\nPin modules to a specific major and minor version as shown in the example below to ensure stability. You can use looser restrictions if you are certain that the module does not introduce breaking changes outside of major version updates.\n\nWe also recommend that you set a minimum required version of the Terraform binary using the required_version in your terraform block. This requires all operators to use a Terraform version that has all of your configuration's required features.\n\nterraform {\n\n  required_providers {\n\n    aws = {\n\n      source  = \"hashicorp/aws\"\n\n      version = \"5.34.0\"\n\n    }\n\n  }\n\n\n\n  required_version = \">= 1.7\"\n\n}\n\n\nThe above example pins the version of the hashicorp/aws provider to version 5.34.0, and requires that operators use Terraform 1.7 or newer.\n\nFor modules sourced from a registry, use the version parameter in the module block to pin the version. For local modules, Terraform ignores the version parameter.\n\nmodule \"vault_starter\" {\n\n  source  = \"hashicorp/vault-starter/aws\"\n\n  version = \"1.0.0\"\n\n  ##...\n\n}\n\nModule repository names\n\nThe Terraform registry requires that repositories match a naming convention for all modules that you publish to the registry. Module repositories must use this three-part name terraform-<PROVIDER>-<NAME>, where <NAME> reflects the type of infrastructure the module manages and <PROVIDER> is the main provider the module uses. The <NAME> segment can contain additional hyphens, for example, terraform-google-vault or terraform-aws-ec2-instance.\n\nModule structure\n\nTerraform modules define self-contained, reusable pieces of infrastructure-as-code.\n\nUse modules to group together logically related resources that you need to provision together. For example:\n\nA networking module that defines a VPC, along with its subnets, gateway, and security groups.\nAn application module defining all resources required for each deployment. This stack could include web servers, databases, storage, and supported networking.\n\nReview the module creation recommended pattern documentation and standard module structure for guidance on how to structure your modules.\n\nLocal modules\n\nLocal modules are sourced from local disk rather than a remote module registry. We recommend publishing your modules to a module registry, such as the Terraform Cloud private module registry, to easily version, share, and reuse modules across your organization. If you cannot use a module registry, using local modules can simplify maintaining and updating your code.\n\nWe recommend that you define child modules in the ./modules/<module_name> directory.\n\nRepository structure\n\nHow you structure your modules and Terraform configuration in version control significantly impacts versioning and operations. We recommend that you store your actual infrastructure configuration separately from your module code.\n\nStore each module in an individual repository. This lets you independently version each module and makes it easier to publish your modules in the private Terraform registry.\n\nOrganize your infrastructure configuration in repositories that group together logically-related resources. For example, a single repository for a web application that requires compute, networking, and database resources . By separating your resources into groups, you limit the number of resources that may be impacted by failures for any operation.\n\nAnother approach is to group all modules and infrastructure configuration into a single monolithic repository, or monorepo. For example, a monorepo may define a collection of local modules for each component of the infrastructure stack, and deploy them in the root module.\n\n.\n\n├── modules\n\n│   ├── function\n\n│   │   ├── main.tf      # contains aws_iam_role, aws_lambda_function\n\n│   │   ├── outputs.tf\n\n│   │   └── variables.tf\n\n│   ├── queue\n\n│   │   ├── main.tf      # contains aws_sqs_queue\n\n│   │   ├── outputs.tf\n\n│   │   └── variables.tf\n\n│   └── vpc\n\n│       ├── main.tf      # contains aws_vpc, aws_subnet\n\n│       ├── outputs.tf\n\n│       └── variables.tf\n\n├── main.tf\n\n├── outputs.tf\n\n└── variables.tf\n\n\nThe advantage of monolithic repositories is having a single source of truth that tracks every infrastructure change. However, monolithic repositories can complicate your CI/CD automation: since any code change triggers a deployment that operates on your entire repository, your workflow must target only the modified directories. You also lose the granular access control, since anyone with repository access can modify any file in it.\n\nIf your organization requires a monolithic approach, Terraform Cloud and Terraform Enterprise let you scope a workspace to a specific directory in a repository, simplifying your workflows.\n\nBranching strategy\n\nTo collaborate on your Terraform code, we recommend using the GitHub flow. This approach uses short-lived branches to help your team quickly review, test, and merge changes to your code. To make changes to your code, you would:\n\nCreate a new branch from your main branch\nWrite, commit, and push your changes to the new branch\nCreate a pull request\nReview the changes with your team\nMerge the pull request\nDelete the branch\n\nTerraform Cloud and Terraform Enterprise can run speculative plans for pull requests. These speculative plans run automatically when you create or update a pull request, and you can use them to see the effect that your changes will have on your infrastructure before you merge them to your main branch. When you merge your pull request, Terraform Cloud will start a new run to apply these changes.\n\nMultiple environments\n\nWe recommend that your repository's main branch be the source of truth for all environments. For Terraform Cloud and Terraform Enterprise users, we recommend that you use separate workspaces for each environment. For larger codebases, we recommend that you split your resources across multiple workspaces to prevent large state files and limit unintended consequences from changes. For example, you could structure your code as follows:\n\n.\n\n├── compute\n\n│   ├── main.tf\n\n│   ├── outputs.tf\n\n│   └── variables.tf\n\n├── database\n\n│   ├── main.tf\n\n│   ├── outputs.tf\n\n│   └── variables.tf\n\n└── networking\n\n    ├── main.tf\n\n    ├── outputs.tf\n\n    └── variables.tf\n\n\nIn this scenario, you would create three workspaces per environment. For example, your production environment would have a prod-compute, prod-database, and prod-networking workspace. Read more about Terraform workspace and project best practices.\n\nIf you do not use Terraform Cloud or Terraform Enterprise, we recommend that you use modules to encapsulate your configuration, and use a directory for each environment so that each one has a separate state file. The configuration in each of these directories would call the local modules, each with parameters specific to their environment. This also lets you maintain separate variable and backend configurations for each environment.\n\n├── modules\n\n│   ├── compute\n\n│   │   └── main.tf\n\n│   ├── database\n\n│   │   └── main.tf\n\n│   └── network\n\n│       └── main.tf\n\n├── dev\n\n│   ├── backend.tf\n\n│   ├── main.tf\n\n│   └── variables.tf\n\n├── prod\n\n│   ├── backend.tf\n\n│   ├── main.tf\n\n│   └── variables.tf\n\n└── staging\n\n    ├── backend.tf\n\n    ├── main.tf\n\n    └── variables.tf\n\nState sharing\n\nSince your state contains sensitive information, avoid sharing full state files when possible.\n\nIf you use Terraform Cloud or Terraform Enterprise and need to reference resources across workspaces, use the tfe_outputs data source.\n\nIf you do not use Terraform Cloud or Terraform Enterprise but still need to reference data about other infrastructure resources, use data sources to query the provider. For example, you can use the aws_instance data source to look up an AWS EC2 instance by its ID or tags.\n\nSecrets management\n\nIf you do not configure remote state storage, the Terraform CLI stores the entire state in plaintext on the local disk. State can include sensitive data, such as passwords and private keys. Terraform Cloud and Terraform Enterprise provide state encryption through HashiCorp Vault.\n\nIf you use Terraform Cloud or Terraform Enterprise, we recommend the following:\n\nWhen using Terraform Enterprise, define and enforce a Sentinel policy to prevent use of the local_exec provisioner or external data sources.\nWhen using Terraform Cloud or Terraform Enterprise, use dynamic provider credentials to avoid using long-lived static credentials.\n\nIf you use Terraform Community Edition, we recommend the following:\n\nConfigure provider credentials using provider-specific environment variables.\nAccess secrets from a secrets management system such as HashiCorp Vault with the Terraform Vault provider. Be aware that Terraform will still write these values in plaintext to your state file.\n\nIf you use a custom CI/CD pipeline, review your CI/CD tool's best practices for managing sensitive values. Most tools let you access sensitive values as environment variables. For more information, refer to your CI/CD documentation.\n\nUsing secrets in GitHub Actions\nGitlab pipeline security\nIntegrate Vault into your CI/CD pipeline\nIntegration and unit testing\n\nTerraform tests let you validate your modules and catch breaking changes. We recommend that you write tests for your Terraform modules and run them just as you run your tests for your application code, such as pre-merge check in your pull requests or as a prerequisite step in your automated CI/CD pipeline.\n\nTests differ from validation methods such as variable validation, preconditions, postconditions, and check blocks. These features focus on verifying the infrastructure deployed by your code, while tests validate the behavior and logic of your code itself. For more information, refer to the Terraform test documentation and the Write Terraform tests tutorial.\n\nPolicy\n\nPolicies are rules that Terraform Cloud enforces on Terraform runs. You can use policies to validate that the Terraform plan complies with your organization's best practices. For example, you can write policies that:\n\nLimit the size of a web instance\nCheck for required resource tags\nBlock deployments on Fridays\nEnforce security configuration and cost management\n\nWe recommend that you store policies in a separate VCS repository from your Terraform code.\n\nFor more information, refer to the policy enforcement documentation, as well as the enforce policy with Sential and detect infrastructure drift and enforce OPA policies tutorials.\n\nNext steps\n\nThis article introduces some considerations to keep in mind as you standardize your organization's Terraform style guidelines. Enforcing a standard way of writing and organizing your Terraform code across your organization ensures that it is readable, maintainable, and shareable.\n\nThe HashiCorp Well-Architected Framework provides more guidance on adapting your Terraform workflows for scale.\n\nOn this page:\n\nStyle Guide\nCode style\nCode formatting\nCode validation\nFile names\nLinting and static code analysis\nComments\nResource naming\nResource order\nVariables\nOutputs\nLocal values\nProvider aliasing\nDynamic resource count\n.gitignore\nWorkflow style\nVersion pinning\nModule repository names\nModule structure\nLocal modules\nRepository structure\nBranching strategy\nMultiple environments\nState sharing\nSecrets management\nIntegration and unit testing\nPolicy\nNext steps\nTheme\nDark\nLight\nSystem\nCertifications\nSystem Status\nCookie Manager\nTerms of Use\nSecurity\nPrivacy\nTrademark Policy\nTrade Controls\nGive Feedback\n(opens in new tab)\nWe use cookies & other similar technology to collect data to improve your experience on our site, as described in our Privacy Policy and Cookie Policy.\nManage PreferencesACCEPT"
  },
  {
    "title": "Style Guide - Configuration Language | Terraform | HashiCorp Developer",
    "url": "https://developer.hashicorp.com/terraform/language/v1.4.x/style",
    "html": "Skip to main content\nNews\nHashiCorp to join IBM to accelerate multi-cloud automation.\nRead the blog →\nDismiss alert\nTerraform\nInstall\nTutorials\nDocumentation\nRegistry\n(opens in new tab)\nTry Cloud\n(opens in new tab)\nSearch\nCommand or control key\nK key\nTerraform Home\nConfiguration Language\nConfiguration Language\nTerraform Language\nOverview\nStyle Guide\nFiles and Directories\nSyntax\nResources\nData Sources\nProviders\nVariables and Outputs\nModules\nExpressions\nFunctions\nTerraform Settings\nState\nUpgrading to Terraform v1.4\nv1.x Compatibility Promises\nTerraform Internals\nResources\nTutorial Library\nCertifications\nCommunity Forum\n(opens in new tab)\nSupport\n(opens in new tab)\nGitHub\n(opens in new tab)\nTerraform Registry\n(opens in new tab)\n\nYou are viewing documentation for version v1.4.x. View latest version.\n\nDeveloper\nTerraform\nConfiguration Language v1.4.x\nStyle Guide\nv1.4.x\nStyle Guide\n\nThe flexibility of Terraform's configuration language gives you many options to choose from as you write your code, structure your directories, and test your configuration. While some design decisions depend on your organization's needs or preferences, there are some common patterns that we suggest you adopt. Adopting and adhering to a style guide keeps your Terraform code legible, scalable, and maintainable.\n\nThis article discusses best practices and some considerations to keep in mind as you develop your organization's style guide. The article is split into two sections. The first section covers code style recommendations, such as formatting and resource organization. The second section covers operations and workflow recommendations, such as lifecycle management through meta-arguments, versioning, and sensitive data management.\n\nCode style\n\nWriting Terraform code in a consistent style makes it easier to read and maintain. The following sections discuss code style recommendations, including the following:\n\nRun terraform fmt and terraform validate before committing your code to version control.\nUse a linter such as TFLint to enforce your organization's own coding best practices.\nUse # for single and multi-line comments.\nUse nouns for resource names and do not include the resource type in the name.\nUse underscores to separate multiple words in names. Wrap the resource type and name in double quotes in your resource definition.\nLet your code build on itself: define dependent resources after the resources that reference them.\nInclude a type and description for every variable.\nInclude a description for every output.\nAvoid overuse of variables and local values.\nAlways include a default provider configuration.\nUse count and for_each sparingly.\nCode formatting\n\nThe Terraform parser allows you some flexibility in how you lay out the elements in your configuration files, but the Terraform language also has some idiomatic style conventions which we recommend users always follow for consistency between files and modules written by different teams.\n\nIndent two spaces for each nesting level\n\nWhen multiple arguments with single-line values appear on consecutive lines at the same nesting level, align their equals signs:\n\nami           = \"abc123\"\n\ninstance_type = \"t2.micro\"\n\n\nWhen both arguments and blocks appear together inside a block body, place all of the arguments together at the top and then place nested blocks below them. Use one blank line to separate the arguments from the blocks.\n\nUse empty lines to separate logical groups of arguments within a block.\n\nFor blocks that contain both arguments and \"meta-arguments\" (as defined by the Terraform language semantics), list meta-arguments first and separate them from other arguments with one blank line. Place meta-argument blocks last and separate them from other blocks with one blank line. Refer to dynamic resource count for more information on meta-arguments.\n\nresource \"aws_instance\" \"example\" {\n\n  # meta-argument first\n\n  count = 2\n\n\n\n  ami           = \"abc123\"\n\n  instance_type = \"t2.micro\"\n\n\n\n  network_interface {\n\n    # ...\n\n  }\n\n\n\n  # meta-argument block last\n\n  lifecycle {\n\n    create_before_destroy = true\n\n  }\n\n}\n\n\nTop-level blocks should always be separated from one another by one blank line. Nested blocks should also be separated by blank lines, except when grouping together related blocks of the same type (like multiple provisioner blocks in a resource).\n\nAvoid grouping multiple blocks of the same type with other blocks of a different type, unless the block types are defined by semantics to form a family. (For example: root_block_device, ebs_block_device and ephemeral_block_device on aws_instance form a family of block types describing AWS block devices, and can therefore be grouped together and mixed.)\n\nThe terraform fmt command formats your Terraform configuration to a subset of the above recommendations. By default, the terraform fmt command will only modify your Terraform code in the directory that you execute it in, but you can include the -recursive flag to modify code in all subdirectories as well.\n\nWe recommend that you run terraform fmt before each commit to version control. You can use mechanisms such as Git pre-commit hooks to automatically run this command each time you commit your code.\n\nIf you use Microsoft VS Code, use the Terraform VS Code extension to enable features such as syntax highlighting and validation, automatic code formatting, and integration with Terraform Cloud. If your development environment or text editor supports the Language Server Protocol, you can use the Terraform Language Server to access most of the VS Code extension features.\n\nCode validation\n\nThe terraform validate command checks that your configuration is syntactically valid and internally consistent. The validate command does not check if argument values are valid for a specific provider, but it will verify that they are the correct type. It does not evaluate any existing state.\n\nThe terraform validate command is safe to run automatically and frequently. You can configure your text editor to run this command as a post-save check, define it as a pre-commit hook in a Git repository, or run it as a step in a CI/CD pipeline.\n\nFor more information, refer to the Terraform validate documentation.\n\nFile names\n\nWe recommend the following file naming conventions:\n\nA backend.tf file that contains your backend configuration. You can define multiple terraform blocks in your configuration to separate your backend configuration from your Terraform and provider versioning configuration.\nA main.tf file that contains all resource and data source blocks.\nA outputs.tf file that contains all output blocks in alphabetical order.\nA providers.tf file that contains all provider blocks and configuration.\nA terraform.tf file that contains a single terraform block which defines your required_version and required_providers.\nA variables.tf file that contains all variable blocks in alphabetical order.\nA locals.tf file that contains local values. Refer to local values for more information.\nA override.tf file that contains override definitions for your configuration. Terraform loads this and all files ending with _override.tf last. Use them sparingly and add comments to the original resource definitions, as these overrides make your code harder to reason about. Refer to the override files documentation for more information.\n\nAs your codebase grows, limiting it to just these files can become difficult to maintain. If your code becomes hard to navigate due to its size, we recommend that you organize resources and data sources in separate files by logical groups. For example, if your web application requires networking, storage, and compute resources, you might create the following files:\n\nA network.tf file that contains your VPC, subnets, load balancers, and all other networking resources.\nA storage.tf file that contains your object storage and related permissions configuration.\nA compute.tf file that contains your compute instances.\n\nNo matter how you decide to split your code, it should be immediately clear where a maintainer can find a specific resource or data source definition.\n\nAs your configuration grows, you may need to separate it into multiple state files. The HashiCorp Well-Architected Framework provides more guidance about configuration structure and scope.\n\nLinting and static code analysis\n\nTerraform does not have a built-in linter, but many organizations rely on a third party linting tool such as TFLint to enforce code standards. A linter uses static code analysis to compare your Terraform code against a set of rules. Most linters ship with a default set of rules, but also let you write your own.\n\nComments\n\nWrite your code so it is easy to understand. Only when necessary, use comments to clarify complexity for other maintainers.\n\nUse # for both single- and multi-line comments. The // and /* */ comment syntaxes are not considered idiomatic, but Terraform supports them to remain backwards-compatible with earlier versions of HCL.\n\n# Each tunnel is responsible for encrypting and decrypting traffic exiting\n\n# and leaving its associated gateway.\n\nresource \"google_compute_vpn_tunnel\" \"tunnel1\" {\n\n  ## ...\n\nResource naming\n\nEvery resource within a configuration must have a unique name. For consistency and readability, use a descriptive noun and separate words with underscores. Do not include the resource type in the resource identifier since the resource address already includes it. Wrap the resource type and name in double quotes.\n\n❌ Bad:\n\nresource aws_instance webAPI-aws-instance {...}\n\n\n✅ Good:\n\nresource \"aws_instance\" \"web_api\" {...}\n\nResource order\n\nThe order of the resources and data sources in your code does not affect how Terraform builds them, so organize your resources for readability. Terraform determines the creation order based on cross-resource dependencies.\n\nHow you order your resources largely depends on the size and complexity of your code, but we recommend defining data sources alongside the resources that reference them. For readability, your Terraform code should “build on itself” — you should define a data source before the resource that references it.\n\nThe following example defines an aws_instance that relies on two data sources, aws_ami and aws_availability_zone. For readability and continuity, it defines the data sources before the aws_instance resource.\n\ndata \"aws_ami\" \"web\" {\n\n  ##...\n\n}\n\n\n\ndata \"aws_availability_zones\" \"available\" {\n\n  ##...\n\n}\n\n\n\nresource \"aws_instance\" \"web\" {\n\n  ami               = data.aws_ami.web.id\n\n  availability_zone = data.aws_availability_zones.available.names[0]\n\n  ##...\n\n}\n\n\nWe recommend following a consistent order for resource parameters:\n\nIf present, The count or for_each meta-argument.\nResource-specific non-block parameters.\nResource-specific block parameters.\nIf required, a lifecycle block.\nIf required, the depends_on parameter.\nVariables\n\nWhile variables make your modules more flexible, overusing variables can make code difficult to understand. When deciding whether to expose a variable for a resource setting, consider whether that parameter will change between deployments.\n\nDefine a type and a description for every variable.\n\nIf the variable is optional, define a reasonable default.\n\nFor sensitive variables, such as passwords and private keys, set the sensitive parameter to true. Remember that Terraform will still store this value in plain text in its state, but it will not display it when you run terraform plan or terraform apply. Refer to secrets management for more information on how to securely handle sensitive values.\n\nUse input variable validation to create additional rules for your variable values in addition to Terraform's type validation. Only use variable validation when your variable values have uniquely restrictive requirements. For example, if your Terraform configuration requires two web instances, add a validation block to enforce it:\n\nvariable \"web_instance_count\" {\n\n  type        = number\n\n  description = \"Number of web instances to deploy. This application requires at least two instances.\"\n\n\n\n  validation {\n\n    condition     = var.web_instance_count > 1\n\n    error_message = \"This application requires at least two web instances.\"\n\n  }\n\n}\n\n\nWe recommend following a consistent order for variable parameters:\n\nType\nDescription\nDefault (optional)\nSensitive (optional)\nValidation blocks\nOutputs\n\nOutput values let you expose data about your infrastructure on the command line and make it easy to reference in other Terraform configurations. Like you would for variables, provide a description for each output.\n\nWe recommend that you use the following order for your output parameters:\n\nDescription\nValue\nSensitive (optional)\n\nEvery variable and output requires a unique name. For consistency and readability, we recommend that you use a descriptive noun and separate words with underscores.\n\nvariable \"db_disk_size\" {\n\n type        = number\n\n description = \"Disk size for the API database\"\n\n default     = 100\n\n}\n\n\n\nvariable \"db_password\" {\n\n type        = string\n\n description = \"Database password\"\n\n sensitive   = true\n\n}\n\n\n\noutput \"web_public_ip\" {\n\n description = \"Public IP of the web instance\"\n\n value       = aws_instance.web.public_ip\n\n}\n\nLocal values\n\nLocal values let you reference an expression or value multiple times. Use local values sparingly, as overuse can make your code harder to understand.\n\nFor example, you can use a local value to create a suffix for the region and environment (for example, development or test), and append it to multiple resources.\n\nlocals {\n\n  name_suffix = \"${var.region}-${var.environment}\"\n\n}\n\n\n\nresource \"aws_instance\" \"web\" {\n\n  ami           = data.aws_ami.ubuntu.id\n\n  instance_type = \"t3.micro\"\n\n\n\n  tags = {\n\n    Name = \"web-${local.name_suffix}\"\n\n  }\n\n}\n\n\nDefine local values in one of two places:\n\nIf you reference the local value in multiple files, define it in a file named locals.tf.\nIf the local is specific to a file, define it at the top of that file.\n\nAs for other Terraform objects, use descriptive nouns for local value names and underscores to separate multiple words.\n\nFor more information, refer to the local values documentation and the Simplify Terraform configuration with locals tutorial.\n\nProvider aliasing\n\nProvider aliasing lets you define multiple provider blocks for the same Terraform provider. Potential use cases for aliases include provisioning resources in multiple regions within a single configuration. The provider meta-argument for resources and the providers meta-argument for modules specifies which provider to use.\n\nproviders.tf\nprovider \"aws\" {\n\n  region = \"us-east-1\"\n\n}\n\n\n\nprovider \"aws\" {\n\n  alias  = \"west\"\n\n  region = \"us-west-2\"\n\n}\n\nmain.tf\nresource \"aws_instance\" \"example\" {\n\n  provider = aws.west\n\n  # ...\n\n}\n\n\n\nmodule \"aws_vpc\" {\n\n  source = \"./aws_vpc\"\n\n  providers = {\n\n    aws = aws.west\n\n  }\n\n}\n\nAny provider block that does not define the alias parameter is the default provider configuration.\nAlways include a default provider configuration and define all of your providers in the same file.\nIf you define multiple instances of a provider, define the default first.\nFor non-default providers, define the alias as the first parameter of the provider block.\nDynamic resource count\n\nThe for_each and count meta-arguments let you create multiple resources from a single resource block depending on run-time conditions. You can use these meta-arguments to make your code flexible and reduce duplicate resource blocks. If the resources are almost identical, use count. If some of arguments need distinct values that you cannot derive from an integer, use for_each.\n\nThe for_each meta-argument accepts a map or set value, and Terraform will create an instance of that resource for each element in the value you provide. In the following example, Terraform creates an aws_instance for each of the strings defined in the web_instances variable: \"ui\", \"api\", \"db\" and \"metrics\". The example uses each.key to give each instance a unique name. The web_private_ips output uses a for expression to create a map of instance names and their private IP addresses, while the web_ui_public_ip output addresses the instance with the key \"ui\" directly.\n\nvariable \"web_instances\" {\n\n  type        = list(string)\n\n  description = \"A list of instances for the web application\"\n\n  default = [\n\n    \"ui\",\n\n    \"api\",\n\n    \"db\",\n\n    \"metrics\"\n\n  ]\n\n}\n\nresource \"aws_instance\" \"web\" {\n\n  for_each = toset(var.web_instances)\n\n  ami           = data.aws_ami.webapp.id\n\n  instance_type = \"t3.micro\"\n\n  tags = {\n\n    Name = \"web_${each.key}\"\n\n  }\n\n}\n\noutput \"web_private_ips\" {\n\n  description = \"Private IPs of the web instances\"\n\n  value = {\n\n    for k, v in aws_instance.web : k => v.private_ip\n\n  }\n\n}\n\noutput \"web_ui_public_ip\" {\n\n  description = \"Public IP of the web UI instance\"\n\n  value       = aws_instance.web[\"ui\"].public_ip\n\n}\n\n\nThe above example will create the following output:\n\nweb_private_ips = {\n\n  \"api\" = \"172.31.25.29\"\n\n  \"db\" = \"172.31.18.33\"\n\n  \"metrics\" = \"172.31.26.112\"\n\n  \"ui\" = \"172.31.20.142\"\n\n}\n\nweb_ui_public_ip = \"18.216.208.182\"\n\n\nRefer to the for_each meta-argument documentation for more examples.\n\nThe count meta-argument lets you create multiple instances of a resource from a single resource block. Refer to the count meta-argument documentation for examples.\n\nA common practice to conditionally create resources is to use the count meta-argument with a conditional expression. In the following example, Terraform will only create the aws_instance if var.enable_metrics is true.\n\nvariable \"enable_metrics\" {\n\n  description = \"True if the metrics server should be deployed\"\n\n  type        = bool\n\n  default     = true\n\n}\n\n\n\nresource \"aws_instance\" \"web\" {\n\n  count = var.enable_metrics ? 1 : 0\n\n\n\n  ami           = data.aws_ami.webapp.id\n\n  instance_type = \"t3.micro\"\n\n  ##...\n\n}\n\n\nMeta-arguments simplify your code but add complexity, so use them in moderation. If the effect of the meta-argument is not immediately obvious, use a comment for clarification.\n\nTo learn more about these meta-arguments, refer to the for_each and count documentation.\n\n.gitignore\n\nDefine a .gitignore file for your repository to exclude files that you should not publish to version control, such as your state file.\n\nDo not commit:\n\nYour terraform.tfstate state file, including terraform.tfstate.* backup state files.\nYour .terraform.tfstate.lock.info file. Terraform creates and deletes this file automatically when you run a terraform apply command and contains info about your state lock\nYour .terraform directory, where Terraform downloads providers and child modules. Saved plan files that you create when you include the -out flat when you run terraform plan.\nAny .tfvars files that contain sensitive information.\n\nAlways commit:\n\nAll Terraform code files\nYour .terraform.lock.hcl dependency lock file\nA .gitignore file that excludes the files listed below\nA README.md to describe the code, input variables, and outputs\n\nFor an example, refer to GitHub's Terraform .gitignore file.\n\nWorkflow style\n\nThis section reviews standards that enable predictable and secure Terraform workflows, such as:\n\nPin your Terraform, provider, and module versions.\nName your module repositories using this three-part name terraform-<PROVIDER>-<NAME> when using the Terraform Cloud registry.\nStore local modules at ./modules/<module_name>.\nUse the tfe_outputs data source or provider-specific data sources to share state between two state files.\nProtect credentials by using dynamic provider credentials or a secrets manager such as HashiCorp Vault.\nWrite tests for your modules.\nUse policy enforcement on Terraform Cloud to set guardrails for infrastructure operations.\nVersion pinning\n\nTo prevent providers and modules upgrades from introducing unintentional changes to your infrastructure, use version pinning.\n\nSpecify provider versions using the required_providers block. Terraform version constraints support a range of accepted versions.\n\nPin modules to a specific major and minor version as shown in the example below to ensure stability. You can use looser restrictions if you are certain that the module does not introduce breaking changes outside of major version updates.\n\nWe also recommend that you set a minimum required version of the Terraform binary using the required_version in your terraform block. This requires all operators to use a Terraform version that has all of your configuration's required features.\n\nterraform {\n\n  required_providers {\n\n    aws = {\n\n      source  = \"hashicorp/aws\"\n\n      version = \"5.34.0\"\n\n    }\n\n  }\n\n\n\n  required_version = \">= 1.7\"\n\n}\n\n\nThe above example pins the version of the hashicorp/aws provider to version 5.34.0, and requires that operators use Terraform 1.7 or newer.\n\nFor modules sourced from a registry, use the version parameter in the module block to pin the version. For local modules, Terraform ignores the version parameter.\n\nmodule \"vault_starter\" {\n\n  source  = \"hashicorp/vault-starter/aws\"\n\n  version = \"1.0.0\"\n\n  ##...\n\n}\n\nModule repository names\n\nThe Terraform registry requires that repositories match a naming convention for all modules that you publish to the registry. Module repositories must use this three-part name terraform-<PROVIDER>-<NAME>, where <NAME> reflects the type of infrastructure the module manages and <PROVIDER> is the main provider the module uses. The <NAME> segment can contain additional hyphens, for example, terraform-google-vault or terraform-aws-ec2-instance.\n\nModule structure\n\nTerraform modules define self-contained, reusable pieces of infrastructure-as-code.\n\nUse modules to group together logically related resources that you need to provision together. For example:\n\nA networking module that defines a VPC, along with its subnets, gateway, and security groups.\nAn application module defining all resources required for each deployment. This stack could include web servers, databases, storage, and supported networking.\n\nReview the module creation recommended pattern documentation and standard module structure for guidance on how to structure your modules.\n\nLocal modules\n\nLocal modules are sourced from local disk rather than a remote module registry. We recommend publishing your modules to a module registry, such as the Terraform Cloud private module registry, to easily version, share, and reuse modules across your organization. If you cannot use a module registry, using local modules can simplify maintaining and updating your code.\n\nWe recommend that you define child modules in the ./modules/<module_name> directory.\n\nRepository structure\n\nHow you structure your modules and Terraform configuration in version control significantly impacts versioning and operations. We recommend that you store your actual infrastructure configuration separately from your module code.\n\nStore each module in an individual repository. This lets you independently version each module and makes it easier to publish your modules in the private Terraform registry.\n\nOrganize your infrastructure configuration in repositories that group together logically-related resources. For example, a single repository for a web application that requires compute, networking, and database resources . By separating your resources into groups, you limit the number of resources that may be impacted by failures for any operation.\n\nAnother approach is to group all modules and infrastructure configuration into a single monolithic repository, or monorepo. For example, a monorepo may define a collection of local modules for each component of the infrastructure stack, and deploy them in the root module.\n\n.\n\n├── modules\n\n│   ├── function\n\n│   │   ├── main.tf      # contains aws_iam_role, aws_lambda_function\n\n│   │   ├── outputs.tf\n\n│   │   └── variables.tf\n\n│   ├── queue\n\n│   │   ├── main.tf      # contains aws_sqs_queue\n\n│   │   ├── outputs.tf\n\n│   │   └── variables.tf\n\n│   └── vpc\n\n│       ├── main.tf      # contains aws_vpc, aws_subnet\n\n│       ├── outputs.tf\n\n│       └── variables.tf\n\n├── main.tf\n\n├── outputs.tf\n\n└── variables.tf\n\n\nThe advantage of monolithic repositories is having a single source of truth that tracks every infrastructure change. However, monolithic repositories can complicate your CI/CD automation: since any code change triggers a deployment that operates on your entire repository, your workflow must target only the modified directories. You also lose the granular access control, since anyone with repository access can modify any file in it.\n\nIf your organization requires a monolithic approach, Terraform Cloud and Terraform Enterprise let you scope a workspace to a specific directory in a repository, simplifying your workflows.\n\nBranching strategy\n\nTo collaborate on your Terraform code, we recommend using the GitHub flow. This approach uses short-lived branches to help your team quickly review, test, and merge changes to your code. To make changes to your code, you would:\n\nCreate a new branch from your main branch\nWrite, commit, and push your changes to the new branch\nCreate a pull request\nReview the changes with your team\nMerge the pull request\nDelete the branch\n\nTerraform Cloud and Terraform Enterprise can run speculative plans for pull requests. These speculative plans run automatically when you create or update a pull request, and you can use them to see the effect that your changes will have on your infrastructure before you merge them to your main branch. When you merge your pull request, Terraform Cloud will start a new run to apply these changes.\n\nMultiple environments\n\nWe recommend that your repository's main branch be the source of truth for all environments. For Terraform Cloud and Terraform Enterprise users, we recommend that you use separate workspaces for each environment. For larger codebases, we recommend that you split your resources across multiple workspaces to prevent large state files and limit unintended consequences from changes. For example, you could structure your code as follows:\n\n.\n\n├── compute\n\n│   ├── main.tf\n\n│   ├── outputs.tf\n\n│   └── variables.tf\n\n├── database\n\n│   ├── main.tf\n\n│   ├── outputs.tf\n\n│   └── variables.tf\n\n└── networking\n\n    ├── main.tf\n\n    ├── outputs.tf\n\n    └── variables.tf\n\n\nIn this scenario, you would create three workspaces per environment. For example, your production environment would have a prod-compute, prod-database, and prod-networking workspace. Read more about Terraform workspace and project best practices.\n\nIf you do not use Terraform Cloud or Terraform Enterprise, we recommend that you use modules to encapsulate your configuration, and use a directory for each environment so that each one has a separate state file. The configuration in each of these directories would call the local modules, each with parameters specific to their environment. This also lets you maintain separate variable and backend configurations for each environment.\n\n├── modules\n\n│   ├── compute\n\n│   │   └── main.tf\n\n│   ├── database\n\n│   │   └── main.tf\n\n│   └── network\n\n│       └── main.tf\n\n├── dev\n\n│   ├── backend.tf\n\n│   ├── main.tf\n\n│   └── variables.tf\n\n├── prod\n\n│   ├── backend.tf\n\n│   ├── main.tf\n\n│   └── variables.tf\n\n└── staging\n\n    ├── backend.tf\n\n    ├── main.tf\n\n    └── variables.tf\n\nState sharing\n\nSince your state contains sensitive information, avoid sharing full state files when possible.\n\nIf you use Terraform Cloud or Terraform Enterprise and need to reference resources across workspaces, use the tfe_outputs data source.\n\nIf you do not use Terraform Cloud or Terraform Enterprise but still need to reference data about other infrastructure resources, use data sources to query the provider. For example, you can use the aws_instance data source to look up an AWS EC2 instance by its ID or tags.\n\nSecrets management\n\nIf you do not configure remote state storage, the Terraform CLI stores the entire state in plaintext on the local disk. State can include sensitive data, such as passwords and private keys. Terraform Cloud and Terraform Enterprise provide state encryption through HashiCorp Vault.\n\nIf you use Terraform Cloud or Terraform Enterprise, we recommend the following:\n\nWhen using Terraform Enterprise, define and enforce a Sentinel policy to prevent use of the local_exec provisioner or external data sources.\nWhen using Terraform Cloud or Terraform Enterprise, use dynamic provider credentials to avoid using long-lived static credentials.\n\nIf you use Terraform Community Edition, we recommend the following:\n\nConfigure provider credentials using provider-specific environment variables.\nAccess secrets from a secrets management system such as HashiCorp Vault with the Terraform Vault provider. Be aware that Terraform will still write these values in plaintext to your state file.\n\nIf you use a custom CI/CD pipeline, review your CI/CD tool's best practices for managing sensitive values. Most tools let you access sensitive values as environment variables. For more information, refer to your CI/CD documentation.\n\nUsing secrets in GitHub Actions\nGitlab pipeline security\nIntegrate Vault into your CI/CD pipeline\nIntegration and unit testing\n\nTerraform tests let you validate your modules and catch breaking changes. We recommend that you write tests for your Terraform modules and run them just as you run your tests for your application code, such as pre-merge check in your pull requests or as a prerequisite step in your automated CI/CD pipeline.\n\nTests differ from validation methods such as variable validation, preconditions, postconditions, and check blocks. These features focus on verifying the infrastructure deployed by your code, while tests validate the behavior and logic of your code itself. For more information, refer to the Terraform test documentation and the Write Terraform tests tutorial.\n\nPolicy\n\nPolicies are rules that Terraform Cloud enforces on Terraform runs. You can use policies to validate that the Terraform plan complies with your organization's best practices. For example, you can write policies that:\n\nLimit the size of a web instance\nCheck for required resource tags\nBlock deployments on Fridays\nEnforce security configuration and cost management\n\nWe recommend that you store policies in a separate VCS repository from your Terraform code.\n\nFor more information, refer to the policy enforcement documentation, as well as the enforce policy with Sential and detect infrastructure drift and enforce OPA policies tutorials.\n\nNext steps\n\nThis article introduces some considerations to keep in mind as you standardize your organization's Terraform style guidelines. Enforcing a standard way of writing and organizing your Terraform code across your organization ensures that it is readable, maintainable, and shareable.\n\nThe HashiCorp Well-Architected Framework provides more guidance on adapting your Terraform workflows for scale.\n\nOn this page:\n\nStyle Guide\nCode style\nCode formatting\nCode validation\nFile names\nLinting and static code analysis\nComments\nResource naming\nResource order\nVariables\nOutputs\nLocal values\nProvider aliasing\nDynamic resource count\n.gitignore\nWorkflow style\nVersion pinning\nModule repository names\nModule structure\nLocal modules\nRepository structure\nBranching strategy\nMultiple environments\nState sharing\nSecrets management\nIntegration and unit testing\nPolicy\nNext steps\nTheme\nDark\nLight\nSystem\nCertifications\nSystem Status\nCookie Manager\nTerms of Use\nSecurity\nPrivacy\nTrademark Policy\nTrade Controls\nGive Feedback\n(opens in new tab)\nWe use cookies & other similar technology to collect data to improve your experience on our site, as described in our Privacy Policy and Cookie Policy.\nManage PreferencesACCEPT"
  },
  {
    "title": "Style Guide - Configuration Language | Terraform | HashiCorp Developer",
    "url": "https://developer.hashicorp.com/terraform/language/v1.6.x/style",
    "html": "Skip to main content\nNews\nHashiCorp to join IBM to accelerate multi-cloud automation.\nRead the blog →\nDismiss alert\nTerraform\nInstall\nTutorials\nDocumentation\nRegistry\n(opens in new tab)\nTry Cloud\n(opens in new tab)\nSearch\nCommand or control key\nK key\nTerraform Home\nConfiguration Language\nConfiguration Language\nTerraform Language\nOverview\nStyle Guide\nFiles and Directories\nSyntax\nResources\nData Sources\nProviders\nVariables and Outputs\nModules\nMoved block\nChecks\nImport\nExpressions\nFunctions\nTerraform Settings\nState\nTests\nUpgrading to Terraform v1.6\nv1.x Compatibility Promises\nTerraform Internals\nResources\nTutorial Library\nCertifications\nCommunity Forum\n(opens in new tab)\nSupport\n(opens in new tab)\nGitHub\n(opens in new tab)\nTerraform Registry\n(opens in new tab)\n\nYou are viewing documentation for version v1.6.x. View latest version.\n\nDeveloper\nTerraform\nConfiguration Language v1.6.x\nStyle Guide\nv1.6.x\nStyle Guide\n\nThe flexibility of Terraform's configuration language gives you many options to choose from as you write your code, structure your directories, and test your configuration. While some design decisions depend on your organization's needs or preferences, there are some common patterns that we suggest you adopt. Adopting and adhering to a style guide keeps your Terraform code legible, scalable, and maintainable.\n\nThis article discusses best practices and some considerations to keep in mind as you develop your organization's style guide. The article is split into two sections. The first section covers code style recommendations, such as formatting and resource organization. The second section covers operations and workflow recommendations, such as lifecycle management through meta-arguments, versioning, and sensitive data management.\n\nCode style\n\nWriting Terraform code in a consistent style makes it easier to read and maintain. The following sections discuss code style recommendations, including the following:\n\nRun terraform fmt and terraform validate before committing your code to version control.\nUse a linter such as TFLint to enforce your organization's own coding best practices.\nUse # for single and multi-line comments.\nUse nouns for resource names and do not include the resource type in the name.\nUse underscores to separate multiple words in names. Wrap the resource type and name in double quotes in your resource definition.\nLet your code build on itself: define dependent resources after the resources that reference them.\nInclude a type and description for every variable.\nInclude a description for every output.\nAvoid overuse of variables and local values.\nAlways include a default provider configuration.\nUse count and for_each sparingly.\nCode formatting\n\nThe Terraform parser allows you some flexibility in how you lay out the elements in your configuration files, but the Terraform language also has some idiomatic style conventions which we recommend users always follow for consistency between files and modules written by different teams.\n\nIndent two spaces for each nesting level\n\nWhen multiple arguments with single-line values appear on consecutive lines at the same nesting level, align their equals signs:\n\nami           = \"abc123\"\n\ninstance_type = \"t2.micro\"\n\n\nWhen both arguments and blocks appear together inside a block body, place all of the arguments together at the top and then place nested blocks below them. Use one blank line to separate the arguments from the blocks.\n\nUse empty lines to separate logical groups of arguments within a block.\n\nFor blocks that contain both arguments and \"meta-arguments\" (as defined by the Terraform language semantics), list meta-arguments first and separate them from other arguments with one blank line. Place meta-argument blocks last and separate them from other blocks with one blank line. Refer to dynamic resource count for more information on meta-arguments.\n\nresource \"aws_instance\" \"example\" {\n\n  # meta-argument first\n\n  count = 2\n\n\n\n  ami           = \"abc123\"\n\n  instance_type = \"t2.micro\"\n\n\n\n  network_interface {\n\n    # ...\n\n  }\n\n\n\n  # meta-argument block last\n\n  lifecycle {\n\n    create_before_destroy = true\n\n  }\n\n}\n\n\nTop-level blocks should always be separated from one another by one blank line. Nested blocks should also be separated by blank lines, except when grouping together related blocks of the same type (like multiple provisioner blocks in a resource).\n\nAvoid grouping multiple blocks of the same type with other blocks of a different type, unless the block types are defined by semantics to form a family. (For example: root_block_device, ebs_block_device and ephemeral_block_device on aws_instance form a family of block types describing AWS block devices, and can therefore be grouped together and mixed.)\n\nThe terraform fmt command formats your Terraform configuration to a subset of the above recommendations. By default, the terraform fmt command will only modify your Terraform code in the directory that you execute it in, but you can include the -recursive flag to modify code in all subdirectories as well.\n\nWe recommend that you run terraform fmt before each commit to version control. You can use mechanisms such as Git pre-commit hooks to automatically run this command each time you commit your code.\n\nIf you use Microsoft VS Code, use the Terraform VS Code extension to enable features such as syntax highlighting and validation, automatic code formatting, and integration with Terraform Cloud. If your development environment or text editor supports the Language Server Protocol, you can use the Terraform Language Server to access most of the VS Code extension features.\n\nCode validation\n\nThe terraform validate command checks that your configuration is syntactically valid and internally consistent. The validate command does not check if argument values are valid for a specific provider, but it will verify that they are the correct type. It does not evaluate any existing state.\n\nThe terraform validate command is safe to run automatically and frequently. You can configure your text editor to run this command as a post-save check, define it as a pre-commit hook in a Git repository, or run it as a step in a CI/CD pipeline.\n\nFor more information, refer to the Terraform validate documentation.\n\nFile names\n\nWe recommend the following file naming conventions:\n\nA backend.tf file that contains your backend configuration. You can define multiple terraform blocks in your configuration to separate your backend configuration from your Terraform and provider versioning configuration.\nA main.tf file that contains all resource and data source blocks.\nA outputs.tf file that contains all output blocks in alphabetical order.\nA providers.tf file that contains all provider blocks and configuration.\nA terraform.tf file that contains a single terraform block which defines your required_version and required_providers.\nA variables.tf file that contains all variable blocks in alphabetical order.\nA locals.tf file that contains local values. Refer to local values for more information.\nA override.tf file that contains override definitions for your configuration. Terraform loads this and all files ending with _override.tf last. Use them sparingly and add comments to the original resource definitions, as these overrides make your code harder to reason about. Refer to the override files documentation for more information.\n\nAs your codebase grows, limiting it to just these files can become difficult to maintain. If your code becomes hard to navigate due to its size, we recommend that you organize resources and data sources in separate files by logical groups. For example, if your web application requires networking, storage, and compute resources, you might create the following files:\n\nA network.tf file that contains your VPC, subnets, load balancers, and all other networking resources.\nA storage.tf file that contains your object storage and related permissions configuration.\nA compute.tf file that contains your compute instances.\n\nNo matter how you decide to split your code, it should be immediately clear where a maintainer can find a specific resource or data source definition.\n\nAs your configuration grows, you may need to separate it into multiple state files. The HashiCorp Well-Architected Framework provides more guidance about configuration structure and scope.\n\nLinting and static code analysis\n\nTerraform does not have a built-in linter, but many organizations rely on a third party linting tool such as TFLint to enforce code standards. A linter uses static code analysis to compare your Terraform code against a set of rules. Most linters ship with a default set of rules, but also let you write your own.\n\nComments\n\nWrite your code so it is easy to understand. Only when necessary, use comments to clarify complexity for other maintainers.\n\nUse # for both single- and multi-line comments. The // and /* */ comment syntaxes are not considered idiomatic, but Terraform supports them to remain backwards-compatible with earlier versions of HCL.\n\n# Each tunnel is responsible for encrypting and decrypting traffic exiting\n\n# and leaving its associated gateway.\n\nresource \"google_compute_vpn_tunnel\" \"tunnel1\" {\n\n  ## ...\n\nResource naming\n\nEvery resource within a configuration must have a unique name. For consistency and readability, use a descriptive noun and separate words with underscores. Do not include the resource type in the resource identifier since the resource address already includes it. Wrap the resource type and name in double quotes.\n\n❌ Bad:\n\nresource aws_instance webAPI-aws-instance {...}\n\n\n✅ Good:\n\nresource \"aws_instance\" \"web_api\" {...}\n\nResource order\n\nThe order of the resources and data sources in your code does not affect how Terraform builds them, so organize your resources for readability. Terraform determines the creation order based on cross-resource dependencies.\n\nHow you order your resources largely depends on the size and complexity of your code, but we recommend defining data sources alongside the resources that reference them. For readability, your Terraform code should “build on itself” — you should define a data source before the resource that references it.\n\nThe following example defines an aws_instance that relies on two data sources, aws_ami and aws_availability_zone. For readability and continuity, it defines the data sources before the aws_instance resource.\n\ndata \"aws_ami\" \"web\" {\n\n  ##...\n\n}\n\n\n\ndata \"aws_availability_zones\" \"available\" {\n\n  ##...\n\n}\n\n\n\nresource \"aws_instance\" \"web\" {\n\n  ami               = data.aws_ami.web.id\n\n  availability_zone = data.aws_availability_zones.available.names[0]\n\n  ##...\n\n}\n\n\nWe recommend following a consistent order for resource parameters:\n\nIf present, The count or for_each meta-argument.\nResource-specific non-block parameters.\nResource-specific block parameters.\nIf required, a lifecycle block.\nIf required, the depends_on parameter.\nVariables\n\nWhile variables make your modules more flexible, overusing variables can make code difficult to understand. When deciding whether to expose a variable for a resource setting, consider whether that parameter will change between deployments.\n\nDefine a type and a description for every variable.\n\nIf the variable is optional, define a reasonable default.\n\nFor sensitive variables, such as passwords and private keys, set the sensitive parameter to true. Remember that Terraform will still store this value in plain text in its state, but it will not display it when you run terraform plan or terraform apply. Refer to secrets management for more information on how to securely handle sensitive values.\n\nUse input variable validation to create additional rules for your variable values in addition to Terraform's type validation. Only use variable validation when your variable values have uniquely restrictive requirements. For example, if your Terraform configuration requires two web instances, add a validation block to enforce it:\n\nvariable \"web_instance_count\" {\n\n  type        = number\n\n  description = \"Number of web instances to deploy. This application requires at least two instances.\"\n\n\n\n  validation {\n\n    condition     = var.web_instance_count > 1\n\n    error_message = \"This application requires at least two web instances.\"\n\n  }\n\n}\n\n\nWe recommend following a consistent order for variable parameters:\n\nType\nDescription\nDefault (optional)\nSensitive (optional)\nValidation blocks\nOutputs\n\nOutput values let you expose data about your infrastructure on the command line and make it easy to reference in other Terraform configurations. Like you would for variables, provide a description for each output.\n\nWe recommend that you use the following order for your output parameters:\n\nDescription\nValue\nSensitive (optional)\n\nEvery variable and output requires a unique name. For consistency and readability, we recommend that you use a descriptive noun and separate words with underscores.\n\nvariable \"db_disk_size\" {\n\n type        = number\n\n description = \"Disk size for the API database\"\n\n default     = 100\n\n}\n\n\n\nvariable \"db_password\" {\n\n type        = string\n\n description = \"Database password\"\n\n sensitive   = true\n\n}\n\n\n\noutput \"web_public_ip\" {\n\n description = \"Public IP of the web instance\"\n\n value       = aws_instance.web.public_ip\n\n}\n\nLocal values\n\nLocal values let you reference an expression or value multiple times. Use local values sparingly, as overuse can make your code harder to understand.\n\nFor example, you can use a local value to create a suffix for the region and environment (for example, development or test), and append it to multiple resources.\n\nlocals {\n\n  name_suffix = \"${var.region}-${var.environment}\"\n\n}\n\n\n\nresource \"aws_instance\" \"web\" {\n\n  ami           = data.aws_ami.ubuntu.id\n\n  instance_type = \"t3.micro\"\n\n\n\n  tags = {\n\n    Name = \"web-${local.name_suffix}\"\n\n  }\n\n}\n\n\nDefine local values in one of two places:\n\nIf you reference the local value in multiple files, define it in a file named locals.tf.\nIf the local is specific to a file, define it at the top of that file.\n\nAs for other Terraform objects, use descriptive nouns for local value names and underscores to separate multiple words.\n\nFor more information, refer to the local values documentation and the Simplify Terraform configuration with locals tutorial.\n\nProvider aliasing\n\nProvider aliasing lets you define multiple provider blocks for the same Terraform provider. Potential use cases for aliases include provisioning resources in multiple regions within a single configuration. The provider meta-argument for resources and the providers meta-argument for modules specifies which provider to use.\n\nproviders.tf\nprovider \"aws\" {\n\n  region = \"us-east-1\"\n\n}\n\n\n\nprovider \"aws\" {\n\n  alias  = \"west\"\n\n  region = \"us-west-2\"\n\n}\n\nmain.tf\nresource \"aws_instance\" \"example\" {\n\n  provider = aws.west\n\n  # ...\n\n}\n\n\n\nmodule \"aws_vpc\" {\n\n  source = \"./aws_vpc\"\n\n  providers = {\n\n    aws = aws.west\n\n  }\n\n}\n\nAny provider block that does not define the alias parameter is the default provider configuration.\nAlways include a default provider configuration and define all of your providers in the same file.\nIf you define multiple instances of a provider, define the default first.\nFor non-default providers, define the alias as the first parameter of the provider block.\nDynamic resource count\n\nThe for_each and count meta-arguments let you create multiple resources from a single resource block depending on run-time conditions. You can use these meta-arguments to make your code flexible and reduce duplicate resource blocks. If the resources are almost identical, use count. If some of arguments need distinct values that you cannot derive from an integer, use for_each.\n\nThe for_each meta-argument accepts a map or set value, and Terraform will create an instance of that resource for each element in the value you provide. In the following example, Terraform creates an aws_instance for each of the strings defined in the web_instances variable: \"ui\", \"api\", \"db\" and \"metrics\". The example uses each.key to give each instance a unique name. The web_private_ips output uses a for expression to create a map of instance names and their private IP addresses, while the web_ui_public_ip output addresses the instance with the key \"ui\" directly.\n\nvariable \"web_instances\" {\n\n  type        = list(string)\n\n  description = \"A list of instances for the web application\"\n\n  default = [\n\n    \"ui\",\n\n    \"api\",\n\n    \"db\",\n\n    \"metrics\"\n\n  ]\n\n}\n\nresource \"aws_instance\" \"web\" {\n\n  for_each = toset(var.web_instances)\n\n  ami           = data.aws_ami.webapp.id\n\n  instance_type = \"t3.micro\"\n\n  tags = {\n\n    Name = \"web_${each.key}\"\n\n  }\n\n}\n\noutput \"web_private_ips\" {\n\n  description = \"Private IPs of the web instances\"\n\n  value = {\n\n    for k, v in aws_instance.web : k => v.private_ip\n\n  }\n\n}\n\noutput \"web_ui_public_ip\" {\n\n  description = \"Public IP of the web UI instance\"\n\n  value       = aws_instance.web[\"ui\"].public_ip\n\n}\n\n\nThe above example will create the following output:\n\nweb_private_ips = {\n\n  \"api\" = \"172.31.25.29\"\n\n  \"db\" = \"172.31.18.33\"\n\n  \"metrics\" = \"172.31.26.112\"\n\n  \"ui\" = \"172.31.20.142\"\n\n}\n\nweb_ui_public_ip = \"18.216.208.182\"\n\n\nRefer to the for_each meta-argument documentation for more examples.\n\nThe count meta-argument lets you create multiple instances of a resource from a single resource block. Refer to the count meta-argument documentation for examples.\n\nA common practice to conditionally create resources is to use the count meta-argument with a conditional expression. In the following example, Terraform will only create the aws_instance if var.enable_metrics is true.\n\nvariable \"enable_metrics\" {\n\n  description = \"True if the metrics server should be deployed\"\n\n  type        = bool\n\n  default     = true\n\n}\n\n\n\nresource \"aws_instance\" \"web\" {\n\n  count = var.enable_metrics ? 1 : 0\n\n\n\n  ami           = data.aws_ami.webapp.id\n\n  instance_type = \"t3.micro\"\n\n  ##...\n\n}\n\n\nMeta-arguments simplify your code but add complexity, so use them in moderation. If the effect of the meta-argument is not immediately obvious, use a comment for clarification.\n\nTo learn more about these meta-arguments, refer to the for_each and count documentation.\n\n.gitignore\n\nDefine a .gitignore file for your repository to exclude files that you should not publish to version control, such as your state file.\n\nDo not commit:\n\nYour terraform.tfstate state file, including terraform.tfstate.* backup state files.\nYour .terraform.tfstate.lock.info file. Terraform creates and deletes this file automatically when you run a terraform apply command and contains info about your state lock\nYour .terraform directory, where Terraform downloads providers and child modules. Saved plan files that you create when you include the -out flat when you run terraform plan.\nAny .tfvars files that contain sensitive information.\n\nAlways commit:\n\nAll Terraform code files\nYour .terraform.lock.hcl dependency lock file\nA .gitignore file that excludes the files listed below\nA README.md to describe the code, input variables, and outputs\n\nFor an example, refer to GitHub's Terraform .gitignore file.\n\nWorkflow style\n\nThis section reviews standards that enable predictable and secure Terraform workflows, such as:\n\nPin your Terraform, provider, and module versions.\nName your module repositories using this three-part name terraform-<PROVIDER>-<NAME> when using the Terraform Cloud registry.\nStore local modules at ./modules/<module_name>.\nUse the tfe_outputs data source or provider-specific data sources to share state between two state files.\nProtect credentials by using dynamic provider credentials or a secrets manager such as HashiCorp Vault.\nWrite tests for your modules.\nUse policy enforcement on Terraform Cloud to set guardrails for infrastructure operations.\nVersion pinning\n\nTo prevent providers and modules upgrades from introducing unintentional changes to your infrastructure, use version pinning.\n\nSpecify provider versions using the required_providers block. Terraform version constraints support a range of accepted versions.\n\nPin modules to a specific major and minor version as shown in the example below to ensure stability. You can use looser restrictions if you are certain that the module does not introduce breaking changes outside of major version updates.\n\nWe also recommend that you set a minimum required version of the Terraform binary using the required_version in your terraform block. This requires all operators to use a Terraform version that has all of your configuration's required features.\n\nterraform {\n\n  required_providers {\n\n    aws = {\n\n      source  = \"hashicorp/aws\"\n\n      version = \"5.34.0\"\n\n    }\n\n  }\n\n\n\n  required_version = \">= 1.7\"\n\n}\n\n\nThe above example pins the version of the hashicorp/aws provider to version 5.34.0, and requires that operators use Terraform 1.7 or newer.\n\nFor modules sourced from a registry, use the version parameter in the module block to pin the version. For local modules, Terraform ignores the version parameter.\n\nmodule \"vault_starter\" {\n\n  source  = \"hashicorp/vault-starter/aws\"\n\n  version = \"1.0.0\"\n\n  ##...\n\n}\n\nModule repository names\n\nThe Terraform registry requires that repositories match a naming convention for all modules that you publish to the registry. Module repositories must use this three-part name terraform-<PROVIDER>-<NAME>, where <NAME> reflects the type of infrastructure the module manages and <PROVIDER> is the main provider the module uses. The <NAME> segment can contain additional hyphens, for example, terraform-google-vault or terraform-aws-ec2-instance.\n\nModule structure\n\nTerraform modules define self-contained, reusable pieces of infrastructure-as-code.\n\nUse modules to group together logically related resources that you need to provision together. For example:\n\nA networking module that defines a VPC, along with its subnets, gateway, and security groups.\nAn application module defining all resources required for each deployment. This stack could include web servers, databases, storage, and supported networking.\n\nReview the module creation recommended pattern documentation and standard module structure for guidance on how to structure your modules.\n\nLocal modules\n\nLocal modules are sourced from local disk rather than a remote module registry. We recommend publishing your modules to a module registry, such as the Terraform Cloud private module registry, to easily version, share, and reuse modules across your organization. If you cannot use a module registry, using local modules can simplify maintaining and updating your code.\n\nWe recommend that you define child modules in the ./modules/<module_name> directory.\n\nRepository structure\n\nHow you structure your modules and Terraform configuration in version control significantly impacts versioning and operations. We recommend that you store your actual infrastructure configuration separately from your module code.\n\nStore each module in an individual repository. This lets you independently version each module and makes it easier to publish your modules in the private Terraform registry.\n\nOrganize your infrastructure configuration in repositories that group together logically-related resources. For example, a single repository for a web application that requires compute, networking, and database resources . By separating your resources into groups, you limit the number of resources that may be impacted by failures for any operation.\n\nAnother approach is to group all modules and infrastructure configuration into a single monolithic repository, or monorepo. For example, a monorepo may define a collection of local modules for each component of the infrastructure stack, and deploy them in the root module.\n\n.\n\n├── modules\n\n│   ├── function\n\n│   │   ├── main.tf      # contains aws_iam_role, aws_lambda_function\n\n│   │   ├── outputs.tf\n\n│   │   └── variables.tf\n\n│   ├── queue\n\n│   │   ├── main.tf      # contains aws_sqs_queue\n\n│   │   ├── outputs.tf\n\n│   │   └── variables.tf\n\n│   └── vpc\n\n│       ├── main.tf      # contains aws_vpc, aws_subnet\n\n│       ├── outputs.tf\n\n│       └── variables.tf\n\n├── main.tf\n\n├── outputs.tf\n\n└── variables.tf\n\n\nThe advantage of monolithic repositories is having a single source of truth that tracks every infrastructure change. However, monolithic repositories can complicate your CI/CD automation: since any code change triggers a deployment that operates on your entire repository, your workflow must target only the modified directories. You also lose the granular access control, since anyone with repository access can modify any file in it.\n\nIf your organization requires a monolithic approach, Terraform Cloud and Terraform Enterprise let you scope a workspace to a specific directory in a repository, simplifying your workflows.\n\nBranching strategy\n\nTo collaborate on your Terraform code, we recommend using the GitHub flow. This approach uses short-lived branches to help your team quickly review, test, and merge changes to your code. To make changes to your code, you would:\n\nCreate a new branch from your main branch\nWrite, commit, and push your changes to the new branch\nCreate a pull request\nReview the changes with your team\nMerge the pull request\nDelete the branch\n\nTerraform Cloud and Terraform Enterprise can run speculative plans for pull requests. These speculative plans run automatically when you create or update a pull request, and you can use them to see the effect that your changes will have on your infrastructure before you merge them to your main branch. When you merge your pull request, Terraform Cloud will start a new run to apply these changes.\n\nMultiple environments\n\nWe recommend that your repository's main branch be the source of truth for all environments. For Terraform Cloud and Terraform Enterprise users, we recommend that you use separate workspaces for each environment. For larger codebases, we recommend that you split your resources across multiple workspaces to prevent large state files and limit unintended consequences from changes. For example, you could structure your code as follows:\n\n.\n\n├── compute\n\n│   ├── main.tf\n\n│   ├── outputs.tf\n\n│   └── variables.tf\n\n├── database\n\n│   ├── main.tf\n\n│   ├── outputs.tf\n\n│   └── variables.tf\n\n└── networking\n\n    ├── main.tf\n\n    ├── outputs.tf\n\n    └── variables.tf\n\n\nIn this scenario, you would create three workspaces per environment. For example, your production environment would have a prod-compute, prod-database, and prod-networking workspace. Read more about Terraform workspace and project best practices.\n\nIf you do not use Terraform Cloud or Terraform Enterprise, we recommend that you use modules to encapsulate your configuration, and use a directory for each environment so that each one has a separate state file. The configuration in each of these directories would call the local modules, each with parameters specific to their environment. This also lets you maintain separate variable and backend configurations for each environment.\n\n├── modules\n\n│   ├── compute\n\n│   │   └── main.tf\n\n│   ├── database\n\n│   │   └── main.tf\n\n│   └── network\n\n│       └── main.tf\n\n├── dev\n\n│   ├── backend.tf\n\n│   ├── main.tf\n\n│   └── variables.tf\n\n├── prod\n\n│   ├── backend.tf\n\n│   ├── main.tf\n\n│   └── variables.tf\n\n└── staging\n\n    ├── backend.tf\n\n    ├── main.tf\n\n    └── variables.tf\n\nState sharing\n\nSince your state contains sensitive information, avoid sharing full state files when possible.\n\nIf you use Terraform Cloud or Terraform Enterprise and need to reference resources across workspaces, use the tfe_outputs data source.\n\nIf you do not use Terraform Cloud or Terraform Enterprise but still need to reference data about other infrastructure resources, use data sources to query the provider. For example, you can use the aws_instance data source to look up an AWS EC2 instance by its ID or tags.\n\nSecrets management\n\nIf you do not configure remote state storage, the Terraform CLI stores the entire state in plaintext on the local disk. State can include sensitive data, such as passwords and private keys. Terraform Cloud and Terraform Enterprise provide state encryption through HashiCorp Vault.\n\nIf you use Terraform Cloud or Terraform Enterprise, we recommend the following:\n\nWhen using Terraform Enterprise, define and enforce a Sentinel policy to prevent use of the local_exec provisioner or external data sources.\nWhen using Terraform Cloud or Terraform Enterprise, use dynamic provider credentials to avoid using long-lived static credentials.\n\nIf you use Terraform Community Edition, we recommend the following:\n\nConfigure provider credentials using provider-specific environment variables.\nAccess secrets from a secrets management system such as HashiCorp Vault with the Terraform Vault provider. Be aware that Terraform will still write these values in plaintext to your state file.\n\nIf you use a custom CI/CD pipeline, review your CI/CD tool's best practices for managing sensitive values. Most tools let you access sensitive values as environment variables. For more information, refer to your CI/CD documentation.\n\nUsing secrets in GitHub Actions\nGitlab pipeline security\nIntegrate Vault into your CI/CD pipeline\nIntegration and unit testing\n\nTerraform tests let you validate your modules and catch breaking changes. We recommend that you write tests for your Terraform modules and run them just as you run your tests for your application code, such as pre-merge check in your pull requests or as a prerequisite step in your automated CI/CD pipeline.\n\nTests differ from validation methods such as variable validation, preconditions, postconditions, and check blocks. These features focus on verifying the infrastructure deployed by your code, while tests validate the behavior and logic of your code itself. For more information, refer to the Terraform test documentation and the Write Terraform tests tutorial.\n\nPolicy\n\nPolicies are rules that Terraform Cloud enforces on Terraform runs. You can use policies to validate that the Terraform plan complies with your organization's best practices. For example, you can write policies that:\n\nLimit the size of a web instance\nCheck for required resource tags\nBlock deployments on Fridays\nEnforce security configuration and cost management\n\nWe recommend that you store policies in a separate VCS repository from your Terraform code.\n\nFor more information, refer to the policy enforcement documentation, as well as the enforce policy with Sential and detect infrastructure drift and enforce OPA policies tutorials.\n\nNext steps\n\nThis article introduces some considerations to keep in mind as you standardize your organization's Terraform style guidelines. Enforcing a standard way of writing and organizing your Terraform code across your organization ensures that it is readable, maintainable, and shareable.\n\nThe HashiCorp Well-Architected Framework provides more guidance on adapting your Terraform workflows for scale.\n\nOn this page:\n\nStyle Guide\nCode style\nCode formatting\nCode validation\nFile names\nLinting and static code analysis\nComments\nResource naming\nResource order\nVariables\nOutputs\nLocal values\nProvider aliasing\nDynamic resource count\n.gitignore\nWorkflow style\nVersion pinning\nModule repository names\nModule structure\nLocal modules\nRepository structure\nBranching strategy\nMultiple environments\nState sharing\nSecrets management\nIntegration and unit testing\nPolicy\nNext steps\nTheme\nDark\nLight\nSystem\nCertifications\nSystem Status\nCookie Manager\nTerms of Use\nSecurity\nPrivacy\nTrademark Policy\nTrade Controls\nGive Feedback\n(opens in new tab)\nWe use cookies & other similar technology to collect data to improve your experience on our site, as described in our Privacy Policy and Cookie Policy.\nManage PreferencesACCEPT"
  },
  {
    "title": "HashiCorp Developer",
    "url": "https://developer.hashicorp.com/terraform/language/v1.5.x/style",
    "html": "News\nHashiCorp to join IBM to accelerate multi-cloud automation.\nRead the blog →\nDismiss alert\nProducts\nTutorials\nSearch\nCommand or control key\nK key\nThis page does not exist for version v1.5.x.\n\nPlease select either the most recent version or a valid version that includes the page you are looking for.\n\nGo back to Documentation\nTheme\nCertifications\nSystem Status\nCookie Manager\nTerms of Use\nSecurity\nPrivacy\nTrademark Policy\nTrade Controls\nGive Feedback\n(opens in new tab)"
  },
  {
    "title": "Providers - Configuration Language | Terraform | HashiCorp Developer",
    "url": "https://developer.hashicorp.com/terraform/language/v1.1.x/providers",
    "html": "Skip to main content\nNews\nHashiCorp to join IBM to accelerate multi-cloud automation.\nRead the blog →\nDismiss alert\nTerraform\nInstall\nTutorials\nDocumentation\nRegistry\n(opens in new tab)\nTry Cloud\n(opens in new tab)\nSearch\nCommand or control key\nK key\nTerraform Home\nConfiguration Language\nConfiguration Language\nTerraform Language\nOverview\nFiles and Directories\nSyntax\nResources\nData Sources\nProviders\nOverview\nProvider Configuration\nProvider Requirements\nDependency Lock File\nVariables and Outputs\nModules\nExpressions\nFunctions\nTerraform Settings\nState\nUpgrade Guides\nHistorical docs: 0.11 and Older\nResources\nTutorial Library\nCertifications\nCommunity Forum\n(opens in new tab)\nSupport\n(opens in new tab)\nGitHub\n(opens in new tab)\nTerraform Registry\n(opens in new tab)\n\nYou are viewing documentation for version v1.1 and earlier. View latest version.\n\nDeveloper\nTerraform\nConfiguration Language v1.1.x\nProviders\nv1.1 and earlier\nProviders\n\nHands-on: Try the Perform CRUD Operations with Providers tutorial on HashiCorp Learn.\n\nTerraform relies on plugins called \"providers\" to interact with cloud providers, SaaS providers, and other APIs.\n\nTerraform configurations must declare which providers they require so that Terraform can install and use them. Additionally, some providers require configuration (like endpoint URLs or cloud regions) before they can be used.\n\nWhat Providers Do\n\nEach provider adds a set of resource types and/or data sources that Terraform can manage.\n\nEvery resource type is implemented by a provider; without providers, Terraform can't manage any kind of infrastructure.\n\nMost providers configure a specific infrastructure platform (either cloud or self-hosted). Providers can also offer local utilities for tasks like generating random numbers for unique resource names.\n\nWhere Providers Come From\n\nProviders are distributed separately from Terraform itself, and each provider has its own release cadence and version numbers.\n\nThe Terraform Registry is the main directory of publicly available Terraform providers, and hosts providers for most major infrastructure platforms.\n\nProvider Documentation\n\nEach provider has its own documentation, describing its resource types and their arguments.\n\nThe Terraform Registry includes documentation for a wide range of providers developed by HashiCorp, third-party vendors, and our Terraform community. Use the \"Documentation\" link in a provider's header to browse its documentation.\n\nProvider documentation in the Registry is versioned; you can use the version menu in the header to change which version you're viewing.\n\nFor details about writing, generating, and previewing provider documentation, see the provider publishing documentation.\n\nHow to Use Providers\n\nTo use resources from a given provider, you need to include some information about it in your configuration. See the following pages for details:\n\nProvider Requirements documents how to declare providers so Terraform can install them.\n\nProvider Configuration documents how to configure settings for providers.\n\nDependency Lock File documents an additional HCL file that can be included with a configuration, which tells Terraform to always use a specific set of provider versions.\n\nProvider Installation\n\nTerraform Cloud and Terraform Enterprise install providers as part of every run.\n\nTerraform CLI finds and installs providers when initializing a working directory. It can automatically download providers from a Terraform registry, or load them from a local mirror or cache. If you are using a persistent working directory, you must reinitialize whenever you change a configuration's providers.\n\nTo save time and bandwidth, Terraform CLI supports an optional plugin cache. You can enable the cache using the plugin_cache_dir setting in the CLI configuration file.\n\nTo ensure Terraform always installs the same provider versions for a given configuration, you can use Terraform CLI to create a dependency lock file and commit it to version control along with your configuration. If a lock file is present, Terraform Cloud, CLI, and Enterprise will all obey it when installing providers.\n\nHands-on: Try the Lock and Upgrade Provider Versions tutorial on HashiCorp Learn.\n\nHow to Find Providers\n\nTo find providers for the infrastructure platforms you use, browse the providers section of the Terraform Registry.\n\nSome providers on the Registry are developed and published by HashiCorp, some are published by platform maintainers, and some are published by users and volunteers. The provider listings use the following badges to indicate who develops and maintains a given provider.\n\nTier\tDescription\tNamespace\n\nOfficial\n\tOfficial providers are owned and maintained by HashiCorp\thashicorp\n\nPartner\n\tPartner providers are written, maintained, validated and published by third-party companies against their own APIs. To earn a partner provider badge the partner must participate in the HashiCorp Technology Partner Program.\tThird-party organization, e.g. mongodb/mongodbatlas\n\nCommunity\n\tCommunity providers are published to the Terraform Registry by individual maintainers, groups of maintainers, or other members of the Terraform community.\tMaintainer’s individual or organization account, e.g. DeviaVir/gsuite\n\nArchived\n\tArchived Providers are Official or Partner Providers that are no longer maintained by HashiCorp or the community. This may occur if an API is deprecated or interest was low.\thashicorp or third-party\n\nHow to Develop Providers\n\nProviders are written in Go, using the Terraform Plugin SDK. For more information on developing providers, see:\n\nThe Plugin Development documentation\nThe Call APIs with Terraform Providers collection on HashiCorp Learn\n\nOn this page:\n\nProviders\nWhat Providers Do\nWhere Providers Come From\nProvider Documentation\nHow to Use Providers\nProvider Installation\nHow to Find Providers\nHow to Develop Providers\nTheme\nDark\nLight\nSystem\nCertifications\nSystem Status\nCookie Manager\nTerms of Use\nSecurity\nPrivacy\nTrademark Policy\nTrade Controls\nGive Feedback\n(opens in new tab)\nWe use cookies & other similar technology to collect data to improve your experience on our site, as described in our Privacy Policy and Cookie Policy.\nManage PreferencesACCEPT"
  },
  {
    "title": "Style Guide - Configuration Language | Terraform | HashiCorp Developer",
    "url": "https://developer.hashicorp.com/terraform/language/v1.7.x/style",
    "html": "Skip to main content\nNews\nHashiCorp to join IBM to accelerate multi-cloud automation.\nRead the blog →\nDismiss alert\nTerraform\nInstall\nTutorials\nDocumentation\nRegistry\n(opens in new tab)\nTry Cloud\n(opens in new tab)\nSearch\nCommand or control key\nK key\nTerraform Home\nConfiguration Language\nConfiguration Language\nTerraform Language\nOverview\nStyle Guide\nFiles and Directories\nSyntax\nResources\nData Sources\nProviders\nVariables and Outputs\nModules\nMoved block\nChecks\nImport\nExpressions\nFunctions\nTerraform Settings\nState\nTests\nUpgrading to Terraform v1.7\nv1.x Compatibility Promises\nTerraform Internals\nResources\nTutorial Library\nCertifications\nCommunity Forum\n(opens in new tab)\nSupport\n(opens in new tab)\nGitHub\n(opens in new tab)\nTerraform Registry\n(opens in new tab)\n\nYou are viewing documentation for version v1.7.x. View latest version.\n\nDeveloper\nTerraform\nConfiguration Language v1.7.x\nStyle Guide\nv1.7.x\nStyle Guide\n\nThe flexibility of Terraform's configuration language gives you many options to choose from as you write your code, structure your directories, and test your configuration. While some design decisions depend on your organization's needs or preferences, there are some common patterns that we suggest you adopt. Adopting and adhering to a style guide keeps your Terraform code legible, scalable, and maintainable.\n\nThis article discusses best practices and some considerations to keep in mind as you develop your organization's style guide. The article is split into two sections. The first section covers code style recommendations, such as formatting and resource organization. The second section covers operations and workflow recommendations, such as lifecycle management through meta-arguments, versioning, and sensitive data management.\n\nCode style\n\nWriting Terraform code in a consistent style makes it easier to read and maintain. The following sections discuss code style recommendations, including the following:\n\nRun terraform fmt and terraform validate before committing your code to version control.\nUse a linter such as TFLint to enforce your organization's own coding best practices.\nUse # for single and multi-line comments.\nUse nouns for resource names and do not include the resource type in the name.\nUse underscores to separate multiple words in names. Wrap the resource type and name in double quotes in your resource definition.\nLet your code build on itself: define dependent resources after the resources that reference them.\nInclude a type and description for every variable.\nInclude a description for every output.\nAvoid overuse of variables and local values.\nAlways include a default provider configuration.\nUse count and for_each sparingly.\nCode formatting\n\nThe Terraform parser allows you some flexibility in how you lay out the elements in your configuration files, but the Terraform language also has some idiomatic style conventions which we recommend users always follow for consistency between files and modules written by different teams.\n\nIndent two spaces for each nesting level\n\nWhen multiple arguments with single-line values appear on consecutive lines at the same nesting level, align their equals signs:\n\nami           = \"abc123\"\n\ninstance_type = \"t2.micro\"\n\n\nWhen both arguments and blocks appear together inside a block body, place all of the arguments together at the top and then place nested blocks below them. Use one blank line to separate the arguments from the blocks.\n\nUse empty lines to separate logical groups of arguments within a block.\n\nFor blocks that contain both arguments and \"meta-arguments\" (as defined by the Terraform language semantics), list meta-arguments first and separate them from other arguments with one blank line. Place meta-argument blocks last and separate them from other blocks with one blank line. Refer to dynamic resource count for more information on meta-arguments.\n\nresource \"aws_instance\" \"example\" {\n\n  # meta-argument first\n\n  count = 2\n\n\n\n  ami           = \"abc123\"\n\n  instance_type = \"t2.micro\"\n\n\n\n  network_interface {\n\n    # ...\n\n  }\n\n\n\n  # meta-argument block last\n\n  lifecycle {\n\n    create_before_destroy = true\n\n  }\n\n}\n\n\nTop-level blocks should always be separated from one another by one blank line. Nested blocks should also be separated by blank lines, except when grouping together related blocks of the same type (like multiple provisioner blocks in a resource).\n\nAvoid grouping multiple blocks of the same type with other blocks of a different type, unless the block types are defined by semantics to form a family. (For example: root_block_device, ebs_block_device and ephemeral_block_device on aws_instance form a family of block types describing AWS block devices, and can therefore be grouped together and mixed.)\n\nThe terraform fmt command formats your Terraform configuration to a subset of the above recommendations. By default, the terraform fmt command will only modify your Terraform code in the directory that you execute it in, but you can include the -recursive flag to modify code in all subdirectories as well.\n\nWe recommend that you run terraform fmt before each commit to version control. You can use mechanisms such as Git pre-commit hooks to automatically run this command each time you commit your code.\n\nIf you use Microsoft VS Code, use the Terraform VS Code extension to enable features such as syntax highlighting and validation, automatic code formatting, and integration with Terraform Cloud. If your development environment or text editor supports the Language Server Protocol, you can use the Terraform Language Server to access most of the VS Code extension features.\n\nCode validation\n\nThe terraform validate command checks that your configuration is syntactically valid and internally consistent. The validate command does not check if argument values are valid for a specific provider, but it will verify that they are the correct type. It does not evaluate any existing state.\n\nThe terraform validate command is safe to run automatically and frequently. You can configure your text editor to run this command as a post-save check, define it as a pre-commit hook in a Git repository, or run it as a step in a CI/CD pipeline.\n\nFor more information, refer to the Terraform validate documentation.\n\nFile names\n\nWe recommend the following file naming conventions:\n\nA backend.tf file that contains your backend configuration. You can define multiple terraform blocks in your configuration to separate your backend configuration from your Terraform and provider versioning configuration.\nA main.tf file that contains all resource and data source blocks.\nA outputs.tf file that contains all output blocks in alphabetical order.\nA providers.tf file that contains all provider blocks and configuration.\nA terraform.tf file that contains a single terraform block which defines your required_version and required_providers.\nA variables.tf file that contains all variable blocks in alphabetical order.\nA locals.tf file that contains local values. Refer to local values for more information.\nA override.tf file that contains override definitions for your configuration. Terraform loads this and all files ending with _override.tf last. Use them sparingly and add comments to the original resource definitions, as these overrides make your code harder to reason about. Refer to the override files documentation for more information.\n\nAs your codebase grows, limiting it to just these files can become difficult to maintain. If your code becomes hard to navigate due to its size, we recommend that you organize resources and data sources in separate files by logical groups. For example, if your web application requires networking, storage, and compute resources, you might create the following files:\n\nA network.tf file that contains your VPC, subnets, load balancers, and all other networking resources.\nA storage.tf file that contains your object storage and related permissions configuration.\nA compute.tf file that contains your compute instances.\n\nNo matter how you decide to split your code, it should be immediately clear where a maintainer can find a specific resource or data source definition.\n\nAs your configuration grows, you may need to separate it into multiple state files. The HashiCorp Well-Architected Framework provides more guidance about configuration structure and scope.\n\nLinting and static code analysis\n\nTerraform does not have a built-in linter, but many organizations rely on a third party linting tool such as TFLint to enforce code standards. A linter uses static code analysis to compare your Terraform code against a set of rules. Most linters ship with a default set of rules, but also let you write your own.\n\nComments\n\nWrite your code so it is easy to understand. Only when necessary, use comments to clarify complexity for other maintainers.\n\nUse # for both single- and multi-line comments. The // and /* */ comment syntaxes are not considered idiomatic, but Terraform supports them to remain backwards-compatible with earlier versions of HCL.\n\n# Each tunnel is responsible for encrypting and decrypting traffic exiting\n\n# and leaving its associated gateway.\n\nresource \"google_compute_vpn_tunnel\" \"tunnel1\" {\n\n  ## ...\n\nResource naming\n\nEvery resource within a configuration must have a unique name. For consistency and readability, use a descriptive noun and separate words with underscores. Do not include the resource type in the resource identifier since the resource address already includes it. Wrap the resource type and name in double quotes.\n\n❌ Bad:\n\nresource aws_instance webAPI-aws-instance {...}\n\n\n✅ Good:\n\nresource \"aws_instance\" \"web_api\" {...}\n\nResource order\n\nThe order of the resources and data sources in your code does not affect how Terraform builds them, so organize your resources for readability. Terraform determines the creation order based on cross-resource dependencies.\n\nHow you order your resources largely depends on the size and complexity of your code, but we recommend defining data sources alongside the resources that reference them. For readability, your Terraform code should “build on itself” — you should define a data source before the resource that references it.\n\nThe following example defines an aws_instance that relies on two data sources, aws_ami and aws_availability_zone. For readability and continuity, it defines the data sources before the aws_instance resource.\n\ndata \"aws_ami\" \"web\" {\n\n  ##...\n\n}\n\n\n\ndata \"aws_availability_zones\" \"available\" {\n\n  ##...\n\n}\n\n\n\nresource \"aws_instance\" \"web\" {\n\n  ami               = data.aws_ami.web.id\n\n  availability_zone = data.aws_availability_zones.available.names[0]\n\n  ##...\n\n}\n\n\nWe recommend following a consistent order for resource parameters:\n\nIf present, The count or for_each meta-argument.\nResource-specific non-block parameters.\nResource-specific block parameters.\nIf required, a lifecycle block.\nIf required, the depends_on parameter.\nVariables\n\nWhile variables make your modules more flexible, overusing variables can make code difficult to understand. When deciding whether to expose a variable for a resource setting, consider whether that parameter will change between deployments.\n\nDefine a type and a description for every variable.\n\nIf the variable is optional, define a reasonable default.\n\nFor sensitive variables, such as passwords and private keys, set the sensitive parameter to true. Remember that Terraform will still store this value in plain text in its state, but it will not display it when you run terraform plan or terraform apply. Refer to secrets management for more information on how to securely handle sensitive values.\n\nUse input variable validation to create additional rules for your variable values in addition to Terraform's type validation. Only use variable validation when your variable values have uniquely restrictive requirements. For example, if your Terraform configuration requires two web instances, add a validation block to enforce it:\n\nvariable \"web_instance_count\" {\n\n  type        = number\n\n  description = \"Number of web instances to deploy. This application requires at least two instances.\"\n\n\n\n  validation {\n\n    condition     = var.web_instance_count > 1\n\n    error_message = \"This application requires at least two web instances.\"\n\n  }\n\n}\n\n\nWe recommend following a consistent order for variable parameters:\n\nType\nDescription\nDefault (optional)\nSensitive (optional)\nValidation blocks\nOutputs\n\nOutput values let you expose data about your infrastructure on the command line and make it easy to reference in other Terraform configurations. Like you would for variables, provide a description for each output.\n\nWe recommend that you use the following order for your output parameters:\n\nDescription\nValue\nSensitive (optional)\n\nEvery variable and output requires a unique name. For consistency and readability, we recommend that you use a descriptive noun and separate words with underscores.\n\nvariable \"db_disk_size\" {\n\n type        = number\n\n description = \"Disk size for the API database\"\n\n default     = 100\n\n}\n\n\n\nvariable \"db_password\" {\n\n type        = string\n\n description = \"Database password\"\n\n sensitive   = true\n\n}\n\n\n\noutput \"web_public_ip\" {\n\n description = \"Public IP of the web instance\"\n\n value       = aws_instance.web.public_ip\n\n}\n\nLocal values\n\nLocal values let you reference an expression or value multiple times. Use local values sparingly, as overuse can make your code harder to understand.\n\nFor example, you can use a local value to create a suffix for the region and environment (for example, development or test), and append it to multiple resources.\n\nlocals {\n\n  name_suffix = \"${var.region}-${var.environment}\"\n\n}\n\n\n\nresource \"aws_instance\" \"web\" {\n\n  ami           = data.aws_ami.ubuntu.id\n\n  instance_type = \"t3.micro\"\n\n\n\n  tags = {\n\n    Name = \"web-${local.name_suffix}\"\n\n  }\n\n}\n\n\nDefine local values in one of two places:\n\nIf you reference the local value in multiple files, define it in a file named locals.tf.\nIf the local is specific to a file, define it at the top of that file.\n\nAs for other Terraform objects, use descriptive nouns for local value names and underscores to separate multiple words.\n\nFor more information, refer to the local values documentation and the Simplify Terraform configuration with locals tutorial.\n\nProvider aliasing\n\nProvider aliasing lets you define multiple provider blocks for the same Terraform provider. Potential use cases for aliases include provisioning resources in multiple regions within a single configuration. The provider meta-argument for resources and the providers meta-argument for modules specifies which provider to use.\n\nproviders.tf\nprovider \"aws\" {\n\n  region = \"us-east-1\"\n\n}\n\n\n\nprovider \"aws\" {\n\n  alias  = \"west\"\n\n  region = \"us-west-2\"\n\n}\n\nmain.tf\nresource \"aws_instance\" \"example\" {\n\n  provider = aws.west\n\n  # ...\n\n}\n\n\n\nmodule \"aws_vpc\" {\n\n  source = \"./aws_vpc\"\n\n  providers = {\n\n    aws = aws.west\n\n  }\n\n}\n\nAny provider block that does not define the alias parameter is the default provider configuration.\nAlways include a default provider configuration and define all of your providers in the same file.\nIf you define multiple instances of a provider, define the default first.\nFor non-default providers, define the alias as the first parameter of the provider block.\nDynamic resource count\n\nThe for_each and count meta-arguments let you create multiple resources from a single resource block depending on run-time conditions. You can use these meta-arguments to make your code flexible and reduce duplicate resource blocks. If the resources are almost identical, use count. If some of arguments need distinct values that you cannot derive from an integer, use for_each.\n\nThe for_each meta-argument accepts a map or set value, and Terraform will create an instance of that resource for each element in the value you provide. In the following example, Terraform creates an aws_instance for each of the strings defined in the web_instances variable: \"ui\", \"api\", \"db\" and \"metrics\". The example uses each.key to give each instance a unique name. The web_private_ips output uses a for expression to create a map of instance names and their private IP addresses, while the web_ui_public_ip output addresses the instance with the key \"ui\" directly.\n\nvariable \"web_instances\" {\n\n  type        = list(string)\n\n  description = \"A list of instances for the web application\"\n\n  default = [\n\n    \"ui\",\n\n    \"api\",\n\n    \"db\",\n\n    \"metrics\"\n\n  ]\n\n}\n\nresource \"aws_instance\" \"web\" {\n\n  for_each = toset(var.web_instances)\n\n  ami           = data.aws_ami.webapp.id\n\n  instance_type = \"t3.micro\"\n\n  tags = {\n\n    Name = \"web_${each.key}\"\n\n  }\n\n}\n\noutput \"web_private_ips\" {\n\n  description = \"Private IPs of the web instances\"\n\n  value = {\n\n    for k, v in aws_instance.web : k => v.private_ip\n\n  }\n\n}\n\noutput \"web_ui_public_ip\" {\n\n  description = \"Public IP of the web UI instance\"\n\n  value       = aws_instance.web[\"ui\"].public_ip\n\n}\n\n\nThe above example will create the following output:\n\nweb_private_ips = {\n\n  \"api\" = \"172.31.25.29\"\n\n  \"db\" = \"172.31.18.33\"\n\n  \"metrics\" = \"172.31.26.112\"\n\n  \"ui\" = \"172.31.20.142\"\n\n}\n\nweb_ui_public_ip = \"18.216.208.182\"\n\n\nRefer to the for_each meta-argument documentation for more examples.\n\nThe count meta-argument lets you create multiple instances of a resource from a single resource block. Refer to the count meta-argument documentation for examples.\n\nA common practice to conditionally create resources is to use the count meta-argument with a conditional expression. In the following example, Terraform will only create the aws_instance if var.enable_metrics is true.\n\nvariable \"enable_metrics\" {\n\n  description = \"True if the metrics server should be deployed\"\n\n  type        = bool\n\n  default     = true\n\n}\n\n\n\nresource \"aws_instance\" \"web\" {\n\n  count = var.enable_metrics ? 1 : 0\n\n\n\n  ami           = data.aws_ami.webapp.id\n\n  instance_type = \"t3.micro\"\n\n  ##...\n\n}\n\n\nMeta-arguments simplify your code but add complexity, so use them in moderation. If the effect of the meta-argument is not immediately obvious, use a comment for clarification.\n\nTo learn more about these meta-arguments, refer to the for_each and count documentation.\n\n.gitignore\n\nDefine a .gitignore file for your repository to exclude files that you should not publish to version control, such as your state file.\n\nDo not commit:\n\nYour terraform.tfstate state file, including terraform.tfstate.* backup state files.\nYour .terraform.tfstate.lock.info file. Terraform creates and deletes this file automatically when you run a terraform apply command and contains info about your state lock\nYour .terraform directory, where Terraform downloads providers and child modules. Saved plan files that you create when you include the -out flat when you run terraform plan.\nAny .tfvars files that contain sensitive information.\n\nAlways commit:\n\nAll Terraform code files\nYour .terraform.lock.hcl dependency lock file\nA .gitignore file that excludes the files listed below\nA README.md to describe the code, input variables, and outputs\n\nFor an example, refer to GitHub's Terraform .gitignore file.\n\nWorkflow style\n\nThis section reviews standards that enable predictable and secure Terraform workflows, such as:\n\nPin your Terraform, provider, and module versions.\nName your module repositories using this three-part name terraform-<PROVIDER>-<NAME> when using the Terraform Cloud registry.\nStore local modules at ./modules/<module_name>.\nUse the tfe_outputs data source or provider-specific data sources to share state between two state files.\nProtect credentials by using dynamic provider credentials or a secrets manager such as HashiCorp Vault.\nWrite tests for your modules.\nUse policy enforcement on Terraform Cloud to set guardrails for infrastructure operations.\nVersion pinning\n\nTo prevent providers and modules upgrades from introducing unintentional changes to your infrastructure, use version pinning.\n\nSpecify provider versions using the required_providers block. Terraform version constraints support a range of accepted versions.\n\nPin modules to a specific major and minor version as shown in the example below to ensure stability. You can use looser restrictions if you are certain that the module does not introduce breaking changes outside of major version updates.\n\nWe also recommend that you set a minimum required version of the Terraform binary using the required_version in your terraform block. This requires all operators to use a Terraform version that has all of your configuration's required features.\n\nterraform {\n\n  required_providers {\n\n    aws = {\n\n      source  = \"hashicorp/aws\"\n\n      version = \"5.34.0\"\n\n    }\n\n  }\n\n\n\n  required_version = \">= 1.7\"\n\n}\n\n\nThe above example pins the version of the hashicorp/aws provider to version 5.34.0, and requires that operators use Terraform 1.7 or newer.\n\nFor modules sourced from a registry, use the version parameter in the module block to pin the version. For local modules, Terraform ignores the version parameter.\n\nmodule \"vault_starter\" {\n\n  source  = \"hashicorp/vault-starter/aws\"\n\n  version = \"1.0.0\"\n\n  ##...\n\n}\n\nModule repository names\n\nThe Terraform registry requires that repositories match a naming convention for all modules that you publish to the registry. Module repositories must use this three-part name terraform-<PROVIDER>-<NAME>, where <NAME> reflects the type of infrastructure the module manages and <PROVIDER> is the main provider the module uses. The <NAME> segment can contain additional hyphens, for example, terraform-google-vault or terraform-aws-ec2-instance.\n\nModule structure\n\nTerraform modules define self-contained, reusable pieces of infrastructure-as-code.\n\nUse modules to group together logically related resources that you need to provision together. For example:\n\nA networking module that defines a VPC, along with its subnets, gateway, and security groups.\nAn application module defining all resources required for each deployment. This stack could include web servers, databases, storage, and supported networking.\n\nReview the module creation recommended pattern documentation and standard module structure for guidance on how to structure your modules.\n\nLocal modules\n\nLocal modules are sourced from local disk rather than a remote module registry. We recommend publishing your modules to a module registry, such as the Terraform Cloud private module registry, to easily version, share, and reuse modules across your organization. If you cannot use a module registry, using local modules can simplify maintaining and updating your code.\n\nWe recommend that you define child modules in the ./modules/<module_name> directory.\n\nRepository structure\n\nHow you structure your modules and Terraform configuration in version control significantly impacts versioning and operations. We recommend that you store your actual infrastructure configuration separately from your module code.\n\nStore each module in an individual repository. This lets you independently version each module and makes it easier to publish your modules in the private Terraform registry.\n\nOrganize your infrastructure configuration in repositories that group together logically-related resources. For example, a single repository for a web application that requires compute, networking, and database resources . By separating your resources into groups, you limit the number of resources that may be impacted by failures for any operation.\n\nAnother approach is to group all modules and infrastructure configuration into a single monolithic repository, or monorepo. For example, a monorepo may define a collection of local modules for each component of the infrastructure stack, and deploy them in the root module.\n\n.\n\n├── modules\n\n│   ├── function\n\n│   │   ├── main.tf      # contains aws_iam_role, aws_lambda_function\n\n│   │   ├── outputs.tf\n\n│   │   └── variables.tf\n\n│   ├── queue\n\n│   │   ├── main.tf      # contains aws_sqs_queue\n\n│   │   ├── outputs.tf\n\n│   │   └── variables.tf\n\n│   └── vpc\n\n│       ├── main.tf      # contains aws_vpc, aws_subnet\n\n│       ├── outputs.tf\n\n│       └── variables.tf\n\n├── main.tf\n\n├── outputs.tf\n\n└── variables.tf\n\n\nThe advantage of monolithic repositories is having a single source of truth that tracks every infrastructure change. However, monolithic repositories can complicate your CI/CD automation: since any code change triggers a deployment that operates on your entire repository, your workflow must target only the modified directories. You also lose the granular access control, since anyone with repository access can modify any file in it.\n\nIf your organization requires a monolithic approach, Terraform Cloud and Terraform Enterprise let you scope a workspace to a specific directory in a repository, simplifying your workflows.\n\nBranching strategy\n\nTo collaborate on your Terraform code, we recommend using the GitHub flow. This approach uses short-lived branches to help your team quickly review, test, and merge changes to your code. To make changes to your code, you would:\n\nCreate a new branch from your main branch\nWrite, commit, and push your changes to the new branch\nCreate a pull request\nReview the changes with your team\nMerge the pull request\nDelete the branch\n\nTerraform Cloud and Terraform Enterprise can run speculative plans for pull requests. These speculative plans run automatically when you create or update a pull request, and you can use them to see the effect that your changes will have on your infrastructure before you merge them to your main branch. When you merge your pull request, Terraform Cloud will start a new run to apply these changes.\n\nMultiple environments\n\nWe recommend that your repository's main branch be the source of truth for all environments. For Terraform Cloud and Terraform Enterprise users, we recommend that you use separate workspaces for each environment. For larger codebases, we recommend that you split your resources across multiple workspaces to prevent large state files and limit unintended consequences from changes. For example, you could structure your code as follows:\n\n.\n\n├── compute\n\n│   ├── main.tf\n\n│   ├── outputs.tf\n\n│   └── variables.tf\n\n├── database\n\n│   ├── main.tf\n\n│   ├── outputs.tf\n\n│   └── variables.tf\n\n└── networking\n\n    ├── main.tf\n\n    ├── outputs.tf\n\n    └── variables.tf\n\n\nIn this scenario, you would create three workspaces per environment. For example, your production environment would have a prod-compute, prod-database, and prod-networking workspace. Read more about Terraform workspace and project best practices.\n\nIf you do not use Terraform Cloud or Terraform Enterprise, we recommend that you use modules to encapsulate your configuration, and use a directory for each environment so that each one has a separate state file. The configuration in each of these directories would call the local modules, each with parameters specific to their environment. This also lets you maintain separate variable and backend configurations for each environment.\n\n├── modules\n\n│   ├── compute\n\n│   │   └── main.tf\n\n│   ├── database\n\n│   │   └── main.tf\n\n│   └── network\n\n│       └── main.tf\n\n├── dev\n\n│   ├── backend.tf\n\n│   ├── main.tf\n\n│   └── variables.tf\n\n├── prod\n\n│   ├── backend.tf\n\n│   ├── main.tf\n\n│   └── variables.tf\n\n└── staging\n\n    ├── backend.tf\n\n    ├── main.tf\n\n    └── variables.tf\n\nState sharing\n\nSince your state contains sensitive information, avoid sharing full state files when possible.\n\nIf you use Terraform Cloud or Terraform Enterprise and need to reference resources across workspaces, use the tfe_outputs data source.\n\nIf you do not use Terraform Cloud or Terraform Enterprise but still need to reference data about other infrastructure resources, use data sources to query the provider. For example, you can use the aws_instance data source to look up an AWS EC2 instance by its ID or tags.\n\nSecrets management\n\nIf you do not configure remote state storage, the Terraform CLI stores the entire state in plaintext on the local disk. State can include sensitive data, such as passwords and private keys. Terraform Cloud and Terraform Enterprise provide state encryption through HashiCorp Vault.\n\nIf you use Terraform Cloud or Terraform Enterprise, we recommend the following:\n\nWhen using Terraform Enterprise, define and enforce a Sentinel policy to prevent use of the local_exec provisioner or external data sources.\nWhen using Terraform Cloud or Terraform Enterprise, use dynamic provider credentials to avoid using long-lived static credentials.\n\nIf you use Terraform Community Edition, we recommend the following:\n\nConfigure provider credentials using provider-specific environment variables.\nAccess secrets from a secrets management system such as HashiCorp Vault with the Terraform Vault provider. Be aware that Terraform will still write these values in plaintext to your state file.\n\nIf you use a custom CI/CD pipeline, review your CI/CD tool's best practices for managing sensitive values. Most tools let you access sensitive values as environment variables. For more information, refer to your CI/CD documentation.\n\nUsing secrets in GitHub Actions\nGitlab pipeline security\nIntegrate Vault into your CI/CD pipeline\nIntegration and unit testing\n\nTerraform tests let you validate your modules and catch breaking changes. We recommend that you write tests for your Terraform modules and run them just as you run your tests for your application code, such as pre-merge check in your pull requests or as a prerequisite step in your automated CI/CD pipeline.\n\nTests differ from validation methods such as variable validation, preconditions, postconditions, and check blocks. These features focus on verifying the infrastructure deployed by your code, while tests validate the behavior and logic of your code itself. For more information, refer to the Terraform test documentation and the Write Terraform tests tutorial.\n\nPolicy\n\nPolicies are rules that Terraform Cloud enforces on Terraform runs. You can use policies to validate that the Terraform plan complies with your organization's best practices. For example, you can write policies that:\n\nLimit the size of a web instance\nCheck for required resource tags\nBlock deployments on Fridays\nEnforce security configuration and cost management\n\nWe recommend that you store policies in a separate VCS repository from your Terraform code.\n\nFor more information, refer to the policy enforcement documentation, as well as the enforce policy with Sential and detect infrastructure drift and enforce OPA policies tutorials.\n\nNext steps\n\nThis article introduces some considerations to keep in mind as you standardize your organization's Terraform style guidelines. Enforcing a standard way of writing and organizing your Terraform code across your organization ensures that it is readable, maintainable, and shareable.\n\nThe HashiCorp Well-Architected Framework provides more guidance on adapting your Terraform workflows for scale.\n\nOn this page:\n\nStyle Guide\nCode style\nCode formatting\nCode validation\nFile names\nLinting and static code analysis\nComments\nResource naming\nResource order\nVariables\nOutputs\nLocal values\nProvider aliasing\nDynamic resource count\n.gitignore\nWorkflow style\nVersion pinning\nModule repository names\nModule structure\nLocal modules\nRepository structure\nBranching strategy\nMultiple environments\nState sharing\nSecrets management\nIntegration and unit testing\nPolicy\nNext steps\nTheme\nDark\nLight\nSystem\nCertifications\nSystem Status\nCookie Manager\nTerms of Use\nSecurity\nPrivacy\nTrademark Policy\nTrade Controls\nGive Feedback\n(opens in new tab)\nWe use cookies & other similar technology to collect data to improve your experience on our site, as described in our Privacy Policy and Cookie Policy.\nManage PreferencesACCEPT"
  },
  {
    "title": "Style Guide - Configuration Language | Terraform | HashiCorp Developer",
    "url": "https://developer.hashicorp.com/terraform/language/v1.9.x/style",
    "html": "Skip to main content\nNews\nHashiCorp to join IBM to accelerate multi-cloud automation.\nRead the blog →\nDismiss alert\nTerraform\nInstall\nTutorials\nDocumentation\nRegistry\n(opens in new tab)\nTry Cloud\n(opens in new tab)\nSearch\nCommand or control key\nK key\nTerraform Home\nConfiguration Language\nConfiguration Language\nTerraform Language\nOverview\nStyle Guide\nFiles and Directories\nSyntax\nResources\nData Sources\nProviders\nVariables and Outputs\nModules\nChecks\nImport\nExpressions\nFunctions\nTerraform Settings\nState\nTests\nUpgrading to Terraform v1.7\nv1.x Compatibility Promises\nTerraform Internals\nResources\nTutorial Library\nCertifications\nCommunity Forum\n(opens in new tab)\nSupport\n(opens in new tab)\nGitHub\n(opens in new tab)\nTerraform Registry\n(opens in new tab)\n\nYou are viewing documentation for pre-release version v1.9.0 (alpha). View latest version.\n\nDeveloper\nTerraform\nConfiguration Language v1.9.x\nStyle Guide\nv1.9.0 (alpha)\nStyle Guide\n\nThe flexibility of Terraform's configuration language gives you many options to choose from as you write your code, structure your directories, and test your configuration. While some design decisions depend on your organization's needs or preferences, there are some common patterns that we suggest you adopt. Adopting and adhering to a style guide keeps your Terraform code legible, scalable, and maintainable.\n\nThis article discusses best practices and some considerations to keep in mind as you develop your organization's style guide. The article is split into two sections. The first section covers code style recommendations, such as formatting and resource organization. The second section covers operations and workflow recommendations, such as lifecycle management through meta-arguments, versioning, and sensitive data management.\n\nCode style\n\nWriting Terraform code in a consistent style makes it easier to read and maintain. The following sections discuss code style recommendations, including the following:\n\nRun terraform fmt and terraform validate before committing your code to version control.\nUse a linter such as TFLint to enforce your organization's own coding best practices.\nUse # for single and multi-line comments.\nUse nouns for resource names and do not include the resource type in the name.\nUse underscores to separate multiple words in names. Wrap the resource type and name in double quotes in your resource definition.\nLet your code build on itself: define dependent resources after the resources that reference them.\nInclude a type and description for every variable.\nInclude a description for every output.\nAvoid overuse of variables and local values.\nAlways include a default provider configuration.\nUse count and for_each sparingly.\nCode formatting\n\nThe Terraform parser allows you some flexibility in how you lay out the elements in your configuration files, but the Terraform language also has some idiomatic style conventions which we recommend users always follow for consistency between files and modules written by different teams.\n\nIndent two spaces for each nesting level\n\nWhen multiple arguments with single-line values appear on consecutive lines at the same nesting level, align their equals signs:\n\nami           = \"abc123\"\n\ninstance_type = \"t2.micro\"\n\n\nWhen both arguments and blocks appear together inside a block body, place all of the arguments together at the top and then place nested blocks below them. Use one blank line to separate the arguments from the blocks.\n\nUse empty lines to separate logical groups of arguments within a block.\n\nFor blocks that contain both arguments and \"meta-arguments\" (as defined by the Terraform language semantics), list meta-arguments first and separate them from other arguments with one blank line. Place meta-argument blocks last and separate them from other blocks with one blank line. Refer to dynamic resource count for more information on meta-arguments.\n\nresource \"aws_instance\" \"example\" {\n\n  # meta-argument first\n\n  count = 2\n\n\n\n  ami           = \"abc123\"\n\n  instance_type = \"t2.micro\"\n\n\n\n  network_interface {\n\n    # ...\n\n  }\n\n\n\n  # meta-argument block last\n\n  lifecycle {\n\n    create_before_destroy = true\n\n  }\n\n}\n\n\nTop-level blocks should always be separated from one another by one blank line. Nested blocks should also be separated by blank lines, except when grouping together related blocks of the same type (like multiple provisioner blocks in a resource).\n\nAvoid grouping multiple blocks of the same type with other blocks of a different type, unless the block types are defined by semantics to form a family. (For example: root_block_device, ebs_block_device and ephemeral_block_device on aws_instance form a family of block types describing AWS block devices, and can therefore be grouped together and mixed.)\n\nThe terraform fmt command formats your Terraform configuration to a subset of the above recommendations. By default, the terraform fmt command will only modify your Terraform code in the directory that you execute it in, but you can include the -recursive flag to modify code in all subdirectories as well.\n\nWe recommend that you run terraform fmt before each commit to version control. You can use mechanisms such as Git pre-commit hooks to automatically run this command each time you commit your code.\n\nIf you use Microsoft VS Code, use the Terraform VS Code extension to enable features such as syntax highlighting and validation, automatic code formatting, and integration with HCP Terraform. If your development environment or text editor supports the Language Server Protocol, you can use the Terraform Language Server to access most of the VS Code extension features.\n\nCode validation\n\nThe terraform validate command checks that your configuration is syntactically valid and internally consistent. The validate command does not check if argument values are valid for a specific provider, but it will verify that they are the correct type. It does not evaluate any existing state.\n\nThe terraform validate command is safe to run automatically and frequently. You can configure your text editor to run this command as a post-save check, define it as a pre-commit hook in a Git repository, or run it as a step in a CI/CD pipeline.\n\nFor more information, refer to the Terraform validate documentation.\n\nFile names\n\nWe recommend the following file naming conventions:\n\nA backend.tf file that contains your backend configuration. You can define multiple terraform blocks in your configuration to separate your backend configuration from your Terraform and provider versioning configuration.\nA main.tf file that contains all resource and data source blocks.\nA outputs.tf file that contains all output blocks in alphabetical order.\nA providers.tf file that contains all provider blocks and configuration.\nA terraform.tf file that contains a single terraform block which defines your required_version and required_providers.\nA variables.tf file that contains all variable blocks in alphabetical order.\nA locals.tf file that contains local values. Refer to local values for more information.\nA override.tf file that contains override definitions for your configuration. Terraform loads this and all files ending with _override.tf last. Use them sparingly and add comments to the original resource definitions, as these overrides make your code harder to reason about. Refer to the override files documentation for more information.\n\nAs your codebase grows, limiting it to just these files can become difficult to maintain. If your code becomes hard to navigate due to its size, we recommend that you organize resources and data sources in separate files by logical groups. For example, if your web application requires networking, storage, and compute resources, you might create the following files:\n\nA network.tf file that contains your VPC, subnets, load balancers, and all other networking resources.\nA storage.tf file that contains your object storage and related permissions configuration.\nA compute.tf file that contains your compute instances.\n\nNo matter how you decide to split your code, it should be immediately clear where a maintainer can find a specific resource or data source definition.\n\nAs your configuration grows, you may need to separate it into multiple state files. The HashiCorp Well-Architected Framework provides more guidance about configuration structure and scope.\n\nLinting and static code analysis\n\nTerraform does not have a built-in linter, but many organizations rely on a third party linting tool such as TFLint to enforce code standards. A linter uses static code analysis to compare your Terraform code against a set of rules. Most linters ship with a default set of rules, but also let you write your own.\n\nComments\n\nWrite your code so it is easy to understand. Only when necessary, use comments to clarify complexity for other maintainers.\n\nUse # for both single- and multi-line comments. The // and /* */ comment syntaxes are not considered idiomatic, but Terraform supports them to remain backwards-compatible with earlier versions of HCL.\n\n# Each tunnel is responsible for encrypting and decrypting traffic exiting\n\n# and leaving its associated gateway.\n\nresource \"google_compute_vpn_tunnel\" \"tunnel1\" {\n\n  ## ...\n\nResource naming\n\nEvery resource within a configuration must have a unique name. For consistency and readability, use a descriptive noun and separate words with underscores. Do not include the resource type in the resource identifier since the resource address already includes it. Wrap the resource type and name in double quotes.\n\n❌ Bad:\n\nresource aws_instance webAPI-aws-instance {...}\n\n\n✅ Good:\n\nresource \"aws_instance\" \"web_api\" {...}\n\nResource order\n\nThe order of the resources and data sources in your code does not affect how Terraform builds them, so organize your resources for readability. Terraform determines the creation order based on cross-resource dependencies.\n\nHow you order your resources largely depends on the size and complexity of your code, but we recommend defining data sources alongside the resources that reference them. For readability, your Terraform code should “build on itself” — you should define a data source before the resource that references it.\n\nThe following example defines an aws_instance that relies on two data sources, aws_ami and aws_availability_zone. For readability and continuity, it defines the data sources before the aws_instance resource.\n\ndata \"aws_ami\" \"web\" {\n\n  ##...\n\n}\n\n\n\ndata \"aws_availability_zones\" \"available\" {\n\n  ##...\n\n}\n\n\n\nresource \"aws_instance\" \"web\" {\n\n  ami               = data.aws_ami.web.id\n\n  availability_zone = data.aws_availability_zones.available.names[0]\n\n  ##...\n\n}\n\n\nWe recommend following a consistent order for resource parameters:\n\nIf present, The count or for_each meta-argument.\nResource-specific non-block parameters.\nResource-specific block parameters.\nIf required, a lifecycle block.\nIf required, the depends_on parameter.\nVariables\n\nWhile variables make your modules more flexible, overusing variables can make code difficult to understand. When deciding whether to expose a variable for a resource setting, consider whether that parameter will change between deployments.\n\nDefine a type and a description for every variable.\n\nIf the variable is optional, define a reasonable default.\n\nFor sensitive variables, such as passwords and private keys, set the sensitive parameter to true. Remember that Terraform will still store this value in plain text in its state, but it will not display it when you run terraform plan or terraform apply. Refer to secrets management for more information on how to securely handle sensitive values.\n\nUse input variable validation to create additional rules for your variable values in addition to Terraform's type validation. Only use variable validation when your variable values have uniquely restrictive requirements. For example, if your Terraform configuration requires two web instances, add a validation block to enforce it:\n\nvariable \"web_instance_count\" {\n\n  type        = number\n\n  description = \"Number of web instances to deploy. This application requires at least two instances.\"\n\n\n\n  validation {\n\n    condition     = var.web_instance_count > 1\n\n    error_message = \"This application requires at least two web instances.\"\n\n  }\n\n}\n\n\nWe recommend following a consistent order for variable parameters:\n\nType\nDescription\nDefault (optional)\nSensitive (optional)\nValidation blocks\nOutputs\n\nOutput values let you expose data about your infrastructure on the command line and make it easy to reference in other Terraform configurations. Like you would for variables, provide a description for each output.\n\nWe recommend that you use the following order for your output parameters:\n\nDescription\nValue\nSensitive (optional)\n\nEvery variable and output requires a unique name. For consistency and readability, we recommend that you use a descriptive noun and separate words with underscores.\n\nvariable \"db_disk_size\" {\n\n type        = number\n\n description = \"Disk size for the API database\"\n\n default     = 100\n\n}\n\n\n\nvariable \"db_password\" {\n\n type        = string\n\n description = \"Database password\"\n\n sensitive   = true\n\n}\n\n\n\noutput \"web_public_ip\" {\n\n description = \"Public IP of the web instance\"\n\n value       = aws_instance.web.public_ip\n\n}\n\nLocal values\n\nLocal values let you reference an expression or value multiple times. Use local values sparingly, as overuse can make your code harder to understand.\n\nFor example, you can use a local value to create a suffix for the region and environment (for example, development or test), and append it to multiple resources.\n\nlocals {\n\n  name_suffix = \"${var.region}-${var.environment}\"\n\n}\n\n\n\nresource \"aws_instance\" \"web\" {\n\n  ami           = data.aws_ami.ubuntu.id\n\n  instance_type = \"t3.micro\"\n\n\n\n  tags = {\n\n    Name = \"web-${local.name_suffix}\"\n\n  }\n\n}\n\n\nDefine local values in one of two places:\n\nIf you reference the local value in multiple files, define it in a file named locals.tf.\nIf the local is specific to a file, define it at the top of that file.\n\nAs for other Terraform objects, use descriptive nouns for local value names and underscores to separate multiple words.\n\nFor more information, refer to the local values documentation and the Simplify Terraform configuration with locals tutorial.\n\nProvider aliasing\n\nProvider aliasing lets you define multiple provider blocks for the same Terraform provider. Potential use cases for aliases include provisioning resources in multiple regions within a single configuration. The provider meta-argument for resources and the providers meta-argument for modules specifies which provider to use.\n\nproviders.tf\nprovider \"aws\" {\n\n  region = \"us-east-1\"\n\n}\n\n\n\nprovider \"aws\" {\n\n  alias  = \"west\"\n\n  region = \"us-west-2\"\n\n}\n\nmain.tf\nresource \"aws_instance\" \"example\" {\n\n  provider = aws.west\n\n  # ...\n\n}\n\n\n\nmodule \"aws_vpc\" {\n\n  source = \"./aws_vpc\"\n\n  providers = {\n\n    aws = aws.west\n\n  }\n\n}\n\nAny provider block that does not define the alias parameter is the default provider configuration.\nAlways include a default provider configuration and define all of your providers in the same file.\nIf you define multiple instances of a provider, define the default first.\nFor non-default providers, define the alias as the first parameter of the provider block.\nDynamic resource count\n\nThe for_each and count meta-arguments let you create multiple resources from a single resource block depending on run-time conditions. You can use these meta-arguments to make your code flexible and reduce duplicate resource blocks. If the resources are almost identical, use count. If some of arguments need distinct values that you cannot derive from an integer, use for_each.\n\nThe for_each meta-argument accepts a map or set value, and Terraform will create an instance of that resource for each element in the value you provide. In the following example, Terraform creates an aws_instance for each of the strings defined in the web_instances variable: \"ui\", \"api\", \"db\" and \"metrics\". The example uses each.key to give each instance a unique name. The web_private_ips output uses a for expression to create a map of instance names and their private IP addresses, while the web_ui_public_ip output addresses the instance with the key \"ui\" directly.\n\nvariable \"web_instances\" {\n\n  type        = list(string)\n\n  description = \"A list of instances for the web application\"\n\n  default = [\n\n    \"ui\",\n\n    \"api\",\n\n    \"db\",\n\n    \"metrics\"\n\n  ]\n\n}\n\nresource \"aws_instance\" \"web\" {\n\n  for_each = toset(var.web_instances)\n\n  ami           = data.aws_ami.webapp.id\n\n  instance_type = \"t3.micro\"\n\n  tags = {\n\n    Name = \"web_${each.key}\"\n\n  }\n\n}\n\noutput \"web_private_ips\" {\n\n  description = \"Private IPs of the web instances\"\n\n  value = {\n\n    for k, v in aws_instance.web : k => v.private_ip\n\n  }\n\n}\n\noutput \"web_ui_public_ip\" {\n\n  description = \"Public IP of the web UI instance\"\n\n  value       = aws_instance.web[\"ui\"].public_ip\n\n}\n\n\nThe above example will create the following output:\n\nweb_private_ips = {\n\n  \"api\" = \"172.31.25.29\"\n\n  \"db\" = \"172.31.18.33\"\n\n  \"metrics\" = \"172.31.26.112\"\n\n  \"ui\" = \"172.31.20.142\"\n\n}\n\nweb_ui_public_ip = \"18.216.208.182\"\n\n\nRefer to the for_each meta-argument documentation for more examples.\n\nThe count meta-argument lets you create multiple instances of a resource from a single resource block. Refer to the count meta-argument documentation for examples.\n\nA common practice to conditionally create resources is to use the count meta-argument with a conditional expression. In the following example, Terraform will only create the aws_instance if var.enable_metrics is true.\n\nvariable \"enable_metrics\" {\n\n  description = \"True if the metrics server should be deployed\"\n\n  type        = bool\n\n  default     = true\n\n}\n\n\n\nresource \"aws_instance\" \"web\" {\n\n  count = var.enable_metrics ? 1 : 0\n\n\n\n  ami           = data.aws_ami.webapp.id\n\n  instance_type = \"t3.micro\"\n\n  ##...\n\n}\n\n\nMeta-arguments simplify your code but add complexity, so use them in moderation. If the effect of the meta-argument is not immediately obvious, use a comment for clarification.\n\nTo learn more about these meta-arguments, refer to the for_each and count documentation.\n\n.gitignore\n\nDefine a .gitignore file for your repository to exclude files that you should not publish to version control, such as your state file.\n\nDo not commit:\n\nYour terraform.tfstate state file, including terraform.tfstate.* backup state files.\nYour .terraform.tfstate.lock.info file. Terraform creates and deletes this file automatically when you run a terraform apply command and contains info about your state lock\nYour .terraform directory, where Terraform downloads providers and child modules. Saved plan files that you create when you include the -out flag when you run terraform plan.\nAny .tfvars files that contain sensitive information.\n\nAlways commit:\n\nAll Terraform code files\nYour .terraform.lock.hcl dependency lock file\nA .gitignore file that excludes the files listed below\nA README.md to describe the code, input variables, and outputs\n\nFor an example, refer to GitHub's Terraform .gitignore file.\n\nWorkflow style\n\nThis section reviews standards that enable predictable and secure Terraform workflows, such as:\n\nPin your Terraform, provider, and module versions.\nName your module repositories using this three-part name terraform-<PROVIDER>-<NAME> when using the HCP Terraform registry.\nStore local modules at ./modules/<module_name>.\nUse the tfe_outputs data source or provider-specific data sources to share state between two state files.\nProtect credentials by using dynamic provider credentials or a secrets manager such as HashiCorp Vault.\nWrite tests for your modules.\nUse policy enforcement on HCP Terraform to set guardrails for infrastructure operations.\nVersion pinning\n\nTo prevent providers and modules upgrades from introducing unintentional changes to your infrastructure, use version pinning.\n\nSpecify provider versions using the required_providers block. Terraform version constraints support a range of accepted versions.\n\nPin modules to a specific major and minor version as shown in the example below to ensure stability. You can use looser restrictions if you are certain that the module does not introduce breaking changes outside of major version updates.\n\nWe also recommend that you set a minimum required version of the Terraform binary using the required_version in your terraform block. This requires all operators to use a Terraform version that has all of your configuration's required features.\n\nterraform {\n\n  required_providers {\n\n    aws = {\n\n      source  = \"hashicorp/aws\"\n\n      version = \"5.34.0\"\n\n    }\n\n  }\n\n\n\n  required_version = \">= 1.7\"\n\n}\n\n\nThe above example pins the version of the hashicorp/aws provider to version 5.34.0, and requires that operators use Terraform 1.7 or newer.\n\nFor modules sourced from a registry, use the version parameter in the module block to pin the version. For local modules, Terraform ignores the version parameter.\n\nmodule \"vault_starter\" {\n\n  source  = \"hashicorp/vault-starter/aws\"\n\n  version = \"1.0.0\"\n\n  ##...\n\n}\n\nModule repository names\n\nThe Terraform registry requires that repositories match a naming convention for all modules that you publish to the registry. Module repositories must use this three-part name terraform-<PROVIDER>-<NAME>, where <NAME> reflects the type of infrastructure the module manages and <PROVIDER> is the main provider the module uses. The <NAME> segment can contain additional hyphens, for example, terraform-google-vault or terraform-aws-ec2-instance.\n\nModule structure\n\nTerraform modules define self-contained, reusable pieces of infrastructure-as-code.\n\nUse modules to group together logically related resources that you need to provision together. For example:\n\nA networking module that defines a VPC, along with its subnets, gateway, and security groups.\nAn application module defining all resources required for each deployment. This stack could include web servers, databases, storage, and supported networking.\n\nReview the module creation recommended pattern documentation and standard module structure for guidance on how to structure your modules.\n\nLocal modules\n\nLocal modules are sourced from local disk rather than a remote module registry. We recommend publishing your modules to a module registry, such as the HCP Terraform private registry, to easily version, share, and reuse modules across your organization. If you cannot use a module registry, using local modules can simplify maintaining and updating your code.\n\nWe recommend that you define child modules in the ./modules/<module_name> directory.\n\nRepository structure\n\nHow you structure your modules and Terraform configuration in version control significantly impacts versioning and operations. We recommend that you store your actual infrastructure configuration separately from your module code.\n\nStore each module in an individual repository. This lets you independently version each module and makes it easier to publish your modules in the private Terraform registry.\n\nOrganize your infrastructure configuration in repositories that group together logically-related resources. For example, a single repository for a web application that requires compute, networking, and database resources . By separating your resources into groups, you limit the number of resources that may be impacted by failures for any operation.\n\nAnother approach is to group all modules and infrastructure configuration into a single monolithic repository, or monorepo. For example, a monorepo may define a collection of local modules for each component of the infrastructure stack, and deploy them in the root module.\n\n.\n\n├── modules\n\n│   ├── function\n\n│   │   ├── main.tf      # contains aws_iam_role, aws_lambda_function\n\n│   │   ├── outputs.tf\n\n│   │   └── variables.tf\n\n│   ├── queue\n\n│   │   ├── main.tf      # contains aws_sqs_queue\n\n│   │   ├── outputs.tf\n\n│   │   └── variables.tf\n\n│   └── vpc\n\n│       ├── main.tf      # contains aws_vpc, aws_subnet\n\n│       ├── outputs.tf\n\n│       └── variables.tf\n\n├── main.tf\n\n├── outputs.tf\n\n└── variables.tf\n\n\nThe advantage of monolithic repositories is having a single source of truth that tracks every infrastructure change. However, monolithic repositories can complicate your CI/CD automation: since any code change triggers a deployment that operates on your entire repository, your workflow must target only the modified directories. You also lose the granular access control, since anyone with repository access can modify any file in it.\n\nIf your organization requires a monolithic approach, HCP Terraform and Terraform Enterprise let you scope a workspace to a specific directory in a repository, simplifying your workflows.\n\nBranching strategy\n\nTo collaborate on your Terraform code, we recommend using the GitHub flow. This approach uses short-lived branches to help your team quickly review, test, and merge changes to your code. To make changes to your code, you would:\n\nCreate a new branch from your main branch\nWrite, commit, and push your changes to the new branch\nCreate a pull request\nReview the changes with your team\nMerge the pull request\nDelete the branch\n\nHCP Terraform and Terraform Enterprise can run speculative plans for pull requests. These speculative plans run automatically when you create or update a pull request, and you can use them to see the effect that your changes will have on your infrastructure before you merge them to your main branch. When you merge your pull request, HCP Terraform will start a new run to apply these changes.\n\nMultiple environments\n\nWe recommend that your repository's main branch be the source of truth for all environments. For HCP Terraform and Terraform Enterprise users, we recommend that you use separate workspaces for each environment. For larger codebases, we recommend that you split your resources across multiple workspaces to prevent large state files and limit unintended consequences from changes. For example, you could structure your code as follows:\n\n.\n\n├── compute\n\n│   ├── main.tf\n\n│   ├── outputs.tf\n\n│   └── variables.tf\n\n├── database\n\n│   ├── main.tf\n\n│   ├── outputs.tf\n\n│   └── variables.tf\n\n└── networking\n\n    ├── main.tf\n\n    ├── outputs.tf\n\n    └── variables.tf\n\n\nIn this scenario, you would create three workspaces per environment. For example, your production environment would have a prod-compute, prod-database, and prod-networking workspace. Read more about Terraform workspace and project best practices.\n\nIf you do not use HCP Terraform or Terraform Enterprise, we recommend that you use modules to encapsulate your configuration, and use a directory for each environment so that each one has a separate state file. The configuration in each of these directories would call the local modules, each with parameters specific to their environment. This also lets you maintain separate variable and backend configurations for each environment.\n\n├── modules\n\n│   ├── compute\n\n│   │   └── main.tf\n\n│   ├── database\n\n│   │   └── main.tf\n\n│   └── network\n\n│       └── main.tf\n\n├── dev\n\n│   ├── backend.tf\n\n│   ├── main.tf\n\n│   └── variables.tf\n\n├── prod\n\n│   ├── backend.tf\n\n│   ├── main.tf\n\n│   └── variables.tf\n\n└── staging\n\n    ├── backend.tf\n\n    ├── main.tf\n\n    └── variables.tf\n\nState sharing\n\nSince your state contains sensitive information, avoid sharing full state files when possible.\n\nIf you use HCP Terraform or Terraform Enterprise and need to reference resources across workspaces, use the tfe_outputs data source.\n\nIf you do not use HCP Terraform or Terraform Enterprise but still need to reference data about other infrastructure resources, use data sources to query the provider. For example, you can use the aws_instance data source to look up an AWS EC2 instance by its ID or tags.\n\nSecrets management\n\nIf you do not configure remote state storage, the Terraform CLI stores the entire state in plaintext on the local disk. State can include sensitive data, such as passwords and private keys. HCP Terraform and Terraform Enterprise provide state encryption through HashiCorp Vault.\n\nIf you use HCP Terraform or Terraform Enterprise, we recommend the following:\n\nWhen using Terraform Enterprise, define and enforce a Sentinel policy to prevent use of the local_exec provisioner or external data sources.\nWhen using HCP Terraform or Terraform Enterprise, use dynamic provider credentials to avoid using long-lived static credentials.\n\nIf you use Terraform Community Edition, we recommend the following:\n\nConfigure provider credentials using provider-specific environment variables.\nAccess secrets from a secrets management system such as HashiCorp Vault with the Terraform Vault provider. Be aware that Terraform will still write these values in plaintext to your state file.\n\nIf you use a custom CI/CD pipeline, review your CI/CD tool's best practices for managing sensitive values. Most tools let you access sensitive values as environment variables. For more information, refer to your CI/CD documentation.\n\nUsing secrets in GitHub Actions\nGitlab pipeline security\nIntegrate Vault into your CI/CD pipeline\nIntegration and unit testing\n\nTerraform tests let you validate your modules and catch breaking changes. We recommend that you write tests for your Terraform modules and run them just as you run your tests for your application code, such as pre-merge check in your pull requests or as a prerequisite step in your automated CI/CD pipeline.\n\nTests differ from validation methods such as variable validation, preconditions, postconditions, and check blocks. These features focus on verifying the infrastructure deployed by your code, while tests validate the behavior and logic of your code itself. For more information, refer to the Terraform test documentation and the Write Terraform tests tutorial.\n\nPolicy\n\nPolicies are rules that HCP Terraform enforces on Terraform runs. You can use policies to validate that the Terraform plan complies with your organization's best practices. For example, you can write policies that:\n\nLimit the size of a web instance\nCheck for required resource tags\nBlock deployments on Fridays\nEnforce security configuration and cost management\n\nWe recommend that you store policies in a separate VCS repository from your Terraform code.\n\nFor more information, refer to the policy enforcement documentation, as well as the enforce policy with Sentinel and detect infrastructure drift and enforce OPA policies tutorials.\n\nNext steps\n\nThis article introduces some considerations to keep in mind as you standardize your organization's Terraform style guidelines. Enforcing a standard way of writing and organizing your Terraform code across your organization ensures that it is readable, maintainable, and shareable.\n\nThe HashiCorp Well-Architected Framework provides more guidance on adapting your Terraform workflows for scale.\n\nOn this page:\n\nStyle Guide\nCode style\nCode formatting\nCode validation\nFile names\nLinting and static code analysis\nComments\nResource naming\nResource order\nVariables\nOutputs\nLocal values\nProvider aliasing\nDynamic resource count\n.gitignore\nWorkflow style\nVersion pinning\nModule repository names\nModule structure\nLocal modules\nRepository structure\nBranching strategy\nMultiple environments\nState sharing\nSecrets management\nIntegration and unit testing\nPolicy\nNext steps\nTheme\nDark\nLight\nSystem\nCertifications\nSystem Status\nCookie Manager\nTerms of Use\nSecurity\nPrivacy\nTrademark Policy\nTrade Controls\nGive Feedback\n(opens in new tab)\nWe use cookies & other similar technology to collect data to improve your experience on our site, as described in our Privacy Policy and Cookie Policy.\nManage PreferencesACCEPT"
  },
  {
    "title": "Providers - Configuration Language | Terraform | HashiCorp Developer",
    "url": "https://developer.hashicorp.com/terraform/language/v1.2.x/providers",
    "html": "Skip to main content\nNews\nHashiCorp to join IBM to accelerate multi-cloud automation.\nRead the blog →\nDismiss alert\nTerraform\nInstall\nTutorials\nDocumentation\nRegistry\n(opens in new tab)\nTry Cloud\n(opens in new tab)\nSearch\nCommand or control key\nK key\nTerraform Home\nConfiguration Language\nConfiguration Language\nTerraform Language\nOverview\nStyle Guide\nFiles and Directories\nSyntax\nResources\nData Sources\nProviders\nOverview\nProvider Configuration\nProvider Requirements\nDependency Lock File\nVariables and Outputs\nModules\nExpressions\nFunctions\nTerraform Settings\nState\nUpgrading to Terraform v1.2\nv1.x Compatibility Promises\nTerraform Internals\nResources\nTutorial Library\nCertifications\nCommunity Forum\n(opens in new tab)\nSupport\n(opens in new tab)\nGitHub\n(opens in new tab)\nTerraform Registry\n(opens in new tab)\n\nYou are viewing documentation for version v1.2.x. View latest version.\n\nDeveloper\nTerraform\nConfiguration Language v1.2.x\nProviders\nv1.2.x\nProviders\n\nHands-on: Try the Perform CRUD Operations with Providers tutorial.\n\nTerraform relies on plugins called providers to interact with cloud providers, SaaS providers, and other APIs.\n\nTerraform configurations must declare which providers they require so that Terraform can install and use them. Additionally, some providers require configuration (like endpoint URLs or cloud regions) before they can be used.\n\nWhat Providers Do\n\nEach provider adds a set of resource types and/or data sources that Terraform can manage.\n\nEvery resource type is implemented by a provider; without providers, Terraform can't manage any kind of infrastructure.\n\nMost providers configure a specific infrastructure platform (either cloud or self-hosted). Providers can also offer local utilities for tasks like generating random numbers for unique resource names.\n\nWhere Providers Come From\n\nProviders are distributed separately from Terraform itself, and each provider has its own release cadence and version numbers.\n\nThe Terraform Registry is the main directory of publicly available Terraform providers, and hosts providers for most major infrastructure platforms.\n\nProvider Documentation\n\nEach provider has its own documentation, describing its resource types and their arguments.\n\nThe Terraform Registry includes documentation for a wide range of providers developed by HashiCorp, third-party vendors, and our Terraform community. Use the \"Documentation\" link in a provider's header to browse its documentation.\n\nProvider documentation in the Registry is versioned; you can use the version menu in the header to change which version you're viewing.\n\nFor details about writing, generating, and previewing provider documentation, see the provider publishing documentation.\n\nHow to Use Providers\n\nTo use resources from a given provider, you need to include some information about it in your configuration. See the following pages for details:\n\nProvider Requirements documents how to declare providers so Terraform can install them.\n\nProvider Configuration documents how to configure settings for providers.\n\nDependency Lock File documents an additional HCL file that can be included with a configuration, which tells Terraform to always use a specific set of provider versions.\n\nProvider Installation\n\nTerraform Cloud and Terraform Enterprise install providers as part of every run.\n\nTerraform CLI finds and installs providers when initializing a working directory. It can automatically download providers from a Terraform registry, or load them from a local mirror or cache. If you are using a persistent working directory, you must reinitialize whenever you change a configuration's providers.\n\nTo save time and bandwidth, Terraform CLI supports an optional plugin cache. You can enable the cache using the plugin_cache_dir setting in the CLI configuration file.\n\nTo ensure Terraform always installs the same provider versions for a given configuration, you can use Terraform CLI to create a dependency lock file and commit it to version control along with your configuration. If a lock file is present, Terraform Cloud, CLI, and Enterprise will all obey it when installing providers.\n\nHands-on: Try the Lock and Upgrade Provider Versions tutorial.\n\nHow to Find Providers\n\nTo find providers for the infrastructure platforms you use, browse the providers section of the Terraform Registry.\n\nSome providers on the Registry are developed and published by HashiCorp, some are published by platform maintainers, and some are published by users and volunteers. The provider listings use the following badges to indicate who develops and maintains a given provider.\n\nTier\tDescription\tNamespace\n\nOfficial\n\tOfficial providers are owned and maintained by HashiCorp\thashicorp\n\nPartner\n\tPartner providers are written, maintained, validated and published by third-party companies against their own APIs. To earn a partner provider badge the partner must participate in the HashiCorp Technology Partner Program.\tThird-party organization, e.g. mongodb/mongodbatlas\n\nCommunity\n\tCommunity providers are published to the Terraform Registry by individual maintainers, groups of maintainers, or other members of the Terraform community.\tMaintainer’s individual or organization account, e.g. DeviaVir/gsuite\n\nArchived\n\tArchived Providers are Official or Partner Providers that are no longer maintained by HashiCorp or the community. This may occur if an API is deprecated or interest was low.\thashicorp or third-party\n\nHow to Develop Providers\n\nProviders are written in Go, using the Terraform Plugin SDK. For more information on developing providers, see:\n\nThe Plugin Development documentation\nThe Call APIs with Terraform Providers tutorials\n\nOn this page:\n\nProviders\nWhat Providers Do\nWhere Providers Come From\nProvider Documentation\nHow to Use Providers\nProvider Installation\nHow to Find Providers\nHow to Develop Providers\nTheme\nDark\nLight\nSystem\nCertifications\nSystem Status\nCookie Manager\nTerms of Use\nSecurity\nPrivacy\nTrademark Policy\nTrade Controls\nGive Feedback\n(opens in new tab)\nWe use cookies & other similar technology to collect data to improve your experience on our site, as described in our Privacy Policy and Cookie Policy.\nManage PreferencesACCEPT"
  },
  {
    "title": "Providers - Configuration Language | Terraform | HashiCorp Developer",
    "url": "https://developer.hashicorp.com/terraform/language/v1.5.x/providers",
    "html": "Skip to main content\nNews\nHashiCorp to join IBM to accelerate multi-cloud automation.\nRead the blog →\nDismiss alert\nTerraform\nInstall\nTutorials\nDocumentation\nRegistry\n(opens in new tab)\nTry Cloud\n(opens in new tab)\nSearch\nCommand or control key\nK key\nTerraform Home\nConfiguration Language\nConfiguration Language\nTerraform Language\nOverview\nFiles and Directories\nSyntax\nResources\nData Sources\nProviders\nOverview\nProvider Configuration\nProvider Requirements\nDependency Lock File\nVariables and Outputs\nModules\nChecks\nImport\nExpressions\nFunctions\nTerraform Settings\nState\nUpgrading to Terraform v1.5\nv1.x Compatibility Promises\nTerraform Internals\nResources\nTutorial Library\nCertifications\nCommunity Forum\n(opens in new tab)\nSupport\n(opens in new tab)\nGitHub\n(opens in new tab)\nTerraform Registry\n(opens in new tab)\n\nYou are viewing documentation for version v1.5.x. View latest version.\n\nDeveloper\nTerraform\nConfiguration Language v1.5.x\nProviders\nv1.5.x\nProviders\n\nHands-on: Try the Perform CRUD Operations with Providers tutorial.\n\nTerraform relies on plugins called providers to interact with cloud providers, SaaS providers, and other APIs.\n\nTerraform configurations must declare which providers they require so that Terraform can install and use them. Additionally, some providers require configuration (like endpoint URLs or cloud regions) before they can be used.\n\nWhat Providers Do\n\nEach provider adds a set of resource types and/or data sources that Terraform can manage.\n\nEvery resource type is implemented by a provider; without providers, Terraform can't manage any kind of infrastructure.\n\nMost providers configure a specific infrastructure platform (either cloud or self-hosted). Providers can also offer local utilities for tasks like generating random numbers for unique resource names.\n\nWhere Providers Come From\n\nProviders are distributed separately from Terraform itself, and each provider has its own release cadence and version numbers.\n\nThe Terraform Registry is the main directory of publicly available Terraform providers, and hosts providers for most major infrastructure platforms.\n\nProvider Documentation\n\nEach provider has its own documentation, describing its resource types and their arguments.\n\nThe Terraform Registry includes documentation for a wide range of providers developed by HashiCorp, third-party vendors, and our Terraform community. Use the \"Documentation\" link in a provider's header to browse its documentation.\n\nProvider documentation in the Registry is versioned; you can use the version menu in the header to change which version you're viewing.\n\nFor details about writing, generating, and previewing provider documentation, see the provider publishing documentation.\n\nHow to Use Providers\n\nProviders are released separately from Terraform itself and have their own version numbers. In production we recommend constraining the acceptable provider versions in the configuration's provider requirements block, to make sure that terraform init does not install newer versions of the provider that are incompatible with the configuration.\n\nTo use resources from a given provider, you need to include some information about it in your configuration. See the following pages for details:\n\nProvider Requirements documents how to declare providers so Terraform can install them.\n\nProvider Configuration documents how to configure settings for providers.\n\nDependency Lock File documents an additional HCL file that can be included with a configuration, which tells Terraform to always use a specific set of provider versions.\n\nProvider Installation\n\nTerraform Cloud and Terraform Enterprise install providers as part of every run.\n\nTerraform CLI finds and installs providers when initializing a working directory. It can automatically download providers from a Terraform registry, or load them from a local mirror or cache. If you are using a persistent working directory, you must reinitialize whenever you change a configuration's providers.\n\nTo save time and bandwidth, Terraform CLI supports an optional plugin cache. You can enable the cache using the plugin_cache_dir setting in the CLI configuration file.\n\nTo ensure Terraform always installs the same provider versions for a given configuration, you can use Terraform CLI to create a dependency lock file and commit it to version control along with your configuration. If a lock file is present, Terraform Cloud, CLI, and Enterprise will all obey it when installing providers.\n\nHands-on: Try the Lock and Upgrade Provider Versions tutorial.\n\nHow to Find Providers\n\nTo find providers for the infrastructure platforms you use, browse the providers section of the Terraform Registry.\n\nSome providers on the Registry are developed and published by HashiCorp, some are published by platform maintainers, and some are published by users and volunteers. The provider listings use the following badges to indicate who develops and maintains a given provider.\n\nTier\tDescription\tNamespace\n\nOfficial\n\tOfficial providers are owned and maintained by HashiCorp\thashicorp\n\nPartner\n\tPartner providers are written, maintained, validated and published by third-party companies against their own APIs. To earn a partner provider badge the partner must participate in the HashiCorp Technology Partner Program.\tThird-party organization, e.g. mongodb/mongodbatlas\n\nCommunity\n\tCommunity providers are published to the Terraform Registry by individual maintainers, groups of maintainers, or other members of the Terraform community.\tMaintainer’s individual or organization account, e.g. DeviaVir/gsuite\n\nArchived\n\tArchived Providers are Official or Partner Providers that are no longer maintained by HashiCorp or the community. This may occur if an API is deprecated or interest was low.\thashicorp or third-party\n\nHow to Develop Providers\n\nProviders are written in Go, using the Terraform Plugin SDK. For more information on developing providers, see:\n\nThe Plugin Development documentation\nThe Call APIs with Terraform Providers tutorials\n\nOn this page:\n\nProviders\nWhat Providers Do\nWhere Providers Come From\nProvider Documentation\nHow to Use Providers\nProvider Installation\nHow to Find Providers\nHow to Develop Providers\nTheme\nDark\nLight\nSystem\nCertifications\nSystem Status\nCookie Manager\nTerms of Use\nSecurity\nPrivacy\nTrademark Policy\nTrade Controls\nGive Feedback\n(opens in new tab)\nWe use cookies & other similar technology to collect data to improve your experience on our site, as described in our Privacy Policy and Cookie Policy.\nManage PreferencesACCEPT"
  },
  {
    "title": "Providers - Configuration Language | Terraform | HashiCorp Developer",
    "url": "https://developer.hashicorp.com/terraform/language/v1.3.x/providers",
    "html": "Skip to main content\nNews\nHashiCorp to join IBM to accelerate multi-cloud automation.\nRead the blog →\nDismiss alert\nTerraform\nInstall\nTutorials\nDocumentation\nRegistry\n(opens in new tab)\nTry Cloud\n(opens in new tab)\nSearch\nCommand or control key\nK key\nTerraform Home\nConfiguration Language\nConfiguration Language\nTerraform Language\nOverview\nStyle Guide\nFiles and Directories\nSyntax\nResources\nData Sources\nProviders\nOverview\nProvider Configuration\nProvider Requirements\nDependency Lock File\nVariables and Outputs\nModules\nMoved block\nExpressions\nFunctions\nTerraform Settings\nState\nUpgrading to Terraform v1.3\nv1.x Compatibility Promises\nTerraform Internals\nResources\nTutorial Library\nCertifications\nCommunity Forum\n(opens in new tab)\nSupport\n(opens in new tab)\nGitHub\n(opens in new tab)\nTerraform Registry\n(opens in new tab)\n\nYou are viewing documentation for version v1.3.x. View latest version.\n\nDeveloper\nTerraform\nConfiguration Language v1.3.x\nProviders\nv1.3.x\nProviders\n\nHands-on: Try the Perform CRUD Operations with Providers tutorial.\n\nTerraform relies on plugins called providers to interact with cloud providers, SaaS providers, and other APIs.\n\nTerraform configurations must declare which providers they require so that Terraform can install and use them. Additionally, some providers require configuration (like endpoint URLs or cloud regions) before they can be used.\n\nWhat Providers Do\n\nEach provider adds a set of resource types and/or data sources that Terraform can manage.\n\nEvery resource type is implemented by a provider; without providers, Terraform can't manage any kind of infrastructure.\n\nMost providers configure a specific infrastructure platform (either cloud or self-hosted). Providers can also offer local utilities for tasks like generating random numbers for unique resource names.\n\nWhere Providers Come From\n\nProviders are distributed separately from Terraform itself, and each provider has its own release cadence and version numbers.\n\nThe Terraform Registry is the main directory of publicly available Terraform providers, and hosts providers for most major infrastructure platforms.\n\nProvider Documentation\n\nEach provider has its own documentation, describing its resource types and their arguments.\n\nThe Terraform Registry includes documentation for a wide range of providers developed by HashiCorp, third-party vendors, and our Terraform community. Use the \"Documentation\" link in a provider's header to browse its documentation.\n\nProvider documentation in the Registry is versioned; you can use the version menu in the header to change which version you're viewing.\n\nFor details about writing, generating, and previewing provider documentation, see the provider publishing documentation.\n\nHow to Use Providers\n\nTo use resources from a given provider, you need to include some information about it in your configuration. See the following pages for details:\n\nProvider Requirements documents how to declare providers so Terraform can install them.\n\nProvider Configuration documents how to configure settings for providers.\n\nDependency Lock File documents an additional HCL file that can be included with a configuration, which tells Terraform to always use a specific set of provider versions.\n\nProvider Installation\n\nTerraform Cloud and Terraform Enterprise install providers as part of every run.\n\nTerraform CLI finds and installs providers when initializing a working directory. It can automatically download providers from a Terraform registry, or load them from a local mirror or cache. If you are using a persistent working directory, you must reinitialize whenever you change a configuration's providers.\n\nTo save time and bandwidth, Terraform CLI supports an optional plugin cache. You can enable the cache using the plugin_cache_dir setting in the CLI configuration file.\n\nTo ensure Terraform always installs the same provider versions for a given configuration, you can use Terraform CLI to create a dependency lock file and commit it to version control along with your configuration. If a lock file is present, Terraform Cloud, CLI, and Enterprise will all obey it when installing providers.\n\nHands-on: Try the Lock and Upgrade Provider Versions tutorial.\n\nHow to Find Providers\n\nTo find providers for the infrastructure platforms you use, browse the providers section of the Terraform Registry.\n\nSome providers on the Registry are developed and published by HashiCorp, some are published by platform maintainers, and some are published by users and volunteers. The provider listings use the following badges to indicate who develops and maintains a given provider.\n\nTier\tDescription\tNamespace\n\nOfficial\n\tOfficial providers are owned and maintained by HashiCorp\thashicorp\n\nPartner\n\tPartner providers are written, maintained, validated and published by third-party companies against their own APIs. To earn a partner provider badge the partner must participate in the HashiCorp Technology Partner Program.\tThird-party organization, e.g. mongodb/mongodbatlas\n\nCommunity\n\tCommunity providers are published to the Terraform Registry by individual maintainers, groups of maintainers, or other members of the Terraform community.\tMaintainer’s individual or organization account, e.g. DeviaVir/gsuite\n\nArchived\n\tArchived Providers are Official or Partner Providers that are no longer maintained by HashiCorp or the community. This may occur if an API is deprecated or interest was low.\thashicorp or third-party\n\nHow to Develop Providers\n\nProviders are written in Go, using the Terraform Plugin SDK. For more information on developing providers, see:\n\nThe Plugin Development documentation\nThe Call APIs with Terraform Providers tutorials\n\nOn this page:\n\nProviders\nWhat Providers Do\nWhere Providers Come From\nProvider Documentation\nHow to Use Providers\nProvider Installation\nHow to Find Providers\nHow to Develop Providers\nTheme\nDark\nLight\nSystem\nCertifications\nSystem Status\nCookie Manager\nTerms of Use\nSecurity\nPrivacy\nTrademark Policy\nTrade Controls\nGive Feedback\n(opens in new tab)\nWe use cookies & other similar technology to collect data to improve your experience on our site, as described in our Privacy Policy and Cookie Policy.\nManage PreferencesACCEPT"
  },
  {
    "title": "Providers - Configuration Language | Terraform | HashiCorp Developer",
    "url": "https://developer.hashicorp.com/terraform/language/v1.4.x/providers",
    "html": "Skip to main content\nNews\nHashiCorp to join IBM to accelerate multi-cloud automation.\nRead the blog →\nDismiss alert\nTerraform\nInstall\nTutorials\nDocumentation\nRegistry\n(opens in new tab)\nTry Cloud\n(opens in new tab)\nSearch\nCommand or control key\nK key\nTerraform Home\nConfiguration Language\nConfiguration Language\nTerraform Language\nOverview\nStyle Guide\nFiles and Directories\nSyntax\nResources\nData Sources\nProviders\nOverview\nProvider Configuration\nProvider Requirements\nDependency Lock File\nVariables and Outputs\nModules\nExpressions\nFunctions\nTerraform Settings\nState\nUpgrading to Terraform v1.4\nv1.x Compatibility Promises\nTerraform Internals\nResources\nTutorial Library\nCertifications\nCommunity Forum\n(opens in new tab)\nSupport\n(opens in new tab)\nGitHub\n(opens in new tab)\nTerraform Registry\n(opens in new tab)\n\nYou are viewing documentation for version v1.4.x. View latest version.\n\nDeveloper\nTerraform\nConfiguration Language v1.4.x\nProviders\nv1.4.x\nProviders\n\nHands-on: Try the Perform CRUD Operations with Providers tutorial.\n\nTerraform relies on plugins called providers to interact with cloud providers, SaaS providers, and other APIs.\n\nTerraform configurations must declare which providers they require so that Terraform can install and use them. Additionally, some providers require configuration (like endpoint URLs or cloud regions) before they can be used.\n\nWhat Providers Do\n\nEach provider adds a set of resource types and/or data sources that Terraform can manage.\n\nEvery resource type is implemented by a provider; without providers, Terraform can't manage any kind of infrastructure.\n\nMost providers configure a specific infrastructure platform (either cloud or self-hosted). Providers can also offer local utilities for tasks like generating random numbers for unique resource names.\n\nWhere Providers Come From\n\nProviders are distributed separately from Terraform itself, and each provider has its own release cadence and version numbers.\n\nThe Terraform Registry is the main directory of publicly available Terraform providers, and hosts providers for most major infrastructure platforms.\n\nProvider Documentation\n\nEach provider has its own documentation, describing its resource types and their arguments.\n\nThe Terraform Registry includes documentation for a wide range of providers developed by HashiCorp, third-party vendors, and our Terraform community. Use the \"Documentation\" link in a provider's header to browse its documentation.\n\nProvider documentation in the Registry is versioned; you can use the version menu in the header to change which version you're viewing.\n\nFor details about writing, generating, and previewing provider documentation, see the provider publishing documentation.\n\nHow to Use Providers\n\nProviders are released separately from Terraform itself, and have their own version numbers. In production we recommend constraining the acceptable provider versions in the configuration's provider requirements block, to make sure that terraform init does not install newer versions of the provider that are incompatible with the configuration.\n\nTo use resources from a given provider, you need to include some information about it in your configuration. See the following pages for details:\n\nProvider Requirements documents how to declare providers so Terraform can install them.\n\nProvider Configuration documents how to configure settings for providers.\n\nDependency Lock File documents an additional HCL file that can be included with a configuration, which tells Terraform to always use a specific set of provider versions.\n\nProvider Installation\n\nTerraform Cloud and Terraform Enterprise install providers as part of every run.\n\nTerraform CLI finds and installs providers when initializing a working directory. It can automatically download providers from a Terraform registry, or load them from a local mirror or cache. If you are using a persistent working directory, you must reinitialize whenever you change a configuration's providers.\n\nTo save time and bandwidth, Terraform CLI supports an optional plugin cache. You can enable the cache using the plugin_cache_dir setting in the CLI configuration file.\n\nTo ensure Terraform always installs the same provider versions for a given configuration, you can use Terraform CLI to create a dependency lock file and commit it to version control along with your configuration. If a lock file is present, Terraform Cloud, CLI, and Enterprise will all obey it when installing providers.\n\nHands-on: Try the Lock and Upgrade Provider Versions tutorial.\n\nHow to Find Providers\n\nTo find providers for the infrastructure platforms you use, browse the providers section of the Terraform Registry.\n\nSome providers on the Registry are developed and published by HashiCorp, some are published by platform maintainers, and some are published by users and volunteers. The provider listings use the following badges to indicate who develops and maintains a given provider.\n\nTier\tDescription\tNamespace\n\nOfficial\n\tOfficial providers are owned and maintained by HashiCorp\thashicorp\n\nPartner\n\tPartner providers are written, maintained, validated and published by third-party companies against their own APIs. To earn a partner provider badge the partner must participate in the HashiCorp Technology Partner Program.\tThird-party organization, e.g. mongodb/mongodbatlas\n\nCommunity\n\tCommunity providers are published to the Terraform Registry by individual maintainers, groups of maintainers, or other members of the Terraform community.\tMaintainer’s individual or organization account, e.g. DeviaVir/gsuite\n\nArchived\n\tArchived Providers are Official or Partner Providers that are no longer maintained by HashiCorp or the community. This may occur if an API is deprecated or interest was low.\thashicorp or third-party\n\nHow to Develop Providers\n\nProviders are written in Go, using the Terraform Plugin SDK. For more information on developing providers, see:\n\nThe Plugin Development documentation\nThe Call APIs with Terraform Providers tutorials\n\nOn this page:\n\nProviders\nWhat Providers Do\nWhere Providers Come From\nProvider Documentation\nHow to Use Providers\nProvider Installation\nHow to Find Providers\nHow to Develop Providers\nTheme\nDark\nLight\nSystem\nCertifications\nSystem Status\nCookie Manager\nTerms of Use\nSecurity\nPrivacy\nTrademark Policy\nTrade Controls\nGive Feedback\n(opens in new tab)\nWe use cookies & other similar technology to collect data to improve your experience on our site, as described in our Privacy Policy and Cookie Policy.\nManage PreferencesACCEPT"
  },
  {
    "title": "Providers - Configuration Language | Terraform | HashiCorp Developer",
    "url": "https://developer.hashicorp.com/terraform/language/v1.6.x/providers",
    "html": "Skip to main content\nNews\nHashiCorp to join IBM to accelerate multi-cloud automation.\nRead the blog →\nDismiss alert\nTerraform\nInstall\nTutorials\nDocumentation\nRegistry\n(opens in new tab)\nTry Cloud\n(opens in new tab)\nSearch\nCommand or control key\nK key\nTerraform Home\nConfiguration Language\nConfiguration Language\nTerraform Language\nOverview\nStyle Guide\nFiles and Directories\nSyntax\nResources\nData Sources\nProviders\nOverview\nProvider Configuration\nProvider Requirements\nDependency Lock File\nVariables and Outputs\nModules\nMoved block\nChecks\nImport\nExpressions\nFunctions\nTerraform Settings\nState\nTests\nUpgrading to Terraform v1.6\nv1.x Compatibility Promises\nTerraform Internals\nResources\nTutorial Library\nCertifications\nCommunity Forum\n(opens in new tab)\nSupport\n(opens in new tab)\nGitHub\n(opens in new tab)\nTerraform Registry\n(opens in new tab)\n\nYou are viewing documentation for version v1.6.x. View latest version.\n\nDeveloper\nTerraform\nConfiguration Language v1.6.x\nProviders\nv1.6.x\nProviders\n\nHands-on: Try the Perform CRUD Operations with Providers tutorial.\n\nTerraform relies on plugins called providers to interact with cloud providers, SaaS providers, and other APIs.\n\nTerraform configurations must declare which providers they require so that Terraform can install and use them. Additionally, some providers require configuration (like endpoint URLs or cloud regions) before they can be used.\n\nWhat Providers Do\n\nEach provider adds a set of resource types and/or data sources that Terraform can manage.\n\nEvery resource type is implemented by a provider; without providers, Terraform can't manage any kind of infrastructure.\n\nMost providers configure a specific infrastructure platform (either cloud or self-hosted). Providers can also offer local utilities for tasks like generating random numbers for unique resource names.\n\nWhere Providers Come From\n\nProviders are distributed separately from Terraform itself, and each provider has its own release cadence and version numbers.\n\nThe Terraform Registry is the main directory of publicly available Terraform providers, and hosts providers for most major infrastructure platforms.\n\nProvider Documentation\n\nEach provider has its own documentation, describing its resource types and their arguments.\n\nThe Terraform Registry includes documentation for a wide range of providers developed by HashiCorp, third-party vendors, and our Terraform community. Use the \"Documentation\" link in a provider's header to browse its documentation.\n\nProvider documentation in the Registry is versioned; you can use the version menu in the header to change which version you're viewing.\n\nFor details about writing, generating, and previewing provider documentation, see the provider publishing documentation.\n\nHow to Use Providers\n\nProviders are released separately from Terraform itself and have their own version numbers. In production we recommend constraining the acceptable provider versions in the configuration's provider requirements block, to make sure that terraform init does not install newer versions of the provider that are incompatible with the configuration.\n\nTo use resources from a given provider, you need to include some information about it in your configuration. See the following pages for details:\n\nProvider Requirements documents how to declare providers so Terraform can install them.\n\nProvider Configuration documents how to configure settings for providers.\n\nDependency Lock File documents an additional HCL file that can be included with a configuration, which tells Terraform to always use a specific set of provider versions.\n\nProvider Installation\n\nTerraform Cloud and Terraform Enterprise install providers as part of every run.\n\nTerraform CLI finds and installs providers when initializing a working directory. It can automatically download providers from a Terraform registry, or load them from a local mirror or cache. If you are using a persistent working directory, you must reinitialize whenever you change a configuration's providers.\n\nTo save time and bandwidth, Terraform CLI supports an optional plugin cache. You can enable the cache using the plugin_cache_dir setting in the CLI configuration file.\n\nTo ensure Terraform always installs the same provider versions for a given configuration, you can use Terraform CLI to create a dependency lock file and commit it to version control along with your configuration. If a lock file is present, Terraform Cloud, CLI, and Enterprise will all obey it when installing providers.\n\nHands-on: Try the Lock and Upgrade Provider Versions tutorial.\n\nHow to Find Providers\n\nTo find providers for the infrastructure platforms you use, browse the providers section of the Terraform Registry.\n\nSome providers on the Registry are developed and published by HashiCorp, some are published by platform maintainers, and some are published by users and volunteers. The provider listings use the following badges to indicate who develops and maintains a given provider.\n\nTier\tDescription\tNamespace\n\nOfficial\n\tOfficial providers are owned and maintained by HashiCorp\thashicorp\n\nPartner\n\tPartner providers are written, maintained, validated and published by third-party companies against their own APIs. To earn a partner provider badge the partner must participate in the HashiCorp Technology Partner Program.\tThird-party organization, e.g. mongodb/mongodbatlas\n\nCommunity\n\tCommunity providers are published to the Terraform Registry by individual maintainers, groups of maintainers, or other members of the Terraform community.\tMaintainer’s individual or organization account, e.g. DeviaVir/gsuite\n\nArchived\n\tArchived Providers are Official or Partner Providers that are no longer maintained by HashiCorp or the community. This may occur if an API is deprecated or interest was low.\thashicorp or third-party\n\nHow to Develop Providers\n\nProviders are written in Go, using the Terraform Plugin SDK. For more information on developing providers, see:\n\nThe Plugin Development documentation\nThe Call APIs with Terraform Providers tutorials\n\nOn this page:\n\nProviders\nWhat Providers Do\nWhere Providers Come From\nProvider Documentation\nHow to Use Providers\nProvider Installation\nHow to Find Providers\nHow to Develop Providers\nTheme\nDark\nLight\nSystem\nCertifications\nSystem Status\nCookie Manager\nTerms of Use\nSecurity\nPrivacy\nTrademark Policy\nTrade Controls\nGive Feedback\n(opens in new tab)\nWe use cookies & other similar technology to collect data to improve your experience on our site, as described in our Privacy Policy and Cookie Policy.\nManage PreferencesACCEPT"
  },
  {
    "title": "Providers - Configuration Language | Terraform | HashiCorp Developer",
    "url": "https://developer.hashicorp.com/terraform/language/v1.9.x/providers",
    "html": "Skip to main content\nNews\nHashiCorp to join IBM to accelerate multi-cloud automation.\nRead the blog →\nDismiss alert\nTerraform\nInstall\nTutorials\nDocumentation\nRegistry\n(opens in new tab)\nTry Cloud\n(opens in new tab)\nSearch\nCommand or control key\nK key\nTerraform Home\nConfiguration Language\nConfiguration Language\nTerraform Language\nOverview\nStyle Guide\nFiles and Directories\nSyntax\nResources\nData Sources\nProviders\nOverview\nProvider Configuration\nProvider Requirements\nDependency Lock File\nVariables and Outputs\nModules\nChecks\nImport\nExpressions\nFunctions\nTerraform Settings\nState\nTests\nUpgrading to Terraform v1.7\nv1.x Compatibility Promises\nTerraform Internals\nResources\nTutorial Library\nCertifications\nCommunity Forum\n(opens in new tab)\nSupport\n(opens in new tab)\nGitHub\n(opens in new tab)\nTerraform Registry\n(opens in new tab)\n\nYou are viewing documentation for pre-release version v1.9.0 (alpha). View latest version.\n\nDeveloper\nTerraform\nConfiguration Language v1.9.x\nProviders\nv1.9.0 (alpha)\nProviders\n\nHands-on: Try the Perform CRUD Operations with Providers tutorial.\n\nTerraform relies on plugins called providers to interact with cloud providers, SaaS providers, and other APIs.\n\nTerraform configurations must declare which providers they require so that Terraform can install and use them. Additionally, some providers require configuration (like endpoint URLs or cloud regions) before they can be used.\n\nWhat Providers Do\n\nEach provider adds a set of resource types and/or data sources that Terraform can manage.\n\nEvery resource type is implemented by a provider; without providers, Terraform can't manage any kind of infrastructure.\n\nMost providers configure a specific infrastructure platform (either cloud or self-hosted). Providers can also offer local utilities for tasks like generating random numbers for unique resource names.\n\nWhere Providers Come From\n\nProviders are distributed separately from Terraform itself, and each provider has its own release cadence and version numbers.\n\nThe Terraform Registry is the main directory of publicly available Terraform providers, and hosts providers for most major infrastructure platforms.\n\nProvider Documentation\n\nEach provider has its own documentation, describing its resource types and their arguments.\n\nThe Terraform Registry includes documentation for a wide range of providers developed by HashiCorp, third-party vendors, and our Terraform community. Use the \"Documentation\" link in a provider's header to browse its documentation.\n\nProvider documentation in the Registry is versioned; you can use the version menu in the header to change which version you're viewing.\n\nFor details about writing, generating, and previewing provider documentation, see the provider publishing documentation.\n\nHow to Use Providers\n\nProviders are released separately from Terraform itself and have their own version numbers. In production we recommend constraining the acceptable provider versions in the configuration's provider requirements block, to make sure that terraform init does not install newer versions of the provider that are incompatible with the configuration.\n\nTo use resources from a given provider, you need to include some information about it in your configuration. See the following pages for details:\n\nProvider Requirements documents how to declare providers so Terraform can install them.\n\nProvider Configuration documents how to configure settings for providers.\n\nDependency Lock File documents an additional HCL file that can be included with a configuration, which tells Terraform to always use a specific set of provider versions.\n\nProvider Installation\n\nHCP Terraform and Terraform Enterprise install providers as part of every run.\n\nTerraform CLI finds and installs providers when initializing a working directory. It can automatically download providers from a Terraform registry, or load them from a local mirror or cache. If you are using a persistent working directory, you must reinitialize whenever you change a configuration's providers.\n\nTo save time and bandwidth, Terraform CLI supports an optional plugin cache. You can enable the cache using the plugin_cache_dir setting in the CLI configuration file.\n\nTo ensure Terraform always installs the same provider versions for a given configuration, you can use Terraform CLI to create a dependency lock file and commit it to version control along with your configuration. If a lock file is present, HCP Terraform, CLI, and Enterprise will all obey it when installing providers.\n\nHands-on: Try the Lock and Upgrade Provider Versions tutorial.\n\nHow to Find Providers\n\nTo find providers for the infrastructure platforms you use, browse the providers section of the Terraform Registry.\n\nSome providers on the Registry are developed and published by HashiCorp, some are published by platform maintainers, and some are published by users and volunteers. The provider listings use the following badges to indicate who develops and maintains a given provider.\n\nTier\tDescription\tNamespace\n\nOfficial\n\tOfficial providers are owned and maintained by HashiCorp\thashicorp\n\nPartner\n\tPartner providers are written, maintained, validated and published by third-party companies against their own APIs. To earn a partner provider badge the partner must participate in the HashiCorp Technology Partner Program.\tThird-party organization, e.g. mongodb/mongodbatlas\n\nCommunity\n\tCommunity providers are published to the Terraform Registry by individual maintainers, groups of maintainers, or other members of the Terraform community.\tMaintainer’s individual or organization account, e.g. DeviaVir/gsuite\n\nArchived\n\tArchived Providers are Official or Partner Providers that are no longer maintained by HashiCorp or the community. This may occur if an API is deprecated or interest was low.\thashicorp or third-party\n\nHow to Develop Providers\n\nProviders are written in Go, using the Terraform Plugin SDK. For more information on developing providers, see:\n\nThe Plugin Development documentation\nThe Call APIs with Terraform Providers tutorials\n\nOn this page:\n\nProviders\nWhat Providers Do\nWhere Providers Come From\nProvider Documentation\nHow to Use Providers\nProvider Installation\nHow to Find Providers\nHow to Develop Providers\nTheme\nDark\nLight\nSystem\nCertifications\nSystem Status\nCookie Manager\nTerms of Use\nSecurity\nPrivacy\nTrademark Policy\nTrade Controls\nGive Feedback\n(opens in new tab)\nWe use cookies & other similar technology to collect data to improve your experience on our site, as described in our Privacy Policy and Cookie Policy.\nManage PreferencesACCEPT"
  },
  {
    "title": "Providers - Configuration Language | Terraform | HashiCorp Developer",
    "url": "https://developer.hashicorp.com/terraform/language/v1.7.x/providers",
    "html": "Skip to main content\nNews\nHashiCorp to join IBM to accelerate multi-cloud automation.\nRead the blog →\nDismiss alert\nTerraform\nInstall\nTutorials\nDocumentation\nRegistry\n(opens in new tab)\nTry Cloud\n(opens in new tab)\nSearch\nCommand or control key\nK key\nTerraform Home\nConfiguration Language\nConfiguration Language\nTerraform Language\nOverview\nStyle Guide\nFiles and Directories\nSyntax\nResources\nData Sources\nProviders\nOverview\nProvider Configuration\nProvider Requirements\nDependency Lock File\nVariables and Outputs\nModules\nMoved block\nChecks\nImport\nExpressions\nFunctions\nTerraform Settings\nState\nTests\nUpgrading to Terraform v1.7\nv1.x Compatibility Promises\nTerraform Internals\nResources\nTutorial Library\nCertifications\nCommunity Forum\n(opens in new tab)\nSupport\n(opens in new tab)\nGitHub\n(opens in new tab)\nTerraform Registry\n(opens in new tab)\n\nYou are viewing documentation for version v1.7.x. View latest version.\n\nDeveloper\nTerraform\nConfiguration Language v1.7.x\nProviders\nv1.7.x\nProviders\n\nHands-on: Try the Perform CRUD Operations with Providers tutorial.\n\nTerraform relies on plugins called providers to interact with cloud providers, SaaS providers, and other APIs.\n\nTerraform configurations must declare which providers they require so that Terraform can install and use them. Additionally, some providers require configuration (like endpoint URLs or cloud regions) before they can be used.\n\nWhat Providers Do\n\nEach provider adds a set of resource types and/or data sources that Terraform can manage.\n\nEvery resource type is implemented by a provider; without providers, Terraform can't manage any kind of infrastructure.\n\nMost providers configure a specific infrastructure platform (either cloud or self-hosted). Providers can also offer local utilities for tasks like generating random numbers for unique resource names.\n\nWhere Providers Come From\n\nProviders are distributed separately from Terraform itself, and each provider has its own release cadence and version numbers.\n\nThe Terraform Registry is the main directory of publicly available Terraform providers, and hosts providers for most major infrastructure platforms.\n\nProvider Documentation\n\nEach provider has its own documentation, describing its resource types and their arguments.\n\nThe Terraform Registry includes documentation for a wide range of providers developed by HashiCorp, third-party vendors, and our Terraform community. Use the \"Documentation\" link in a provider's header to browse its documentation.\n\nProvider documentation in the Registry is versioned; you can use the version menu in the header to change which version you're viewing.\n\nFor details about writing, generating, and previewing provider documentation, see the provider publishing documentation.\n\nHow to Use Providers\n\nProviders are released separately from Terraform itself and have their own version numbers. In production we recommend constraining the acceptable provider versions in the configuration's provider requirements block, to make sure that terraform init does not install newer versions of the provider that are incompatible with the configuration.\n\nTo use resources from a given provider, you need to include some information about it in your configuration. See the following pages for details:\n\nProvider Requirements documents how to declare providers so Terraform can install them.\n\nProvider Configuration documents how to configure settings for providers.\n\nDependency Lock File documents an additional HCL file that can be included with a configuration, which tells Terraform to always use a specific set of provider versions.\n\nProvider Installation\n\nTerraform Cloud and Terraform Enterprise install providers as part of every run.\n\nTerraform CLI finds and installs providers when initializing a working directory. It can automatically download providers from a Terraform registry, or load them from a local mirror or cache. If you are using a persistent working directory, you must reinitialize whenever you change a configuration's providers.\n\nTo save time and bandwidth, Terraform CLI supports an optional plugin cache. You can enable the cache using the plugin_cache_dir setting in the CLI configuration file.\n\nTo ensure Terraform always installs the same provider versions for a given configuration, you can use Terraform CLI to create a dependency lock file and commit it to version control along with your configuration. If a lock file is present, Terraform Cloud, CLI, and Enterprise will all obey it when installing providers.\n\nHands-on: Try the Lock and Upgrade Provider Versions tutorial.\n\nHow to Find Providers\n\nTo find providers for the infrastructure platforms you use, browse the providers section of the Terraform Registry.\n\nSome providers on the Registry are developed and published by HashiCorp, some are published by platform maintainers, and some are published by users and volunteers. The provider listings use the following badges to indicate who develops and maintains a given provider.\n\nTier\tDescription\tNamespace\n\nOfficial\n\tOfficial providers are owned and maintained by HashiCorp\thashicorp\n\nPartner\n\tPartner providers are written, maintained, validated and published by third-party companies against their own APIs. To earn a partner provider badge the partner must participate in the HashiCorp Technology Partner Program.\tThird-party organization, e.g. mongodb/mongodbatlas\n\nCommunity\n\tCommunity providers are published to the Terraform Registry by individual maintainers, groups of maintainers, or other members of the Terraform community.\tMaintainer’s individual or organization account, e.g. DeviaVir/gsuite\n\nArchived\n\tArchived Providers are Official or Partner Providers that are no longer maintained by HashiCorp or the community. This may occur if an API is deprecated or interest was low.\thashicorp or third-party\n\nHow to Develop Providers\n\nProviders are written in Go, using the Terraform Plugin SDK. For more information on developing providers, see:\n\nThe Plugin Development documentation\nThe Call APIs with Terraform Providers tutorials\n\nOn this page:\n\nProviders\nWhat Providers Do\nWhere Providers Come From\nProvider Documentation\nHow to Use Providers\nProvider Installation\nHow to Find Providers\nHow to Develop Providers\nTheme\nDark\nLight\nSystem\nCertifications\nSystem Status\nCookie Manager\nTerms of Use\nSecurity\nPrivacy\nTrademark Policy\nTrade Controls\nGive Feedback\n(opens in new tab)\nWe use cookies & other similar technology to collect data to improve your experience on our site, as described in our Privacy Policy and Cookie Policy.\nManage PreferencesACCEPT"
  },
  {
    "title": "Dependency Lock File (.terraform.lock.hcl) - Configuration Language | Terraform | HashiCorp Developer",
    "url": "https://developer.hashicorp.com/terraform/language/files/dependency-lock",
    "html": "Skip to main content\nNews\nHashiCorp to join IBM to accelerate multi-cloud automation.\nRead the blog →\nDismiss alert\nTerraform\nInstall\nTutorials\nDocumentation\nRegistry\n(opens in new tab)\nTry Cloud\n(opens in new tab)\nSearch\nCommand or control key\nK key\nTerraform Home\nConfiguration Language\nConfiguration Language\nStyle Guide\nFiles and Directories\nOverview\nOverride Files\nDependency Lock File\nTest Files\nSyntax\nResources\nData Sources\nProviders\nVariables and Outputs\nModules\nMoved block\nChecks\nImport\nExpressions\nFunctions\nTerraform Settings\nState\nTests\nUpgrading to Terraform v1.7\nv1.x Compatibility Promises\nTerraform Internals\nResources\nTutorial Library\nCertifications\nCommunity Forum\n(opens in new tab)\nSupport\n(opens in new tab)\nGitHub\n(opens in new tab)\nTerraform Registry\n(opens in new tab)\nDeveloper\nTerraform\nConfiguration Language\nFiles and Directories\nDependency Lock File\nv1.8.x (latest)\nDependency Lock File\n\nNote: This page is about a feature of Terraform 0.14 and later. Prior versions of Terraform did not track dependency selections at all, so the information here is not relevant to those versions.\n\nHands-on: Try the Lock and Upgrade Provider Versions tutorial.\n\nA Terraform configuration may refer to two different kinds of external dependency that come from outside of its own codebase:\n\nProviders, which are plugins for Terraform that extend it with support for interacting with various external systems.\nModules, which allow splitting out groups of Terraform configuration constructs (written in the Terraform language) into reusable abstractions.\n\nBoth of these dependency types can be published and updated independently from Terraform itself and from the configurations that depend on them. For that reason, Terraform must determine which versions of those dependencies are potentially compatible with the current configuration and which versions are currently selected for use.\n\nVersion constraints within the configuration itself determine which versions of dependencies are potentially compatible, but after selecting a specific version of each dependency Terraform remembers the decisions it made in a dependency lock file so that it can (by default) make the same decisions again in future.\n\nAt present, the dependency lock file tracks only provider dependencies. Terraform does not remember version selections for remote modules, and so Terraform will always select the newest available module version that meets the specified version constraints. You can use an exact version constraint to ensure that Terraform will always select the same module version.\n\nLock File Location\n\nThe dependency lock file is a file that belongs to the configuration as a whole, rather than to each separate module in the configuration. For that reason Terraform creates it and expects to find it in your current working directory when you run Terraform, which is also the directory containing the .tf files for the root module of your configuration.\n\nThe lock file is always named .terraform.lock.hcl, and this name is intended to signify that it is a lock file for various items that Terraform caches in the .terraform subdirectory of your working directory.\n\nTerraform automatically creates or updates the dependency lock file each time you run the terraform init command. You should include this file in your version control repository so that you can discuss potential changes to your external dependencies via code review, just as you would discuss potential changes to your configuration itself.\n\nThe dependency lock file uses the same low-level syntax as the main Terraform language, but the dependency lock file is not itself a Terraform language configuration file. It is named with the suffix .hcl instead of .tf in order to signify that difference.\n\nDependency Installation Behavior\n\nWhen terraform init is working on installing all of the providers needed for a configuration, Terraform considers both the version constraints in the configuration and the version selections recorded in the lock file.\n\nIf a particular provider has no existing recorded selection, Terraform will select the newest available version that matches the given version constraint, and then update the lock file to include that selection.\n\nIf a particular provider already has a selection recorded in the lock file, Terraform will always re-select that version for installation, even if a newer version has become available. You can override that behavior by adding the -upgrade option when you run terraform init, in which case Terraform will disregard the existing selections and once again select the newest available version matching the version constraint.\n\nIf a particular terraform init call makes changes to the lock file, Terraform will mention that as part of its output:\n\nTerraform has made some changes to the provider dependency selections recorded\n\nin the .terraform.lock.hcl file. Review those changes and commit them to your\n\nversion control system if they represent changes you intended to make.\n\nCopy\n\nWhen you see this message, you can use your version control system to review the changes Terraform has proposed in the file, and if they represent changes you made intentionally you can send the change through your team's usual code review process.\n\nChecksum verification\n\nTerraform will also verify that each package it installs matches at least one of the checksums it previously recorded in the lock file, if any, returning an error if none of the checksums match:\n\nError: Failed to install provider\n\n\n\nError while installing hashicorp/azurerm v2.1.0: the current package for\n\nregistry.terraform.io/hashicorp/azurerm 2.1.0 doesn't match any of the\n\nchecksums previously recorded in the dependency lock file.\n\nCopy\n\nThis checksum verification is intended to represent a trust on first use approach. When you add a new provider for the first time you can verify it in whatever way you choose or any way you are required to by relevant regulations, and then trust that Terraform will raise an error if a future run of terraform init encounters a non-matching package for the same provider version.\n\nThere are two special considerations with the \"trust on first use\" model:\n\nIf you install a provider from an origin registry which provides checksums that are signed with a cryptographic signature, Terraform will treat all of the signed checksums as valid as long as one checksum matches. The lock file will therefore include checksums for both the package you installed for your current platform and any other packages that might be available for other platforms.\n\nIn this case, the terraform init output will include the fingerprint of the key that signed the checksums, with a message like (signed by a HashiCorp partner, key ID DC9FC6B1FCE47986). You may wish to confirm that you trust the holder of the given key before committing the lock file containing the signed checksums, or to retrieve and verify the full set of available packages for the given provider version.\n\nIf you install a provider for the first time using an alternative installation method, such as a filesystem or network mirror, Terraform will not be able to verify the checksums for any platform other than the one where you ran terraform init, and so it will not record the checksums for other platforms and so the configuration will not be usable on any other platform.\n\nTo avoid this problem you can pre-populate checksums for a variety of different platforms in your lock file using the terraform providers lock command, which will then allow future calls to terraform init to verify that the packages available in your chosen mirror match the official packages from the provider's origin registry.\n\nUnderstanding Lock File Changes\n\nBecause the dependency lock file is primarily maintained automatically by Terraform itself, rather than being updated manually by you or your team, your version control system may show you that the file has changed.\n\nThere are a few different types of changes that Terraform can potentially make to your lock file, which you may need to understand in order to review the proposed changes. The following sections will describe these common situations.\n\nDependency on a new provider\n\nIf you add a new entry to the provider requirements for any module in your configuration, or if you add an external module that includes a new provider dependency itself, terraform init will respond to that by selecting the newest version of that provider which meets all of the version constraints in the configuration, and it will record its decision as a new provider block in the dependency lock file.\n\n--- .terraform.lock.hcl 2020-10-07 16:12:07.539570634 -0700\n\n+++ .terraform.lock.hcl 2020-10-07 16:12:15.267487237 -0700\n\n@@ -6,6 +6,26 @@\n\n   ]\n\n }\n\n\n\n+provider \"registry.terraform.io/hashicorp/azurerm\" {\n\n+  version     = \"2.30.0\"\n\n+  constraints = \"~> 2.12\"\n\n+  hashes = [\n\n+    \"h1:FJwsuowaG5CIdZ0WQyFZH9r6kIJeRKts9+GcRsTz1+Y=\",\n\n+    \"h1:c/ntSXrDYM1mUir2KufijYebPcwKqS9CRGd3duDSGfY=\",\n\n+    \"h1:yre4Ph76g9H84MbuhZ2z5MuldjSA4FsrX6538O7PCcY=\",\n\n+    \"zh:04f0a50bb2ba92f3bea6f0a9e549ace5a4c13ef0cbb6975494cac0ef7d4acb43\",\n\n+    \"zh:2082e12548ebcdd6fd73580e83f626ed4ed13f8cdfd51205d8696ffe54f30734\",\n\n+    \"zh:246bcc449e9a92679fb30f3c0a77f05513886565e2dcc66b16c4486f51533064\",\n\n+    \"zh:24de3930625ac9014594d79bfa42d600eca65e9022b9668b54bfd0d924e21d14\",\n\n+    \"zh:2a22893a576ff6f268d9bf81cf4a56406f7ba79f77826f6df51ee787f6d2840a\",\n\n+    \"zh:2b27485e19c2aaa9f15f29c4cff46154a9720647610171e30fc6c18ddc42ec28\",\n\n+    \"zh:435f24ce1fb2b63f7f02aa3c84ac29c5757cd29ec4d297ed0618423387fe7bd4\",\n\n+    \"zh:7d99725923de5240ff8b34b5510569aa4ebdc0bdb27b7bac2aa911a8037a3893\",\n\n+    \"zh:7e3b5d0af3b7411dd9dc65ec9ab6caee8c191aee0fa7f20fc4f51716e67f50c0\",\n\n+    \"zh:da0af4552bef5a29b88f6a0718253f3bf71ce471c959816eb7602b0dadb469ca\",\n\n+  ]\n\n+}\n\n+\n\n provider \"registry.terraform.io/newrelic/newrelic\" {\n\n   version     = \"2.1.2\"\n\n   constraints = \"~> 2.1.1\"\n\nCopy\n\nThe new lock file entry records several pieces of information:\n\nversion: the exact version that Terraform selected based on the version constraints in the configuration.\nconstraints: all of the version constraints that Terraform considered when making this selection. (Terraform doesn't actually use this information to make installation decisions, but includes it to help explain to human readers how the previous decision was made.)\nhashes: a number of checksums that are all considered to be valid for packages implementing the selected version of this provider on different platforms. The meaning of these hashes is explained more under New provider package checksums below.\nNew version of an existing provider\n\nIf you run terraform init -upgrade to ask Terraform to consider newer provider versions that still match the configured version constraints, Terraform may then select a newer version for a provider and update its existing provider block to reflect that change.\n\n--- .terraform.lock.hcl 2020-10-07 16:44:25.819579509 -0700\n\n+++ .terraform.lock.hcl 2020-10-07 16:43:42.785665945 -0700\n\n@@ -7,22 +7,22 @@\n\n }\n\n\n\n provider \"registry.terraform.io/hashicorp/azurerm\" {\n\n-  version     = \"2.1.0\"\n\n-  constraints = \"~> 2.1.0\"\n\n+  version     = \"2.0.0\"\n\n+  constraints = \"2.0.0\"\n\n   hashes      = [\n\n-    \"h1:EOJImaEaVThWasdqnJjfYc6/P8N/MRAq1J7avx5ZbV4=\",\n\n-    \"zh:0015b491cf9151235e57e35ea6b89381098e61bd923f56dffc86026d58748880\",\n\n-    \"zh:4c5682ba1e0fc7e2e602d3f103af1638f868c31fe80cc1a884a97f6dad6e1c11\",\n\n-    \"zh:57bac885b108c91ade4a41590062309c832c9ab6bf6a68046161636fcaef1499\",\n\n-    \"zh:5810d48f574c0e363c969b3f45276369c8f0a35b34d6202fdfceb7b85b3ac597\",\n\n-    \"zh:5c6e37a44462b8662cf9bdd29ce30523712a45c27c5d4711738705be0785db41\",\n\n-    \"zh:64548940a3387aa3a752e709ee9eb9982fa820fe60eb60e5f212cc1d2c58549e\",\n\n-    \"zh:7f46749163da17330bbb5293dc825333c86304baa0a7c6256650ac536b4567c8\",\n\n-    \"zh:8f8970f2df75ac43ffdd112055ee069d8bd1030f7eb4367cc4cf494a1fa802c3\",\n\n-    \"zh:9ad693d00dc5d7d455d06faba70e716bce727c6706f7293288e87fd7956b8fe0\",\n\n-    \"zh:b6e3cb55e6aec62b47edd0d2bd5e14bd6a2bcfdac65930a6e9e819934734c57b\",\n\n-    \"zh:d6a3f3b9b05c28ecf3919e9e7afa185805a6d7442fc4b3eedba749c2731d1f0e\",\n\n-    \"zh:d81fb624a357c57c7ea457ce543d865b39b12f26c2edd58a2f7cd43326c91010\",\n\n+    \"h1:bigGXBoRbp7dv79bEEn+aaju8575qEXHQ57XHVPJeB8=\",\n\n+    \"zh:09c603c8904ca4a5bc19e82335afbc2837dcc4bee81e395f9daccef2f2cba1c8\",\n\n+    \"zh:194a919d4836d6c6d4ce598d0c66cce00ddc0d0b5c40d01bb32789964d818b42\",\n\n+    \"zh:1f269627df4e266c4e0ef9ee2486534caa3c8bea91a201feda4bca525005aa0a\",\n\n+    \"zh:2bae3071bd5f8e553355c4b3a547d6efe1774a828142b762e9a4e85f79be7f63\",\n\n+    \"zh:6c98dfa5c3468e8d02e2b3af7c4a8a14a5d469ce5a642909643b413a17ca338b\",\n\n+    \"zh:7af78f61666fd45fbf428161c061ea2623162d601b79dc71d6a5158756853ffa\",\n\n+    \"zh:883c2df86ae9ba2a5c167cf5c2c7deca0239171a224d6d335f0fd6dd9c283830\",\n\n+    \"zh:a2028379078577d8ff5ecfca6e8a8b25a25ffb1686de0ee52a7fe8011783488b\",\n\n+    \"zh:abe6ef399552fd3861a454a839cd978c1d15735658fdc00f9054435aff0f4620\",\n\n+    \"zh:c30b1bf14077913c3cdf34979b1434dbb1353cb5995eb3956b191c50538b64a9\",\n\n+    \"zh:ca64ae2ad9793e5631e3b0b9327f7cb22cb5d8e9de57be7d85821791b1d5a375\",\n\n+    \"zh:fffe56904a38109bb8d613b02808a177c3ddfac19f03b3aac799281fea38f475\",\n\n   ]\n\n }\n\nCopy\n\nThe primary effect of selecting a new provider version is to change the value of version in the provider block. If the upgrade came along with a change to the configured version constraints, Terraform will also record that change in the constraints value.\n\nBecause each version has its own set of distribution packages, switching to a new version will also tend to replace all of the values in hashes, to reflect the checksums of the packages for the new version.\n\nNew provider package checksums\n\nA more subtle change you may see in a provider block is the addition of new checksums that were not previously recorded, even though nothing else in the provider block has changed:\n\n--- .terraform.lock.hcl 2020-10-07 17:24:23.397892140 -0700\n\n+++ .terraform.lock.hcl 2020-10-07 17:24:57.423130253 -0700\n\n@@ -10,6 +10,7 @@\n\n   version     = \"2.1.0\"\n\n   constraints = \"~> 2.1.0\"\n\n   hashes = [\n\n+    \"h1:1xvaS5D8B8t6J6XmXxX8spo97tAzjhacjedFX1B47Fk=\",\n\n     \"h1:EOJImaEaVThWasdqnJjfYc6/P8N/MRAq1J7avx5ZbV4=\",\n\n     \"zh:0015b491cf9151235e57e35ea6b89381098e61bd923f56dffc86026d58748880\",\n\n     \"zh:4c5682ba1e0fc7e2e602d3f103af1638f868c31fe80cc1a884a97f6dad6e1c11\",\n\nCopy\n\nThe addition of a new checksum into the hashes value represents Terraform gradually transitioning between different hashing schemes. The h1: and zh: prefixes on these values represent different hashing schemes, each of which represents calculating a checksum using a different algorithm. We may occasionally introduce new hashing schemes if we learn of limitations in the existing schemes or if a new scheme offers some considerable additional benefit.\n\nThe two hashing schemes currently supported are:\n\nzh:: a mnemonic for \"zip hash\", this is a legacy hash format which is part of the Terraform provider registry protocol and is therefore used for providers that you install directly from an origin registry.\n\nThis hashing scheme captures a SHA256 hash of each of the official .zip packages indexed in the origin registry. This is an effective scheme for verifying the official release packages when installed from a registry, but it's not suitable for verifying packages that come from other provider installation methods, such as filesystem mirrors using the unpacked directory layout.\n\nh1:: a mnemonic for \"hash scheme 1\", which is the current preferred hashing scheme.\n\nHash scheme 1 is also a SHA256 hash, but is one computed from the contents of the provider distribution package, rather than of the .zip archive it's contained within. This scheme therefore has the advantage that it can be calculated for an official .zip file, an unpacked directory with the same contents, or a recompressed .zip file which contains the same files but potentially different metadata or compression schemes.\n\nDue to the limited scope of the zh: scheme, Terraform will opportunistically add in the corresponding h1: checksums as it learns of them, which is what caused the addition of a second h1: checksum in the example change shown above.\n\nTerraform will add a new hash to an existing provider only if the hash is calculated from a package that also matches one of the existing hashes. In the above example, Terraform installed a hashicorp/azurerm package for a different platform than that which produced the original h1: checksum, but was able to match it against one of the zh: checksums recorded previously. After confirming the zh: checksum match, Terraform then recorded the corresponding h1: checksum in order to gradually migrate from the old scheme to the new scheme.\n\nWhen installing a particular provider for the first time (where there is no existing provider block for it), Terraform will pre-populate the hashes value with any checksums that are covered by the provider developer's cryptographic signature, which usually covers all of the available packages for that provider version across all supported platforms. However, because the provider registry protocol still uses the zh: scheme, the initial set will consist primarily of hashes using that scheme, which Terraform will then upgrade opportunistically as you install the packages on different platforms.\n\nIf you wish to avoid ongoing additions of new h1: hashes as you work with your configuration on new target platforms, or if you are installing providers from a mirror that therefore can't provide official signed checksums, you can ask Terraform to pre-populate hashes for a chosen set of platforms using the terraform providers lock command:\n\nterraform providers lock \\\n\n  -platform=linux_arm64 \\\n\n  -platform=linux_amd64 \\\n\n  -platform=darwin_amd64 \\\n\n  -platform=windows_amd64\n\nCopy\n\nThe above command will download and verify the official packages for all of the required providers across all four of the given platforms, and then record both zh: and h1: checksums for each of them in the lock file, thus avoiding the case where Terraform will learn about a h1: equivalent only at a later time. See the terraform providers lock documentation for more information on this command.\n\nProviders that are no longer required\n\nTo determine whether there still exists a dependency on a given provider, Terraform uses two sources of truth: the configuration itself, and the state. If you remove the last dependency on a particular provider from both your configuration and state, then terraform init will remove any existing lock file entry for that provider.\n\n--- .terraform.lock.hcl 2020-10-07 16:12:07.539570634 -0700\n\n+++ .terraform.lock.hcl 2020-10-07 16:12:15.267487237 -0700\n\n@@ -6,26 +6,6 @@\n\n   ]\n\n }\n\n\n\n-provider \"registry.terraform.io/hashicorp/azurerm\" {\n\n-  version     = \"2.30.0\"\n\n-  constraints = \"~> 2.12\"\n\n-  hashes = [\n\n-    \"h1:FJwsuowaG5CIdZ0WQyFZH9r6kIJeRKts9+GcRsTz1+Y=\",\n\n-    \"h1:c/ntSXrDYM1mUir2KufijYebPcwKqS9CRGd3duDSGfY=\",\n\n-    \"h1:yre4Ph76g9H84MbuhZ2z5MuldjSA4FsrX6538O7PCcY=\",\n\n-    \"zh:04f0a50bb2ba92f3bea6f0a9e549ace5a4c13ef0cbb6975494cac0ef7d4acb43\",\n\n-    \"zh:2082e12548ebcdd6fd73580e83f626ed4ed13f8cdfd51205d8696ffe54f30734\",\n\n-    \"zh:246bcc449e9a92679fb30f3c0a77f05513886565e2dcc66b16c4486f51533064\",\n\n-    \"zh:24de3930625ac9014594d79bfa42d600eca65e9022b9668b54bfd0d924e21d14\",\n\n-    \"zh:2a22893a576ff6f268d9bf81cf4a56406f7ba79f77826f6df51ee787f6d2840a\",\n\n-    \"zh:2b27485e19c2aaa9f15f29c4cff46154a9720647610171e30fc6c18ddc42ec28\",\n\n-    \"zh:435f24ce1fb2b63f7f02aa3c84ac29c5757cd29ec4d297ed0618423387fe7bd4\",\n\n-    \"zh:7d99725923de5240ff8b34b5510569aa4ebdc0bdb27b7bac2aa911a8037a3893\",\n\n-    \"zh:7e3b5d0af3b7411dd9dc65ec9ab6caee8c191aee0fa7f20fc4f51716e67f50c0\",\n\n-    \"zh:da0af4552bef5a29b88f6a0718253f3bf71ce471c959816eb7602b0dadb469ca\",\n\n-  ]\n\n-}\n\n-\n\n provider \"registry.terraform.io/newrelic/newrelic\" {\n\n   version     = \"2.1.2\"\n\n   constraints = \"~> 2.1.1\"\n\nCopy\n\nIf you add a new requirement for the same provider at a later date and run terraform init again, Terraform will treat it as if it were an entirely new provider and so will not necessarily select the same version that was previously selected and will not be able to verify that the checksums remained unchanged.\n\nNote: In Terraform v1.0 and earlier, terraform init does not automatically remove now-unneeded providers from the lock file, and instead just ignores them. If you removed a provider dependency while using an earlier version of Terraform and then upgraded to Terraform v1.1 or later then you may see the error \"missing or corrupted provider plugins\", referring to the stale lock file entries. If so, run terraform init with the new Terraform version to tidy those unneeded entries and then retry the previous operation.\n\nEdit this page on GitHub\n\nOn this page:\n\nDependency Lock File\nLock File Location\nDependency Installation Behavior\nUnderstanding Lock File Changes\nTheme\nDark\nLight\nSystem\nCertifications\nSystem Status\nCookie Manager\nTerms of Use\nSecurity\nPrivacy\nTrademark Policy\nTrade Controls\nGive Feedback\n(opens in new tab)\nWe use cookies & other similar technology to collect data to improve your experience on our site, as described in our Privacy Policy and Cookie Policy.\nManage PreferencesACCEPT"
  },
  {
    "title": "Provider Requirements - Configuration Language | Terraform | HashiCorp Developer",
    "url": "https://developer.hashicorp.com/terraform/language/providers/requirements",
    "html": "Skip to main content\nNews\nHashiCorp to join IBM to accelerate multi-cloud automation.\nRead the blog →\nDismiss alert\nTerraform\nInstall\nTutorials\nDocumentation\nRegistry\n(opens in new tab)\nTry Cloud\n(opens in new tab)\nSearch\nCommand or control key\nK key\nTerraform Home\nConfiguration Language\nConfiguration Language\nStyle Guide\nFiles and Directories\nSyntax\nResources\nData Sources\nProviders\nOverview\nProvider Configuration\nProvider Requirements\nDependency Lock File\nVariables and Outputs\nModules\nMoved block\nChecks\nImport\nExpressions\nFunctions\nTerraform Settings\nState\nTests\nUpgrading to Terraform v1.7\nv1.x Compatibility Promises\nTerraform Internals\nResources\nTutorial Library\nCertifications\nCommunity Forum\n(opens in new tab)\nSupport\n(opens in new tab)\nGitHub\n(opens in new tab)\nTerraform Registry\n(opens in new tab)\nDeveloper\nTerraform\nConfiguration Language\nProviders\nProvider Requirements\nv1.8.x (latest)\nProvider Requirements\n\nTerraform relies on plugins called \"providers\" to interact with remote systems. Terraform configurations must declare which providers they require, so that Terraform can install and use them. This page documents how to declare providers so Terraform can install them.\n\nHands-on: Try the Perform CRUD Operations with Providers tutorial.\n\nAdditionally, some providers require configuration (like endpoint URLs or cloud regions) before they can be used. The Provider Configuration page documents how to configure settings for providers.\n\nNote: This page is about a feature of Terraform 0.13 and later; it also describes how to use the more limited version of that feature that was available in Terraform 0.12.\n\nRequiring Providers\n\nEach Terraform module must declare which providers it requires, so that Terraform can install and use them. Provider requirements are declared in a required_providers block.\n\nA provider requirement consists of a local name, a source location, and a version constraint:\n\nterraform {\n\n  required_providers {\n\n    mycloud = {\n\n      source  = \"mycorp/mycloud\"\n\n      version = \"~> 1.0\"\n\n    }\n\n  }\n\n}\n\nCopy\n\nThe required_providers block must be nested inside the top-level terraform block (which can also contain other settings).\n\nEach argument in the required_providers block enables one provider. The key determines the provider's local name (its unique identifier within this module), and the value is an object with the following elements:\n\nsource - the global source address for the provider you intend to use, such as hashicorp/aws.\n\nversion - a version constraint specifying which subset of available provider versions the module is compatible with.\n\nNote: The name = { source, version } syntax for required_providers was added in Terraform v0.13. Previous versions of Terraform used a version constraint string instead of an object (like mycloud = \"~> 1.0\"), and had no way to specify provider source addresses. If you want to write a module that works with both Terraform v0.12 and v0.13, see v0.12-Compatible Provider Requirements below.\n\nNames and Addresses\n\nEach provider has two identifiers:\n\nA unique source address, which is only used when requiring a provider.\nA local name, which is used everywhere else in a Terraform module.\n\nNote: Prior to Terraform 0.13, providers only had local names, since Terraform could only automatically download providers distributed by HashiCorp.\n\nLocal Names\n\nLocal names are module-specific, and are assigned when requiring a provider. Local names must be unique per-module.\n\nOutside of the required_providers block, Terraform configurations always refer to providers by their local names. For example, the following configuration declares mycloud as the local name for mycorp/mycloud, then uses that local name when configuring the provider:\n\nterraform {\n\n  required_providers {\n\n    mycloud = {\n\n      source  = \"mycorp/mycloud\"\n\n      version = \"~> 1.0\"\n\n    }\n\n  }\n\n}\n\n\n\nprovider \"mycloud\" {\n\n  # ...\n\n}\n\nCopy\n\nUsers of a provider can choose any local name for it. However, nearly every provider has a preferred local name, which it uses as a prefix for all of its resource types. (For example, resources from hashicorp/aws all begin with aws, like aws_instance or aws_security_group.)\n\nWhenever possible, you should use a provider's preferred local name. This makes your configurations easier to understand, and lets you omit the provider meta-argument from most of your resources. (If a resource doesn't specify which provider configuration to use, Terraform interprets the first word of the resource type as a local provider name.)\n\nSource Addresses\n\nA provider's source address is its global identifier. It also specifies the primary location where Terraform can download it.\n\nSource addresses consist of three parts delimited by slashes (/), as follows:\n\n[<HOSTNAME>/]<NAMESPACE>/<TYPE>\n\nExamples of valid provider source address formats include:\n\nNAMESPACE/TYPE\nHOSTNAME/NAMESPACE/TYPE\n\nHostname (optional): The hostname of the Terraform registry that distributes the provider. If omitted, this defaults to registry.terraform.io, the hostname of the public Terraform Registry.\n\nNamespace: An organizational namespace within the specified registry. For the public Terraform Registry and for HCP Terraform's private registry, this represents the organization that publishes the provider. This field may have other meanings for other registry hosts.\n\nType: A short name for the platform or system the provider manages. Must be unique within a particular namespace on a particular registry host.\n\nThe type is usually the provider's preferred local name. (There are exceptions; for example, hashicorp/google-beta is an alternate release channel for hashicorp/google, so its preferred local name is google. If in doubt, check the provider's documentation.)\n\nFor example, the official HTTP provider belongs to the hashicorp namespace on registry.terraform.io, so its source address is registry.terraform.io/hashicorp/http or, more commonly, just hashicorp/http.\n\nThe source address with all three components given explicitly is called the provider's fully-qualified address. You will see fully-qualified address in various outputs, like error messages, but in most cases a simplified display version is used. This display version omits the source host when it is the public registry, so you may see the shortened version \"hashicorp/random\" instead of \"registry.terraform.io/hashicorp/random\".\n\nNote: If you omit the source argument when requiring a provider, Terraform uses an implied source address of registry.terraform.io/hashicorp/<LOCAL NAME>. This is a backward compatibility feature to support the transition to Terraform 0.13; in modules that require 0.13 or later, we recommend using explicit source addresses for all providers.\n\nHandling Local Name Conflicts\n\nWhenever possible, we recommend using a provider's preferred local name, which is usually the same as the \"type\" portion of its source address.\n\nHowever, it's sometimes necessary to use two providers with the same preferred local name in the same module, usually when the providers are named after a generic infrastructure type. Terraform requires unique local names for each provider in a module, so you'll need to use a non-preferred name for at least one of them.\n\nWhen this happens, we recommend combining each provider's namespace with its type name to produce compound local names with a dash:\n\nterraform {\n\n  required_providers {\n\n    # In the rare situation of using two providers that\n\n    # have the same type name -- \"http\" in this example --\n\n    # use a compound local name to distinguish them.\n\n    hashicorp-http = {\n\n      source  = \"hashicorp/http\"\n\n      version = \"~> 2.0\"\n\n    }\n\n    mycorp-http = {\n\n      source  = \"mycorp/http\"\n\n      version = \"~> 1.0\"\n\n    }\n\n  }\n\n}\n\n\n\n# References to these providers elsewhere in the\n\n# module will use these compound local names.\n\nprovider \"mycorp-http\" {\n\n  # ...\n\n}\n\n\n\ndata \"http\" \"example\" {\n\n  provider = hashicorp-http\n\n  #...\n\n}\n\nCopy\n\nTerraform won't be able to guess either provider's name from its resource types, so you'll need to specify a provider meta-argument for every affected resource. However, readers and maintainers of your module will be able to easily understand what's happening, and avoiding confusion is much more important than avoiding typing.\n\nVersion Constraints\n\nEach provider plugin has its own set of available versions, allowing the functionality of the provider to evolve over time. Each provider dependency you declare should have a version constraint given in the version argument so Terraform can select a single version per provider that all modules are compatible with.\n\nThe version argument is optional; if omitted, Terraform will accept any version of the provider as compatible. However, we strongly recommend specifying a version constraint for every provider your module depends on.\n\nTo ensure Terraform always installs the same provider versions for a given configuration, you can use Terraform CLI to create a dependency lock file and commit it to version control along with your configuration. If a lock file is present, HCP Terraform, CLI, and Enterprise will all obey it when installing providers.\n\nHands-on: Try the Lock and Upgrade Provider Versions tutorial.\n\nBest Practices for Provider Versions\n\nEach module should at least declare the minimum provider version it is known to work with, using the >= version constraint syntax:\n\nterraform {\n\n  required_providers {\n\n    mycloud = {\n\n      source  = \"hashicorp/aws\"\n\n      version = \">= 1.0\"\n\n    }\n\n  }\n\n}\n\nCopy\n\nA module intended to be used as the root of a configuration — that is, as the directory where you'd run terraform apply — should also specify the maximum provider version it is intended to work with, to avoid accidental upgrades to incompatible new versions. The ~> operator is a convenient shorthand for allowing the rightmost component of a version to increment. The following example uses the operator to allow only patch releases within a specific minor release:\n\nterraform {\n\n  required_providers {\n\n    mycloud = {\n\n      source  = \"hashicorp/aws\"\n\n      version = \"~> 1.0.4\"\n\n    }\n\n  }\n\n}\n\nCopy\n\nDo not use ~> (or other maximum-version constraints) for modules you intend to reuse across many configurations, even if you know the module isn't compatible with certain newer versions. Doing so can sometimes prevent errors, but more often it forces users of the module to update many modules simultaneously when performing routine upgrades. Specify a minimum version, document any known incompatibilities, and let the root module manage the maximum version.\n\nBuilt-in Providers\n\nMost Terraform providers are distributed separately as plugins, but there is one provider that is built into Terraform itself. This provider enables the the terraform_remote_state data source.\n\nBecause this provider is built in to Terraform, you don't need to declare it in the required_providers block in order to use its features. However, for consistency it does have a special provider source address, which is terraform.io/builtin/terraform. This address may sometimes appear in Terraform's error messages and other output in order to unambiguously refer to the built-in provider, as opposed to a hypothetical third-party provider with the type name \"terraform\".\n\nThere is also an existing provider with the source address hashicorp/terraform, which is an older version of the now-built-in provider that was used by older versions of Terraform. hashicorp/terraform is not compatible with Terraform v0.11 or later and should never be declared in a required_providers block.\n\nIn-house Providers\n\nAnyone can develop and distribute their own Terraform providers. See the Call APIs with Terraform Providers tutorials for more about provider development.\n\nSome organizations develop their own providers to configure proprietary systems, and wish to use these providers from Terraform without publishing them on the public Terraform Registry.\n\nOne option for distributing such a provider is to run an in-house private registry, by implementing the provider registry protocol.\n\nRunning an additional service just to distribute a single provider internally may be undesirable, so Terraform also supports other provider installation methods, including placing provider plugins directly in specific directories in the local filesystem, via filesystem mirrors.\n\nAll providers must have a source address that includes (or implies) the hostname of a registry, but that hostname does not need to provide an actual registry service. For in-house providers that you intend to distribute from a local filesystem directory, you can use an arbitrary hostname in a domain your organization controls.\n\nFor example, if your corporate domain were example.com then you might choose to use terraform.example.com as your placeholder hostname, even if that hostname doesn't actually resolve in DNS. You can then choose any namespace and type you wish to represent your in-house provider under that hostname, giving a source address like terraform.example.com/examplecorp/ourcloud:\n\nterraform {\n\n  required_providers {\n\n    mycloud = {\n\n      source  = \"terraform.example.com/examplecorp/ourcloud\"\n\n      version = \">= 1.0\"\n\n    }\n\n  }\n\n}\n\nCopy\n\nTo make version 1.0.0 of this provider available for installation from the local filesystem, choose one of the implied local mirror directories and create a directory structure under it like this:\n\nterraform.example.com/examplecorp/ourcloud/1.0.0\n\nCopy\n\nUnder that 1.0.0 directory, create one additional directory representing the platform where you are running Terraform, such as linux_amd64 for Linux on an AMD64/x64 processor, and then place the provider plugin executable and any other needed files in that directory.\n\nThus, on a Windows system, the provider plugin executable file might be at the following path:\n\nterraform.example.com/examplecorp/ourcloud/1.0.0/windows_amd64/terraform-provider-ourcloud.exe\n\nCopy\n\nIf you later decide to switch to using a real private provider registry rather than distribute binaries out of band, you can deploy the registry server at terraform.example.com and retain the same namespace and type names, in which case your existing modules will require no changes to locate the same provider using your registry server.\n\nv0.12-Compatible Provider Requirements\n\nExplicit provider source addresses were introduced with Terraform v0.13, so the full provider requirements syntax is not supported by Terraform v0.12.\n\nHowever, in order to allow writing modules that are compatible with both Terraform v0.12 and v0.13, versions of Terraform between v0.12.26 and v0.13 will accept but ignore the source argument in a required_providers block.\n\nConsider the following example written for Terraform v0.13:\n\nterraform {\n\n  required_providers {\n\n    aws = {\n\n      source  = \"hashicorp/aws\"\n\n      version = \"~> 1.0\"\n\n    }\n\n  }\n\n}\n\nCopy\n\nTerraform v0.12.26 will accept syntax like the above but will understand it in the same way as the following v0.12-style syntax:\n\nterraform {\n\n  required_providers {\n\n    aws = \"~> 1.0\"\n\n  }\n\n}\n\nCopy\n\nIn other words, Terraform v0.12.26 ignores the source argument and considers only the version argument, using the given local name as the un-namespaced provider type to install.\n\nWhen writing a module that is compatible with both Terraform v0.12.26 and Terraform v0.13.0 or later, you must follow the following additional rules so that both versions will select the same provider to install:\n\nUse only providers that can be automatically installed by Terraform v0.12. Third-party providers, such as community providers in the Terraform Registry, cannot be selected by Terraform v0.12 because it does not support the hierarchical source address namespace.\n\nEnsure that your chosen local name exactly matches the \"type\" portion of the source address given in the source argument, such as both being \"aws\" in the examples above, because Terraform v0.12 will use the local name to determine which provider plugin to download and install.\n\nIf the provider belongs to the hashicorp namespace, as with the hashicorp/aws provider shown above, omit the source argument and allow Terraform v0.13 to select the hashicorp namespace by default.\n\nProvider type names must always be written in lowercase. Terraform v0.13 treats provider source addresses as case-insensitive, but Terraform v0.12 considers its legacy-style provider names to be case-sensitive. Using lowercase will ensure that the name is selectable by both Terraform major versions.\n\nThis compatibility mechanism is provided as a temporary transitional aid only. When Terraform v0.12 detects a use of the new source argument it doesn't understand, it will emit a warning to alert the user that it is disregarding the source address given in that argument.\n\nEdit this page on GitHub\n\nOn this page:\n\nProvider Requirements\nRequiring Providers\nNames and Addresses\nVersion Constraints\nBuilt-in Providers\nIn-house Providers\nv0.12-Compatible Provider Requirements\nTheme\nDark\nLight\nSystem\nCertifications\nSystem Status\nCookie Manager\nTerms of Use\nSecurity\nPrivacy\nTrademark Policy\nTrade Controls\nGive Feedback\n(opens in new tab)\nWe use cookies & other similar technology to collect data to improve your experience on our site, as described in our Privacy Policy and Cookie Policy.\nManage PreferencesACCEPT"
  },
  {
    "title": "Provider Configuration - Configuration Language | Terraform | HashiCorp Developer",
    "url": "https://developer.hashicorp.com/terraform/language/providers/configuration",
    "html": "Skip to main content\nNews\nHashiCorp to join IBM to accelerate multi-cloud automation.\nRead the blog →\nDismiss alert\nTerraform\nInstall\nTutorials\nDocumentation\nRegistry\n(opens in new tab)\nTry Cloud\n(opens in new tab)\nSearch\nCommand or control key\nK key\nTerraform Home\nConfiguration Language\nConfiguration Language\nStyle Guide\nFiles and Directories\nSyntax\nResources\nData Sources\nProviders\nOverview\nProvider Configuration\nProvider Requirements\nDependency Lock File\nVariables and Outputs\nModules\nMoved block\nChecks\nImport\nExpressions\nFunctions\nTerraform Settings\nState\nTests\nUpgrading to Terraform v1.7\nv1.x Compatibility Promises\nTerraform Internals\nResources\nTutorial Library\nCertifications\nCommunity Forum\n(opens in new tab)\nSupport\n(opens in new tab)\nGitHub\n(opens in new tab)\nTerraform Registry\n(opens in new tab)\nDeveloper\nTerraform\nConfiguration Language\nProviders\nProvider Configuration\nv1.8.x (latest)\nProvider Configuration\n\nProviders allow Terraform to interact with cloud providers, SaaS providers, and other APIs.\n\nSome providers require you to configure them with endpoint URLs, cloud regions, or other settings before Terraform can use them. This page documents how to configure settings for providers.\n\nAdditionally, all Terraform configurations must declare which providers they require so that Terraform can install and use them. The Provider Requirements page documents how to declare providers so Terraform can install them.\n\nProvider Configuration\n\nProvider configurations belong in the root module of a Terraform configuration. (Child modules receive their provider configurations from the root module; for more information, see The Module providers Meta-Argument and Module Development: Providers Within Modules.)\n\nA provider configuration is created using a provider block:\n\nprovider \"google\" {\n\n  project = \"acme-app\"\n\n  region  = \"us-central1\"\n\n}\n\nCopy\n\nThe name given in the block header (\"google\" in this example) is the local name of the provider to configure. This provider should already be included in a required_providers block.\n\nThe body of the block (between { and }) contains configuration arguments for the provider. Most arguments in this section are defined by the provider itself; in this example both project and region are specific to the google provider.\n\nYou can use expressions in the values of these configuration arguments, but can only reference values that are known before the configuration is applied. This means you can safely reference input variables, but not attributes exported by resources (with an exception for resource arguments that are specified directly in the configuration).\n\nA provider's documentation should list which configuration arguments it expects. For providers distributed on the Terraform Registry, versioned documentation is available on each provider's page, via the \"Documentation\" link in the provider's header.\n\nSome providers can use shell environment variables (or other alternate sources, like VM instance profiles) as values for some of their arguments; when available, we recommend using this as a way to keep credentials out of your version-controlled Terraform code.\n\nThere are also two \"meta-arguments\" that are defined by Terraform itself and available for all provider blocks:\n\nalias, for using the same provider with different configurations for different resources\nversion, which we no longer recommend (use provider requirements instead)\n\nUnlike many other objects in the Terraform language, a provider block may be omitted if its contents would otherwise be empty. Terraform assumes an empty default configuration for any provider that is not explicitly configured.\n\nalias: Multiple Provider Configurations\n\nYou can optionally define multiple configurations for the same provider, and select which one to use on a per-resource or per-module basis. The primary reason for this is to support multiple regions for a cloud platform; other examples include targeting multiple Docker hosts, multiple Consul hosts, etc.\n\nTo create multiple configurations for a given provider, include multiple provider blocks with the same provider name. For each additional non-default configuration, use the alias meta-argument to provide an extra name segment. For example:\n\n# The default provider configuration; resources that begin with `aws_` will use\n\n# it as the default, and it can be referenced as `aws`.\n\nprovider \"aws\" {\n\n  region = \"us-east-1\"\n\n}\n\n\n\n# Additional provider configuration for west coast region; resources can\n\n# reference this as `aws.west`.\n\nprovider \"aws\" {\n\n  alias  = \"west\"\n\n  region = \"us-west-2\"\n\n}\n\nCopy\n\nTo declare a configuration alias within a module in order to receive an alternate provider configuration from the parent module, add the configuration_aliases argument to that provider's required_providers entry. The following example declares both the mycloud and mycloud.alternate provider configuration names within the containing module:\n\nterraform {\n\n  required_providers {\n\n    mycloud = {\n\n      source  = \"mycorp/mycloud\"\n\n      version = \"~> 1.0\"\n\n      configuration_aliases = [ mycloud.alternate ]\n\n    }\n\n  }\n\n}\n\nCopy\nDefault Provider Configurations\n\nA provider block without an alias argument is the default configuration for that provider. Resources that don't set the provider meta-argument will use the default provider configuration that matches the first word of the resource type name. (For example, an aws_instance resource uses the default aws provider configuration unless otherwise stated.)\n\nIf every explicit configuration of a provider has an alias, Terraform uses the implied empty configuration as that provider's default configuration. (If the provider has any required configuration arguments, Terraform will raise an error when resources default to the empty configuration.)\n\nReferring to Alternate Provider Configurations\n\nWhen Terraform needs the name of a provider configuration, it expects a reference of the form <PROVIDER NAME>.<ALIAS>. In the example above, aws.west would refer to the provider with the us-west-2 region.\n\nThese references are special expressions. Like references to other named entities (for example, var.image_id), they aren't strings and don't need to be quoted. But they are only valid in specific meta-arguments of resource, data, and module blocks, and can't be used in arbitrary expressions.\n\nSelecting Alternate Provider Configurations\n\nBy default, resources use a default provider configuration (one without an alias argument) inferred from the first word of the resource type name.\n\nTo use an alternate provider configuration for a resource or data source, set its provider meta-argument to a <PROVIDER NAME>.<ALIAS> reference:\n\nresource \"aws_instance\" \"foo\" {\n\n  provider = aws.west\n\n\n\n  # ...\n\n}\n\nCopy\n\nTo select alternate provider configurations for a child module, use its providers meta-argument to specify which provider configurations should be mapped to which local provider names inside the module:\n\nmodule \"aws_vpc\" {\n\n  source = \"./aws_vpc\"\n\n  providers = {\n\n    aws = aws.west\n\n  }\n\n}\n\nCopy\n\nModules have some special requirements when passing in providers; see The Module providers Meta-Argument for more details. In most cases, only root modules should define provider configurations, with all child modules obtaining their provider configurations from their parents.\n\nversion (Deprecated)\n\nThe version meta-argument specifies a version constraint for a provider, and works the same way as the version argument in a required_providers block. The version constraint in a provider configuration is only used if required_providers does not include one for that provider.\n\n~Warning: The version argument in provider configurations is deprecated, and we will remove it in a future Terraform version.\n\nIn Terraform 0.13 and later, always declare provider version constraints in the required_providers block.\n\nEdit this page on GitHub\n\nOn this page:\n\nProvider Configuration\nProvider Configuration\nalias: Multiple Provider Configurations\nversion (Deprecated)\nTheme\nDark\nLight\nSystem\nCertifications\nSystem Status\nCookie Manager\nTerms of Use\nSecurity\nPrivacy\nTrademark Policy\nTrade Controls\nGive Feedback\n(opens in new tab)\nWe use cookies & other similar technology to collect data to improve your experience on our site, as described in our Privacy Policy and Cookie Policy.\nManage PreferencesACCEPT"
  },
  {
    "title": "Overview - Configuration Language | Terraform | HashiCorp Developer",
    "url": "https://developer.hashicorp.com/terraform/language/v1.1.x",
    "html": "Skip to main content\nNews\nHashiCorp to join IBM to accelerate multi-cloud automation.\nRead the blog →\nDismiss alert\nTerraform\nInstall\nTutorials\nDocumentation\nRegistry\n(opens in new tab)\nTry Cloud\n(opens in new tab)\nSearch\nCommand or control key\nK key\nTerraform Home\nConfiguration Language\nConfiguration Language\nTerraform Language\nOverview\nFiles and Directories\nSyntax\nResources\nData Sources\nProviders\nVariables and Outputs\nModules\nExpressions\nFunctions\nTerraform Settings\nState\nUpgrade Guides\nHistorical docs: 0.11 and Older\nResources\nTutorial Library\nCertifications\nCommunity Forum\n(opens in new tab)\nSupport\n(opens in new tab)\nGitHub\n(opens in new tab)\nTerraform Registry\n(opens in new tab)\n\nYou are viewing documentation for version v1.1 and earlier. View latest version.\n\nDeveloper\nTerraform\nConfiguration Language v1.1.x\nv1.1 and earlier\nTerraform Language Documentation\n\nThis is the documentation for Terraform's configuration language. It is relevant to users of Terraform CLI, Terraform Cloud, and Terraform Enterprise. Terraform's language is its primary user interface. Configuration files you write in Terraform language tell Terraform what plugins to install, what infrastructure to create, and what data to fetch. Terraform language also lets you define dependencies between resources and create multiple similar resources from a single configuration block.\n\nHands-on: Try the Write Terraform Configuration tutorials on HashiCorp Learn.\n\nAbout the Terraform Language\n\nThe main purpose of the Terraform language is declaring resources, which represent infrastructure objects. All other language features exist only to make the definition of resources more flexible and convenient.\n\nA Terraform configuration is a complete document in the Terraform language that tells Terraform how to manage a given collection of infrastructure. A configuration can consist of multiple files and directories.\n\nThe syntax of the Terraform language consists of only a few basic elements:\n\nresource \"aws_vpc\" \"main\" {\n\n  cidr_block = var.base_cidr_block\n\n}\n\n\n\n<BLOCK TYPE> \"<BLOCK LABEL>\" \"<BLOCK LABEL>\" {\n\n  # Block body\n\n  <IDENTIFIER> = <EXPRESSION> # Argument\n\n}\n\nCopy\nBlocks are containers for other content and usually represent the configuration of some kind of object, like a resource. Blocks have a block type, can have zero or more labels, and have a body that contains any number of arguments and nested blocks. Most of Terraform's features are controlled by top-level blocks in a configuration file.\nArguments assign a value to a name. They appear within blocks.\nExpressions represent a value, either literally or by referencing and combining other values. They appear as values for arguments, or within other expressions.\n\nThe Terraform language is declarative, describing an intended goal rather than the steps to reach that goal. The ordering of blocks and the files they are organized into are generally not significant; Terraform only considers implicit and explicit relationships between resources when determining an order of operations.\n\nExample\n\nThe following example describes a simple network topology for Amazon Web Services, just to give a sense of the overall structure and syntax of the Terraform language. Similar configurations can be created for other virtual network services, using resource types defined by other providers, and a practical network configuration will often contain additional elements not shown here.\n\nterraform {\n\n  required_providers {\n\n    aws = {\n\n      source  = \"hashicorp/aws\"\n\n      version = \"~> 1.0.4\"\n\n    }\n\n  }\n\n}\n\n\n\nvariable \"aws_region\" {}\n\n\n\nvariable \"base_cidr_block\" {\n\n  description = \"A /16 CIDR range definition, such as 10.1.0.0/16, that the VPC will use\"\n\n  default = \"10.1.0.0/16\"\n\n}\n\n\n\nvariable \"availability_zones\" {\n\n  description = \"A list of availability zones in which to create subnets\"\n\n  type = list(string)\n\n}\n\n\n\nprovider \"aws\" {\n\n  region = var.aws_region\n\n}\n\n\n\nresource \"aws_vpc\" \"main\" {\n\n  # Referencing the base_cidr_block variable allows the network address\n\n  # to be changed without modifying the configuration.\n\n  cidr_block = var.base_cidr_block\n\n}\n\n\n\nresource \"aws_subnet\" \"az\" {\n\n  # Create one subnet for each given availability zone.\n\n  count = length(var.availability_zones)\n\n\n\n  # For each subnet, use one of the specified availability zones.\n\n  availability_zone = var.availability_zones[count.index]\n\n\n\n  # By referencing the aws_vpc.main object, Terraform knows that the subnet\n\n  # must be created only after the VPC is created.\n\n  vpc_id = aws_vpc.main.id\n\n\n\n  # Built-in functions and operators can be used for simple transformations of\n\n  # values, such as computing a subnet address. Here we create a /20 prefix for\n\n  # each subnet, using consecutive addresses for each availability zone,\n\n  # such as 10.1.16.0/20 .\n\n  cidr_block = cidrsubnet(aws_vpc.main.cidr_block, 4, count.index+1)\n\n}\n\nCopy\n\nOn this page:\n\nTerraform Language Documentation\nAbout the Terraform Language\nTheme\nDark\nLight\nSystem\nCertifications\nSystem Status\nCookie Manager\nTerms of Use\nSecurity\nPrivacy\nTrademark Policy\nTrade Controls\nGive Feedback\n(opens in new tab)\nWe use cookies & other similar technology to collect data to improve your experience on our site, as described in our Privacy Policy and Cookie Policy.\nManage PreferencesACCEPT"
  },
  {
    "title": "Resources Overview - Configuration Language | Terraform | HashiCorp Developer",
    "url": "https://developer.hashicorp.com/terraform/language/resources",
    "html": "Skip to main content\nNews\nHashiCorp to join IBM to accelerate multi-cloud automation.\nRead the blog →\nDismiss alert\nTerraform\nInstall\nTutorials\nDocumentation\nRegistry\n(opens in new tab)\nTry Cloud\n(opens in new tab)\nSearch\nCommand or control key\nK key\nTerraform Home\nConfiguration Language\nConfiguration Language\nStyle Guide\nFiles and Directories\nSyntax\nResources\nOverview\nResource Blocks\nResource Behavior\nMeta-Arguments\nProvisioners\nThe terraform_data Resource Type\nData Sources\nProviders\nVariables and Outputs\nModules\nMoved block\nChecks\nImport\nExpressions\nFunctions\nTerraform Settings\nState\nTests\nUpgrading to Terraform v1.7\nv1.x Compatibility Promises\nTerraform Internals\nResources\nTutorial Library\nCertifications\nCommunity Forum\n(opens in new tab)\nSupport\n(opens in new tab)\nGitHub\n(opens in new tab)\nTerraform Registry\n(opens in new tab)\nDeveloper\nTerraform\nConfiguration Language\nResources\nv1.8.x (latest)\nResources\n\nHands-on: Try the Terraform: Get Started tutorials.\n\nResources are the most important element in the Terraform language. Each resource block describes one or more infrastructure objects, such as virtual networks, compute instances, or higher-level components such as DNS records.\n\nResource Blocks documents the syntax for declaring resources.\n\nResource Behavior explains in more detail how Terraform handles resource declarations when applying a configuration.\n\nThe Meta-Arguments section documents special arguments that can be used with every resource type, including depends_on, count, for_each, provider, and lifecycle.\n\nProvisioners documents configuring post-creation actions for a resource using the provisioner and connection blocks. Since provisioners are non-declarative and potentially unpredictable, we strongly recommend that you treat them as a last resort.\n\nEdit this page on GitHub\nTheme\nDark\nLight\nSystem\nCertifications\nSystem Status\nCookie Manager\nTerms of Use\nSecurity\nPrivacy\nTrademark Policy\nTrade Controls\nGive Feedback\n(opens in new tab)\nWe use cookies & other similar technology to collect data to improve your experience on our site, as described in our Privacy Policy and Cookie Policy.\nManage PreferencesACCEPT"
  },
  {
    "title": "Overview - Configuration Language | Terraform | HashiCorp Developer",
    "url": "https://developer.hashicorp.com/terraform/language/v1.3.x",
    "html": "Skip to main content\nNews\nHashiCorp to join IBM to accelerate multi-cloud automation.\nRead the blog →\nDismiss alert\nTerraform\nInstall\nTutorials\nDocumentation\nRegistry\n(opens in new tab)\nTry Cloud\n(opens in new tab)\nSearch\nCommand or control key\nK key\nTerraform Home\nConfiguration Language\nConfiguration Language\nTerraform Language\nOverview\nStyle Guide\nFiles and Directories\nSyntax\nResources\nData Sources\nProviders\nVariables and Outputs\nModules\nMoved block\nExpressions\nFunctions\nTerraform Settings\nState\nUpgrading to Terraform v1.3\nv1.x Compatibility Promises\nTerraform Internals\nResources\nTutorial Library\nCertifications\nCommunity Forum\n(opens in new tab)\nSupport\n(opens in new tab)\nGitHub\n(opens in new tab)\nTerraform Registry\n(opens in new tab)\n\nYou are viewing documentation for version v1.3.x. View latest version.\n\nDeveloper\nTerraform\nConfiguration Language v1.3.x\nv1.3.x\nTerraform Language Documentation\n\nThis is the documentation for Terraform's configuration language. It is relevant to users of Terraform CLI, Terraform Cloud, and Terraform Enterprise. Terraform's language is its primary user interface. Configuration files you write in Terraform language tell Terraform what plugins to install, what infrastructure to create, and what data to fetch. Terraform language also lets you define dependencies between resources and create multiple similar resources from a single configuration block.\n\nHands-on: Try the Write Terraform Configuration tutorials.\n\nAbout the Terraform Language\n\nThe main purpose of the Terraform language is declaring resources, which represent infrastructure objects. All other language features exist only to make the definition of resources more flexible and convenient.\n\nA Terraform configuration is a complete document in the Terraform language that tells Terraform how to manage a given collection of infrastructure. A configuration can consist of multiple files and directories.\n\nThe syntax of the Terraform language consists of only a few basic elements:\n\nresource \"aws_vpc\" \"main\" {\n\n  cidr_block = var.base_cidr_block\n\n}\n\n\n\n<BLOCK TYPE> \"<BLOCK LABEL>\" \"<BLOCK LABEL>\" {\n\n  # Block body\n\n  <IDENTIFIER> = <EXPRESSION> # Argument\n\n}\n\nCopy\nBlocks are containers for other content and usually represent the configuration of some kind of object, like a resource. Blocks have a block type, can have zero or more labels, and have a body that contains any number of arguments and nested blocks. Most of Terraform's features are controlled by top-level blocks in a configuration file.\nArguments assign a value to a name. They appear within blocks.\nExpressions represent a value, either literally or by referencing and combining other values. They appear as values for arguments, or within other expressions.\n\nThe Terraform language is declarative, describing an intended goal rather than the steps to reach that goal. The ordering of blocks and the files they are organized into are generally not significant; Terraform only considers implicit and explicit relationships between resources when determining an order of operations.\n\nExample\n\nThe following example describes a simple network topology for Amazon Web Services, just to give a sense of the overall structure and syntax of the Terraform language. Similar configurations can be created for other virtual network services, using resource types defined by other providers, and a practical network configuration will often contain additional elements not shown here.\n\nterraform {\n\n  required_providers {\n\n    aws = {\n\n      source  = \"hashicorp/aws\"\n\n      version = \"~> 1.0.4\"\n\n    }\n\n  }\n\n}\n\n\n\nvariable \"aws_region\" {}\n\n\n\nvariable \"base_cidr_block\" {\n\n  description = \"A /16 CIDR range definition, such as 10.1.0.0/16, that the VPC will use\"\n\n  default = \"10.1.0.0/16\"\n\n}\n\n\n\nvariable \"availability_zones\" {\n\n  description = \"A list of availability zones in which to create subnets\"\n\n  type = list(string)\n\n}\n\n\n\nprovider \"aws\" {\n\n  region = var.aws_region\n\n}\n\n\n\nresource \"aws_vpc\" \"main\" {\n\n  # Referencing the base_cidr_block variable allows the network address\n\n  # to be changed without modifying the configuration.\n\n  cidr_block = var.base_cidr_block\n\n}\n\n\n\nresource \"aws_subnet\" \"az\" {\n\n  # Create one subnet for each given availability zone.\n\n  count = length(var.availability_zones)\n\n\n\n  # For each subnet, use one of the specified availability zones.\n\n  availability_zone = var.availability_zones[count.index]\n\n\n\n  # By referencing the aws_vpc.main object, Terraform knows that the subnet\n\n  # must be created only after the VPC is created.\n\n  vpc_id = aws_vpc.main.id\n\n\n\n  # Built-in functions and operators can be used for simple transformations of\n\n  # values, such as computing a subnet address. Here we create a /20 prefix for\n\n  # each subnet, using consecutive addresses for each availability zone,\n\n  # such as 10.1.16.0/20 .\n\n  cidr_block = cidrsubnet(aws_vpc.main.cidr_block, 4, count.index+1)\n\n}\n\nCopy\n\nOn this page:\n\nTerraform Language Documentation\nAbout the Terraform Language\nTheme\nDark\nLight\nSystem\nCertifications\nSystem Status\nCookie Manager\nTerms of Use\nSecurity\nPrivacy\nTrademark Policy\nTrade Controls\nGive Feedback\n(opens in new tab)\nWe use cookies & other similar technology to collect data to improve your experience on our site, as described in our Privacy Policy and Cookie Policy.\nManage PreferencesACCEPT"
  },
  {
    "title": "Overview - Configuration Language | Terraform | HashiCorp Developer",
    "url": "https://developer.hashicorp.com/terraform/language/v1.2.x",
    "html": "Skip to main content\nNews\nHashiCorp to join IBM to accelerate multi-cloud automation.\nRead the blog →\nDismiss alert\nTerraform\nInstall\nTutorials\nDocumentation\nRegistry\n(opens in new tab)\nTry Cloud\n(opens in new tab)\nSearch\nCommand or control key\nK key\nTerraform Home\nConfiguration Language\nConfiguration Language\nTerraform Language\nOverview\nStyle Guide\nFiles and Directories\nSyntax\nResources\nData Sources\nProviders\nVariables and Outputs\nModules\nExpressions\nFunctions\nTerraform Settings\nState\nUpgrading to Terraform v1.2\nv1.x Compatibility Promises\nTerraform Internals\nResources\nTutorial Library\nCertifications\nCommunity Forum\n(opens in new tab)\nSupport\n(opens in new tab)\nGitHub\n(opens in new tab)\nTerraform Registry\n(opens in new tab)\n\nYou are viewing documentation for version v1.2.x. View latest version.\n\nDeveloper\nTerraform\nConfiguration Language v1.2.x\nv1.2.x\nTerraform Language Documentation\n\nThis is the documentation for Terraform's configuration language. It is relevant to users of Terraform CLI, Terraform Cloud, and Terraform Enterprise. Terraform's language is its primary user interface. Configuration files you write in Terraform language tell Terraform what plugins to install, what infrastructure to create, and what data to fetch. Terraform language also lets you define dependencies between resources and create multiple similar resources from a single configuration block.\n\nHands-on: Try the Write Terraform Configuration tutorials.\n\nAbout the Terraform Language\n\nThe main purpose of the Terraform language is declaring resources, which represent infrastructure objects. All other language features exist only to make the definition of resources more flexible and convenient.\n\nA Terraform configuration is a complete document in the Terraform language that tells Terraform how to manage a given collection of infrastructure. A configuration can consist of multiple files and directories.\n\nThe syntax of the Terraform language consists of only a few basic elements:\n\nresource \"aws_vpc\" \"main\" {\n\n  cidr_block = var.base_cidr_block\n\n}\n\n\n\n<BLOCK TYPE> \"<BLOCK LABEL>\" \"<BLOCK LABEL>\" {\n\n  # Block body\n\n  <IDENTIFIER> = <EXPRESSION> # Argument\n\n}\n\nCopy\nBlocks are containers for other content and usually represent the configuration of some kind of object, like a resource. Blocks have a block type, can have zero or more labels, and have a body that contains any number of arguments and nested blocks. Most of Terraform's features are controlled by top-level blocks in a configuration file.\nArguments assign a value to a name. They appear within blocks.\nExpressions represent a value, either literally or by referencing and combining other values. They appear as values for arguments, or within other expressions.\n\nThe Terraform language is declarative, describing an intended goal rather than the steps to reach that goal. The ordering of blocks and the files they are organized into are generally not significant; Terraform only considers implicit and explicit relationships between resources when determining an order of operations.\n\nExample\n\nThe following example describes a simple network topology for Amazon Web Services, just to give a sense of the overall structure and syntax of the Terraform language. Similar configurations can be created for other virtual network services, using resource types defined by other providers, and a practical network configuration will often contain additional elements not shown here.\n\nterraform {\n\n  required_providers {\n\n    aws = {\n\n      source  = \"hashicorp/aws\"\n\n      version = \"~> 1.0.4\"\n\n    }\n\n  }\n\n}\n\n\n\nvariable \"aws_region\" {}\n\n\n\nvariable \"base_cidr_block\" {\n\n  description = \"A /16 CIDR range definition, such as 10.1.0.0/16, that the VPC will use\"\n\n  default = \"10.1.0.0/16\"\n\n}\n\n\n\nvariable \"availability_zones\" {\n\n  description = \"A list of availability zones in which to create subnets\"\n\n  type = list(string)\n\n}\n\n\n\nprovider \"aws\" {\n\n  region = var.aws_region\n\n}\n\n\n\nresource \"aws_vpc\" \"main\" {\n\n  # Referencing the base_cidr_block variable allows the network address\n\n  # to be changed without modifying the configuration.\n\n  cidr_block = var.base_cidr_block\n\n}\n\n\n\nresource \"aws_subnet\" \"az\" {\n\n  # Create one subnet for each given availability zone.\n\n  count = length(var.availability_zones)\n\n\n\n  # For each subnet, use one of the specified availability zones.\n\n  availability_zone = var.availability_zones[count.index]\n\n\n\n  # By referencing the aws_vpc.main object, Terraform knows that the subnet\n\n  # must be created only after the VPC is created.\n\n  vpc_id = aws_vpc.main.id\n\n\n\n  # Built-in functions and operators can be used for simple transformations of\n\n  # values, such as computing a subnet address. Here we create a /20 prefix for\n\n  # each subnet, using consecutive addresses for each availability zone,\n\n  # such as 10.1.16.0/20 .\n\n  cidr_block = cidrsubnet(aws_vpc.main.cidr_block, 4, count.index+1)\n\n}\n\nCopy\n\nOn this page:\n\nTerraform Language Documentation\nAbout the Terraform Language\nTheme\nDark\nLight\nSystem\nCertifications\nSystem Status\nCookie Manager\nTerms of Use\nSecurity\nPrivacy\nTrademark Policy\nTrade Controls\nGive Feedback\n(opens in new tab)\nWe use cookies & other similar technology to collect data to improve your experience on our site, as described in our Privacy Policy and Cookie Policy.\nManage PreferencesACCEPT"
  },
  {
    "title": "Overview - Configuration Language | Terraform | HashiCorp Developer",
    "url": "https://developer.hashicorp.com/terraform/language/v1.4.x",
    "html": "Skip to main content\nNews\nHashiCorp to join IBM to accelerate multi-cloud automation.\nRead the blog →\nDismiss alert\nTerraform\nInstall\nTutorials\nDocumentation\nRegistry\n(opens in new tab)\nTry Cloud\n(opens in new tab)\nSearch\nCommand or control key\nK key\nTerraform Home\nConfiguration Language\nConfiguration Language\nTerraform Language\nOverview\nStyle Guide\nFiles and Directories\nSyntax\nResources\nData Sources\nProviders\nVariables and Outputs\nModules\nExpressions\nFunctions\nTerraform Settings\nState\nUpgrading to Terraform v1.4\nv1.x Compatibility Promises\nTerraform Internals\nResources\nTutorial Library\nCertifications\nCommunity Forum\n(opens in new tab)\nSupport\n(opens in new tab)\nGitHub\n(opens in new tab)\nTerraform Registry\n(opens in new tab)\n\nYou are viewing documentation for version v1.4.x. View latest version.\n\nDeveloper\nTerraform\nConfiguration Language v1.4.x\nv1.4.x\nTerraform Language Documentation\n\nThis is the documentation for Terraform's configuration language. It is relevant to users of Terraform CLI, Terraform Cloud, and Terraform Enterprise. Terraform's language is its primary user interface. Configuration files you write in Terraform language tell Terraform what plugins to install, what infrastructure to create, and what data to fetch. Terraform language also lets you define dependencies between resources and create multiple similar resources from a single configuration block.\n\nHands-on: Try the Write Terraform Configuration tutorials.\n\nAbout the Terraform Language\n\nThe main purpose of the Terraform language is declaring resources, which represent infrastructure objects. All other language features exist only to make the definition of resources more flexible and convenient.\n\nA Terraform configuration is a complete document in the Terraform language that tells Terraform how to manage a given collection of infrastructure. A configuration can consist of multiple files and directories.\n\nThe syntax of the Terraform language consists of only a few basic elements:\n\nresource \"aws_vpc\" \"main\" {\n\n  cidr_block = var.base_cidr_block\n\n}\n\n\n\n<BLOCK TYPE> \"<BLOCK LABEL>\" \"<BLOCK LABEL>\" {\n\n  # Block body\n\n  <IDENTIFIER> = <EXPRESSION> # Argument\n\n}\n\nCopy\nBlocks are containers for other content and usually represent the configuration of some kind of object, like a resource. Blocks have a block type, can have zero or more labels, and have a body that contains any number of arguments and nested blocks. Most of Terraform's features are controlled by top-level blocks in a configuration file.\nArguments assign a value to a name. They appear within blocks.\nExpressions represent a value, either literally or by referencing and combining other values. They appear as values for arguments, or within other expressions.\n\nThe Terraform language is declarative, describing an intended goal rather than the steps to reach that goal. The ordering of blocks and the files they are organized into are generally not significant; Terraform only considers implicit and explicit relationships between resources when determining an order of operations.\n\nExample\n\nThe following example describes a simple network topology for Amazon Web Services, just to give a sense of the overall structure and syntax of the Terraform language. Similar configurations can be created for other virtual network services, using resource types defined by other providers, and a practical network configuration will often contain additional elements not shown here.\n\nterraform {\n\n  required_providers {\n\n    aws = {\n\n      source  = \"hashicorp/aws\"\n\n      version = \"~> 1.0.4\"\n\n    }\n\n  }\n\n}\n\n\n\nvariable \"aws_region\" {}\n\n\n\nvariable \"base_cidr_block\" {\n\n  description = \"A /16 CIDR range definition, such as 10.1.0.0/16, that the VPC will use\"\n\n  default = \"10.1.0.0/16\"\n\n}\n\n\n\nvariable \"availability_zones\" {\n\n  description = \"A list of availability zones in which to create subnets\"\n\n  type = list(string)\n\n}\n\n\n\nprovider \"aws\" {\n\n  region = var.aws_region\n\n}\n\n\n\nresource \"aws_vpc\" \"main\" {\n\n  # Referencing the base_cidr_block variable allows the network address\n\n  # to be changed without modifying the configuration.\n\n  cidr_block = var.base_cidr_block\n\n}\n\n\n\nresource \"aws_subnet\" \"az\" {\n\n  # Create one subnet for each given availability zone.\n\n  count = length(var.availability_zones)\n\n\n\n  # For each subnet, use one of the specified availability zones.\n\n  availability_zone = var.availability_zones[count.index]\n\n\n\n  # By referencing the aws_vpc.main object, Terraform knows that the subnet\n\n  # must be created only after the VPC is created.\n\n  vpc_id = aws_vpc.main.id\n\n\n\n  # Built-in functions and operators can be used for simple transformations of\n\n  # values, such as computing a subnet address. Here we create a /20 prefix for\n\n  # each subnet, using consecutive addresses for each availability zone,\n\n  # such as 10.1.16.0/20 .\n\n  cidr_block = cidrsubnet(aws_vpc.main.cidr_block, 4, count.index+1)\n\n}\n\nCopy\n\nOn this page:\n\nTerraform Language Documentation\nAbout the Terraform Language\nTheme\nDark\nLight\nSystem\nCertifications\nSystem Status\nCookie Manager\nTerms of Use\nSecurity\nPrivacy\nTrademark Policy\nTrade Controls\nGive Feedback\n(opens in new tab)\nWe use cookies & other similar technology to collect data to improve your experience on our site, as described in our Privacy Policy and Cookie Policy.\nManage PreferencesACCEPT"
  },
  {
    "title": "Overview - Configuration Language | Terraform | HashiCorp Developer",
    "url": "https://developer.hashicorp.com/terraform/language/v1.5.x",
    "html": "Skip to main content\nNews\nHashiCorp to join IBM to accelerate multi-cloud automation.\nRead the blog →\nDismiss alert\nTerraform\nInstall\nTutorials\nDocumentation\nRegistry\n(opens in new tab)\nTry Cloud\n(opens in new tab)\nSearch\nCommand or control key\nK key\nTerraform Home\nConfiguration Language\nConfiguration Language\nTerraform Language\nOverview\nFiles and Directories\nSyntax\nResources\nData Sources\nProviders\nVariables and Outputs\nModules\nChecks\nImport\nExpressions\nFunctions\nTerraform Settings\nState\nUpgrading to Terraform v1.5\nv1.x Compatibility Promises\nTerraform Internals\nResources\nTutorial Library\nCertifications\nCommunity Forum\n(opens in new tab)\nSupport\n(opens in new tab)\nGitHub\n(opens in new tab)\nTerraform Registry\n(opens in new tab)\n\nYou are viewing documentation for version v1.5.x. View latest version.\n\nDeveloper\nTerraform\nConfiguration Language v1.5.x\nv1.5.x\nTerraform Language Documentation\n\nThis is the documentation for Terraform's configuration language. It is relevant to users of Terraform CLI, Terraform Cloud, and Terraform Enterprise. Terraform's language is its primary user interface. Configuration files you write in Terraform language tell Terraform what plugins to install, what infrastructure to create, and what data to fetch. Terraform language also lets you define dependencies between resources and create multiple similar resources from a single configuration block.\n\nHands-on: Try the Write Terraform Configuration tutorials.\n\nAbout the Terraform Language\n\nThe main purpose of the Terraform language is declaring resources, which represent infrastructure objects. All other language features exist only to make the definition of resources more flexible and convenient.\n\nA Terraform configuration is a complete document in the Terraform language that tells Terraform how to manage a given collection of infrastructure. A configuration can consist of multiple files and directories.\n\nThe syntax of the Terraform language consists of only a few basic elements:\n\nresource \"aws_vpc\" \"main\" {\n\n  cidr_block = var.base_cidr_block\n\n}\n\n\n\n<BLOCK TYPE> \"<BLOCK LABEL>\" \"<BLOCK LABEL>\" {\n\n  # Block body\n\n  <IDENTIFIER> = <EXPRESSION> # Argument\n\n}\n\nCopy\nBlocks are containers for other content and usually represent the configuration of some kind of object, like a resource. Blocks have a block type, can have zero or more labels, and have a body that contains any number of arguments and nested blocks. Most of Terraform's features are controlled by top-level blocks in a configuration file.\nArguments assign a value to a name. They appear within blocks.\nExpressions represent a value, either literally or by referencing and combining other values. They appear as values for arguments, or within other expressions.\n\nThe Terraform language is declarative, describing an intended goal rather than the steps to reach that goal. The ordering of blocks and the files they are organized into are generally not significant; Terraform only considers implicit and explicit relationships between resources when determining an order of operations.\n\nExample\n\nThe following example describes a simple network topology for Amazon Web Services, just to give a sense of the overall structure and syntax of the Terraform language. Similar configurations can be created for other virtual network services, using resource types defined by other providers, and a practical network configuration will often contain additional elements not shown here.\n\nterraform {\n\n  required_providers {\n\n    aws = {\n\n      source  = \"hashicorp/aws\"\n\n      version = \"~> 1.0.4\"\n\n    }\n\n  }\n\n}\n\n\n\nvariable \"aws_region\" {}\n\n\n\nvariable \"base_cidr_block\" {\n\n  description = \"A /16 CIDR range definition, such as 10.1.0.0/16, that the VPC will use\"\n\n  default = \"10.1.0.0/16\"\n\n}\n\n\n\nvariable \"availability_zones\" {\n\n  description = \"A list of availability zones in which to create subnets\"\n\n  type = list(string)\n\n}\n\n\n\nprovider \"aws\" {\n\n  region = var.aws_region\n\n}\n\n\n\nresource \"aws_vpc\" \"main\" {\n\n  # Referencing the base_cidr_block variable allows the network address\n\n  # to be changed without modifying the configuration.\n\n  cidr_block = var.base_cidr_block\n\n}\n\n\n\nresource \"aws_subnet\" \"az\" {\n\n  # Create one subnet for each given availability zone.\n\n  count = length(var.availability_zones)\n\n\n\n  # For each subnet, use one of the specified availability zones.\n\n  availability_zone = var.availability_zones[count.index]\n\n\n\n  # By referencing the aws_vpc.main object, Terraform knows that the subnet\n\n  # must be created only after the VPC is created.\n\n  vpc_id = aws_vpc.main.id\n\n\n\n  # Built-in functions and operators can be used for simple transformations of\n\n  # values, such as computing a subnet address. Here we create a /20 prefix for\n\n  # each subnet, using consecutive addresses for each availability zone,\n\n  # such as 10.1.16.0/20 .\n\n  cidr_block = cidrsubnet(aws_vpc.main.cidr_block, 4, count.index+1)\n\n}\n\nCopy\n\nOn this page:\n\nTerraform Language Documentation\nAbout the Terraform Language\nTheme\nDark\nLight\nSystem\nCertifications\nSystem Status\nCookie Manager\nTerms of Use\nSecurity\nPrivacy\nTrademark Policy\nTrade Controls\nGive Feedback\n(opens in new tab)\nWe use cookies & other similar technology to collect data to improve your experience on our site, as described in our Privacy Policy and Cookie Policy.\nManage PreferencesACCEPT"
  },
  {
    "title": "Overview - Configuration Language | Terraform | HashiCorp Developer",
    "url": "https://developer.hashicorp.com/terraform/language/v1.7.x",
    "html": "Skip to main content\nNews\nHashiCorp to join IBM to accelerate multi-cloud automation.\nRead the blog →\nDismiss alert\nTerraform\nInstall\nTutorials\nDocumentation\nRegistry\n(opens in new tab)\nTry Cloud\n(opens in new tab)\nSearch\nCommand or control key\nK key\nTerraform Home\nConfiguration Language\nConfiguration Language\nTerraform Language\nOverview\nStyle Guide\nFiles and Directories\nSyntax\nResources\nData Sources\nProviders\nVariables and Outputs\nModules\nMoved block\nChecks\nImport\nExpressions\nFunctions\nTerraform Settings\nState\nTests\nUpgrading to Terraform v1.7\nv1.x Compatibility Promises\nTerraform Internals\nResources\nTutorial Library\nCertifications\nCommunity Forum\n(opens in new tab)\nSupport\n(opens in new tab)\nGitHub\n(opens in new tab)\nTerraform Registry\n(opens in new tab)\n\nYou are viewing documentation for version v1.7.x. View latest version.\n\nDeveloper\nTerraform\nConfiguration Language v1.7.x\nv1.7.x\nTerraform Language Documentation\n\nThis is the documentation for Terraform's configuration language. It is relevant to users of Terraform CLI, Terraform Cloud, and Terraform Enterprise. Terraform's language is its primary user interface. Configuration files you write in Terraform language tell Terraform what plugins to install, what infrastructure to create, and what data to fetch. Terraform language also lets you define dependencies between resources and create multiple similar resources from a single configuration block.\n\nHands-on: Try the Write Terraform Configuration tutorials.\n\nAbout the Terraform Language\n\nThe main purpose of the Terraform language is declaring resources, which represent infrastructure objects. All other language features exist only to make the definition of resources more flexible and convenient.\n\nA Terraform configuration is a complete document in the Terraform language that tells Terraform how to manage a given collection of infrastructure. A configuration can consist of multiple files and directories.\n\nThe syntax of the Terraform language consists of only a few basic elements:\n\nresource \"aws_vpc\" \"main\" {\n\n  cidr_block = var.base_cidr_block\n\n}\n\n\n\n<BLOCK TYPE> \"<BLOCK LABEL>\" \"<BLOCK LABEL>\" {\n\n  # Block body\n\n  <IDENTIFIER> = <EXPRESSION> # Argument\n\n}\n\nCopy\nBlocks are containers for other content and usually represent the configuration of some kind of object, like a resource. Blocks have a block type, can have zero or more labels, and have a body that contains any number of arguments and nested blocks. Most of Terraform's features are controlled by top-level blocks in a configuration file.\nArguments assign a value to a name. They appear within blocks.\nExpressions represent a value, either literally or by referencing and combining other values. They appear as values for arguments, or within other expressions.\n\nThe Terraform language is declarative, describing an intended goal rather than the steps to reach that goal. The ordering of blocks and the files they are organized into are generally not significant; Terraform only considers implicit and explicit relationships between resources when determining an order of operations.\n\nExample\n\nThe following example describes a simple network topology for Amazon Web Services, just to give a sense of the overall structure and syntax of the Terraform language. Similar configurations can be created for other virtual network services, using resource types defined by other providers, and a practical network configuration will often contain additional elements not shown here.\n\nterraform {\n\n  required_providers {\n\n    aws = {\n\n      source  = \"hashicorp/aws\"\n\n      version = \"~> 1.0.4\"\n\n    }\n\n  }\n\n}\n\n\n\nvariable \"aws_region\" {}\n\n\n\nvariable \"base_cidr_block\" {\n\n  description = \"A /16 CIDR range definition, such as 10.1.0.0/16, that the VPC will use\"\n\n  default = \"10.1.0.0/16\"\n\n}\n\n\n\nvariable \"availability_zones\" {\n\n  description = \"A list of availability zones in which to create subnets\"\n\n  type = list(string)\n\n}\n\n\n\nprovider \"aws\" {\n\n  region = var.aws_region\n\n}\n\n\n\nresource \"aws_vpc\" \"main\" {\n\n  # Referencing the base_cidr_block variable allows the network address\n\n  # to be changed without modifying the configuration.\n\n  cidr_block = var.base_cidr_block\n\n}\n\n\n\nresource \"aws_subnet\" \"az\" {\n\n  # Create one subnet for each given availability zone.\n\n  count = length(var.availability_zones)\n\n\n\n  # For each subnet, use one of the specified availability zones.\n\n  availability_zone = var.availability_zones[count.index]\n\n\n\n  # By referencing the aws_vpc.main object, Terraform knows that the subnet\n\n  # must be created only after the VPC is created.\n\n  vpc_id = aws_vpc.main.id\n\n\n\n  # Built-in functions and operators can be used for simple transformations of\n\n  # values, such as computing a subnet address. Here we create a /20 prefix for\n\n  # each subnet, using consecutive addresses for each availability zone,\n\n  # such as 10.1.16.0/20 .\n\n  cidr_block = cidrsubnet(aws_vpc.main.cidr_block, 4, count.index+1)\n\n}\n\nCopy\n\nOn this page:\n\nTerraform Language Documentation\nAbout the Terraform Language\nTheme\nDark\nLight\nSystem\nCertifications\nSystem Status\nCookie Manager\nTerms of Use\nSecurity\nPrivacy\nTrademark Policy\nTrade Controls\nGive Feedback\n(opens in new tab)\nWe use cookies & other similar technology to collect data to improve your experience on our site, as described in our Privacy Policy and Cookie Policy.\nManage PreferencesACCEPT"
  },
  {
    "title": "Overview - Configuration Language | Terraform | HashiCorp Developer",
    "url": "https://developer.hashicorp.com/terraform/language/v1.6.x",
    "html": "Skip to main content\nNews\nHashiCorp to join IBM to accelerate multi-cloud automation.\nRead the blog →\nDismiss alert\nTerraform\nInstall\nTutorials\nDocumentation\nRegistry\n(opens in new tab)\nTry Cloud\n(opens in new tab)\nSearch\nCommand or control key\nK key\nTerraform Home\nConfiguration Language\nConfiguration Language\nTerraform Language\nOverview\nStyle Guide\nFiles and Directories\nSyntax\nResources\nData Sources\nProviders\nVariables and Outputs\nModules\nMoved block\nChecks\nImport\nExpressions\nFunctions\nTerraform Settings\nState\nTests\nUpgrading to Terraform v1.6\nv1.x Compatibility Promises\nTerraform Internals\nResources\nTutorial Library\nCertifications\nCommunity Forum\n(opens in new tab)\nSupport\n(opens in new tab)\nGitHub\n(opens in new tab)\nTerraform Registry\n(opens in new tab)\n\nYou are viewing documentation for version v1.6.x. View latest version.\n\nDeveloper\nTerraform\nConfiguration Language v1.6.x\nv1.6.x\nTerraform Language Documentation\n\nThis is the documentation for Terraform's configuration language. It is relevant to users of Terraform CLI, Terraform Cloud, and Terraform Enterprise. Terraform's language is its primary user interface. Configuration files you write in Terraform language tell Terraform what plugins to install, what infrastructure to create, and what data to fetch. Terraform language also lets you define dependencies between resources and create multiple similar resources from a single configuration block.\n\nHands-on: Try the Write Terraform Configuration tutorials.\n\nAbout the Terraform Language\n\nThe main purpose of the Terraform language is declaring resources, which represent infrastructure objects. All other language features exist only to make the definition of resources more flexible and convenient.\n\nA Terraform configuration is a complete document in the Terraform language that tells Terraform how to manage a given collection of infrastructure. A configuration can consist of multiple files and directories.\n\nThe syntax of the Terraform language consists of only a few basic elements:\n\nresource \"aws_vpc\" \"main\" {\n\n  cidr_block = var.base_cidr_block\n\n}\n\n\n\n<BLOCK TYPE> \"<BLOCK LABEL>\" \"<BLOCK LABEL>\" {\n\n  # Block body\n\n  <IDENTIFIER> = <EXPRESSION> # Argument\n\n}\n\nCopy\nBlocks are containers for other content and usually represent the configuration of some kind of object, like a resource. Blocks have a block type, can have zero or more labels, and have a body that contains any number of arguments and nested blocks. Most of Terraform's features are controlled by top-level blocks in a configuration file.\nArguments assign a value to a name. They appear within blocks.\nExpressions represent a value, either literally or by referencing and combining other values. They appear as values for arguments, or within other expressions.\n\nThe Terraform language is declarative, describing an intended goal rather than the steps to reach that goal. The ordering of blocks and the files they are organized into are generally not significant; Terraform only considers implicit and explicit relationships between resources when determining an order of operations.\n\nExample\n\nThe following example describes a simple network topology for Amazon Web Services, just to give a sense of the overall structure and syntax of the Terraform language. Similar configurations can be created for other virtual network services, using resource types defined by other providers, and a practical network configuration will often contain additional elements not shown here.\n\nterraform {\n\n  required_providers {\n\n    aws = {\n\n      source  = \"hashicorp/aws\"\n\n      version = \"~> 1.0.4\"\n\n    }\n\n  }\n\n}\n\n\n\nvariable \"aws_region\" {}\n\n\n\nvariable \"base_cidr_block\" {\n\n  description = \"A /16 CIDR range definition, such as 10.1.0.0/16, that the VPC will use\"\n\n  default = \"10.1.0.0/16\"\n\n}\n\n\n\nvariable \"availability_zones\" {\n\n  description = \"A list of availability zones in which to create subnets\"\n\n  type = list(string)\n\n}\n\n\n\nprovider \"aws\" {\n\n  region = var.aws_region\n\n}\n\n\n\nresource \"aws_vpc\" \"main\" {\n\n  # Referencing the base_cidr_block variable allows the network address\n\n  # to be changed without modifying the configuration.\n\n  cidr_block = var.base_cidr_block\n\n}\n\n\n\nresource \"aws_subnet\" \"az\" {\n\n  # Create one subnet for each given availability zone.\n\n  count = length(var.availability_zones)\n\n\n\n  # For each subnet, use one of the specified availability zones.\n\n  availability_zone = var.availability_zones[count.index]\n\n\n\n  # By referencing the aws_vpc.main object, Terraform knows that the subnet\n\n  # must be created only after the VPC is created.\n\n  vpc_id = aws_vpc.main.id\n\n\n\n  # Built-in functions and operators can be used for simple transformations of\n\n  # values, such as computing a subnet address. Here we create a /20 prefix for\n\n  # each subnet, using consecutive addresses for each availability zone,\n\n  # such as 10.1.16.0/20 .\n\n  cidr_block = cidrsubnet(aws_vpc.main.cidr_block, 4, count.index+1)\n\n}\n\nCopy\n\nOn this page:\n\nTerraform Language Documentation\nAbout the Terraform Language\nTheme\nDark\nLight\nSystem\nCertifications\nSystem Status\nCookie Manager\nTerms of Use\nSecurity\nPrivacy\nTrademark Policy\nTrade Controls\nGive Feedback\n(opens in new tab)\nWe use cookies & other similar technology to collect data to improve your experience on our site, as described in our Privacy Policy and Cookie Policy.\nManage PreferencesACCEPT"
  },
  {
    "title": "Terraform v1.x Compatibility Promises | Terraform | HashiCorp Developer",
    "url": "https://developer.hashicorp.com/terraform/language/v1-compatibility-promises",
    "html": "Skip to main content\nNews\nHashiCorp to join IBM to accelerate multi-cloud automation.\nRead the blog →\nDismiss alert\nTerraform\nInstall\nTutorials\nDocumentation\nRegistry\n(opens in new tab)\nTry Cloud\n(opens in new tab)\nSearch\nCommand or control key\nK key\nTerraform Home\nConfiguration Language\nConfiguration Language\nStyle Guide\nFiles and Directories\nSyntax\nResources\nData Sources\nProviders\nVariables and Outputs\nModules\nMoved block\nChecks\nImport\nExpressions\nFunctions\nTerraform Settings\nState\nTests\nUpgrading to Terraform v1.7\nv1.x Compatibility Promises\nTerraform Internals\nResources\nTutorial Library\nCertifications\nCommunity Forum\n(opens in new tab)\nSupport\n(opens in new tab)\nGitHub\n(opens in new tab)\nTerraform Registry\n(opens in new tab)\nDeveloper\nTerraform\nConfiguration Language\nv1.x Compatibility Promises\nv1.8.x (latest)\nTerraform v1.x Compatibility Promises\n\nThe release of Terraform v1.0 represents an important milestone in the development of the Terraform language and workflow. Terraform v1.0 is a stable platform for describing and managing infrastructure.\n\nIn this release we're defining a number of Terraform behaviors that we intend to remain compatible with throughout the 1.x releases:\n\nA large subset of Terraform language features.\nA more conservative subset of the Terraform CLI workflow commands.\nThe wire protocol for communication between Terraform Core and Terraform providers.\nThe wire protocols for installation of Terraform providers and external Terraform modules.\n\nOur intention is that Terraform modules written for Terraform v1.0 will continue to plan and apply successfully, without required changes, throughout the v1.x releases.\n\nWe also intend that automation built around the workflow subset described in this document will work without changes in all future v1.x releases.\n\nFinally, we intend that providers built against the currently-documented provider wire protocol will be compatible with all future Terraform v1.x releases targeting the same operating system and architecture, without the need for source code modification or binary recompilation.\n\nIn short, we aim to make upgrades between v1.x releases straightforward, requiring no changes to your configuration, no extra commands to run upgrade steps, and no changes to any automation you've set up around Terraform.\n\nThe Terraform v1.x series will be actively maintained for at least 18 months after v1.0.\n\nThe following sections include some specific guidance on what we will promise throughout the v1.x series, for those who would like full details. At a higher level though, we don't intend to make any changes that would cause existing modules or automation to require changes when upgrading to a new v1.x release. We will generally treat compatibility problems in new Terraform CLI releases as bugs to be fixed unless there was a very significant justification for the change, such as in addressing a critical security problem or matching with a breaking change to a remote dependency that isn't directly under our control.\n\nThe Terraform Language\n\nThe main Terraform Language includes the language syntax, the top-level structures such as resource, module, and provider blocks, the \"meta-arguments\" in those blocks, and the documented semantics and behaviors for the operators and built-in functions available for use in expressions.\n\nThere is not a single formal specification for the Terraform language, but the Configuration section of the documentation on the Terraform website serves as a description of the language features and their intended behaviors.\n\nThe following top-level blocks and their defined \"meta-arguments\" (that is, arguments defined by Terraform Core rather than by external plugins such as providers) will retain their current functionality:\n\nresource and data blocks to declare resources, including their nested block types lifecycle, connection, and provisioner, and their meta-argument provider.\nmodule blocks to call other modules, and its meta-argument providers.\nThe count, for_each, and depends_on meta-arguments in resource, data, and module blocks.\nprovider blocks to configure providers, and the alias meta-argument.\nvariable, output, and locals blocks for declaring the various kinds of named values in a module.\nterraform blocks, including the nested required_version and required_providers arguments, and nested backend blocks for backend configuration.\n\nWe also intend to keep compatibility with all expression operators and built-in functions, with the exception of references to terraform.workspace, whose behavior may change as part of future changes to the workspace model.\n\nWe intend to retain broad compatibility with Terraform language features, with a few specific caveats:\n\nWe consider a configuration to be valid if Terraform can create and apply a plan for it without reporting any errors.\n\nA configuration that currently produces errors might generate different errors or exhibit other non-error behaviors in a future version of Terraform. A configuration that generates errors during the apply phase might generate similar errors at an earlier phase in future, because we generally consider it better to detect errors in as early a phase as possible.\n\nGenerally-speaking, the compatibility promises described in this document apply only to valid configurations. Handling of invalid configurations is always subject to change in future Terraform releases.\n\nIf the actual behavior of a feature differs from what we explicitly documented as the feature's behavior, we will usually treat that as a bug and change the feature to match the documentation, although we will avoid making such changes if they seem likely to cause broad compatibility problems. We cannot promise to always remain \"bug-compatible\" with previous releases, but we will consider such fixes carefully to minimize their impact.\n\nAny experimental features may change or may be removed entirely from future releases. Terraform always produces a warning when an experimental language feature is active, to make you aware of that risk. We don't recommend using experimental features in production modules.\n\nWe will introduce new language features, and if you start using them then your configuration won't work with prior Terraform versions that didn't support those features yet.\n\nTerraform Providers are separate plugins which can change independently of Terraform Core and are therefore not subject to these compatibility promises. If you upgrade any of the providers you are using then you might need to change provider or resource configurations related to those providers.\n\nA small number of features remain deprecated with explicit warnings in Terraform v1.0. Those deprecation cycles will end in a future v1.x release, at which point we will remove the corresponding features.\n\nWorkflow\n\nThere is a set of often used Terraform workflows, which we are calling protected workflows. We will not remove these commands, subcommands, and flags or make backward-incompatible changes to protected workflow functionality. If we accidentally change these, we will consider backwards-incompatible changes to core workflows as bugs to be fixed. For a list of the command and option combinations that are part of protected workflows, see Protected Workflow Commands. There is another set of commands that we are explicitly not making compatibility promises about, because we expect their functionality to change in v1.x releases: see Commands That Might Change.\n\nThe supported ways for external software to interact with Terraform are via the JSON output modes offered by some commands and via exit status codes. We may extend certain JSON formats with new object properties but we will not remove or make breaking changes to the definitions of existing properties.\n\nNatural language command output or log output is not a stable interface and may change in any new version. If you write software that parses this output then it may need to be updated when you upgrade Terraform. If you need access to data that is not currently available via one of the machine-readable JSON interfaces, we suggest opening a feature request to discuss your use-case.\n\nUpgrading and Downgrading\n\nThroughout the v1.x series of releases, we intend that you should be able to switch to a newer Terraform version and use it just as before, without any special upgrade steps.\n\nYou should be able to upgrade from any v1.x release to any later v1.x release. You might also be able to downgrade to an earlier v1.x release, but that isn't guaranteed: later releases may introduce new features that earlier versions cannot understand, including new storage formats for Terraform state snapshots.\n\nIf you make use of features introduced in a later v1.x release, your configuration won't be compatible with releases that predate that feature. For example, if a language feature is added in v1.3 and you start using it, your Terraform configuration will no longer be compatible with Terraform v1.2.\n\nProviders\n\nTerraform providers are separate plugins which communicate with Terraform using a documented protocol. Therefore these compatibility promises can only cover the \"client\" side of this protocol as implemented by Terraform Core; the behaviors of individual providers, including which resource types they support and which arguments they expect, are decided by the provider development teams and can change independently of Terraform Core releases.\n\nIf you upgrade to a new version of a provider then you might need to change the parts of your configuration which are interpreted by that provider, even if you are still using a Terraform v1.x release.\n\nProvider Installation Methods\n\nTerraform normally installs providers from a provider registry implementing the Provider Registry Protocol, version 1. All Terraform v1.x releases will remain compatible with that protocol, and so correctly-implemented provider registries will stay compatible.\n\nTerraform also supports installation of providers from local filesystem directories (filesystem mirrors) and from network mirrors (implementing the Provider Mirror Protocol. All Terraform v1.x releases will remain compatible with those installation methods, including the Implied Local Mirror Directories.\n\nSpecific provider registries or network mirrors are run independently from Terraform itself and so their own behaviors are not subject to these compatibility promises.\n\nProvider Protocol Versions\n\nThe current major version of the provider plugin protocol as of Terraform v1.0 is version 5, which is defined by a combination of a Protocol Buffers schema describing the physical wire formats and by additional prose documentation describing the expected provider behaviors.\n\nWe will support protocol version 5 throughout the Terraform v1.x releases. If we make new minor revisions to protocol version 5 in later releases then we will design them such that existing plugins will continue to work, as long as they correctly implemented the protocol.\n\nWe may introduce new major versions of the protocol during the v1.x series. If so, we will continue to support protocol version 5 alongside those new versions. Individual provider teams might decide to remove support for protocol version 5 in later releases, in which case those new provider releases will not be compatible with all of the Terraform v1.x releases.\n\nExternal Modules\n\nTerraform modules are reusable infrastructure components written in the Terraform language. Some modules are \"external\" in the sense that Terraform automatically installs them from a location other than the current configuration directory, in which case their contents could change independently of changes to your local modules, of the providers you use, and of Terraform itself.\n\nModule Installation Methods\n\nTerraform supports installing child modules from a number of different module source types. We will continue to support all of the existing source types throughout the v1.x releases.\n\nOne of the supported source types is a module registry implementing the Module Registry Protocol version 1. All Terraform v1.x releases will remain compatible with correct implementations of that protocol.\n\nSome module source types work directly with services or protocols defined and run by third parties. Although we will not remove Terraform's own client-side support for those, we cannot guarantee that their owners will keep those services running or that they will remain compatible with Terraform's client implementations.\n\nExternal Module Compatibility\n\nIf your configuration depends on external modules, newer versions of those modules may include breaking changes. External modules are not part of Terraform and are therefore not subject to these compatibility promises.\n\nProvisioners\n\nWe will maintain compatibility for the file, local-exec, and remote-exec provisioner types through all v1.x releases.\n\nSome additional vendor-specific provisioners were available in earlier Terraform versions but were deprecated in Terraform v0.13 and removed in Terraform v0.15.\n\nTerraform supports loading additional provisioners as plugins from certain local filesystem directories. We'll continue to support that throughout the Terraform v1.x releases, but since such plugins are separate from Terraform Core itself their own behaviors cannot be subject to these compatibility promises. However, we will continue to support the plugin wire protocol as defined in Terraform v1.0 throughout the v1.x releases, and so correctly-implemented provisioner plugins should remain compatible with future Terraform releases.\n\nState Storage Backends\n\nWhen you use remote state, Terraform interacts with remote services over the network in order to store and manage locks for Terraform state.\n\nFor historical reasons, all supported state storage backends are included as part of Terraform CLI but not all are supported directly by the Terraform Team. Only the following backends maintained by the Terraform team are subject to compatibility promises:\n\nlocal (the default, when you are not using remote state)\nhttp\n\nThe other state storage backends are maintained by external teams via contributions to the Terraform CLI codebase, and so their expected configuration arguments or behaviors might change even in v1.x releases, although we will aim to still ensure a good migration path in such cases, where possible.\n\nWe are considering allowing external state storage backend implementations via plugins, similar to provider plugins. If we introduce such a mechanism during the v1.x releases then you may need to make configuration changes in order to use those plugins, and state storage backends other than those listed above may be removed from later versions of Terraform CLI once equivalent plugins are available.\n\nThe remote Backend and HCP Terraform\n\nThe remote backend is maintained by the HCP Terraform team and so its behavior may change along with ongoing changes to HCP Terraform.\n\nThere will be a supported mechanism to use Terraform CLI with HCP Terraform throughout the v1.x releases, but the exact details may change. HCP Terraform evolves independently of Terraform CLI and is therefore not subject to these compatibility promises.\n\nCommunity-maintained State Storage Backends\n\nThe azurerm, consul, s3, and kubernetes backends are maintained by other teams at HashiCorp. Those teams intend to continue basic maintenence at the level of bug fixes through the v1.x releases, unless we implement a plugin protocol for backends at which point development of these backends is likely to continue in the external plugins only, which may require configuration changes to switch to the plugin equivalents.\n\nThe cos, oss, pg, gcs, and etcdv3 backends are maintained by outside contributors and are not subject to these compatibility promises.\n\nUnmaintained State Storage Backends\n\nThe artifactory, etcdv2, manta, and swift state storage backends do not currently have any maintainers and thus remain in Terraform CLI releases on a best-effort basis. They may be removed in later v1.x releases, and will not be updated in case of any breaking changes to the services they integrate with.\n\nSupported Platforms\n\nThroughout the v1.x series we will continue to produce official releases for the following platforms, and make changes as necessary to support new releases of these operating systems:\n\nmacOS on x64 CPUs (darwin_amd64)\nWindows on x64 CPUs (windows_amd64)\nLinux on x64, 32-bit ARMv6, and 64-bit ARMv8 (linux_amd64, linux_arm, and linux_arm64 respectively)\n\nOver time we may require newer versions of these operating systems. For example, subsequent Terraform releases in the v1.x series might end support for earlier versions of macOS or Windows, or earlier Linux kernel releases.\n\nWe have historically produced official releases for a number of other platforms as a convenience to users of those platforms, and we have no current plans to stop publishing them but we cannot promise ongoing releases or bug fixes for the other platforms throughout the v1.x series. We do not routinely test Terraform on any platforms other than those listed above.\n\nWe might add support for new platforms in later v1.x releases. If so, earlier Terraform releases prior to that support will not be available on those platforms.\n\nAll Terraform plugins, including provider plugins, are separate programs that have their own policies for which platforms they support. We cannot guarantee that all providers currently support or will continue to support the platforms listed above, even though Terraform CLI itself will support them.\n\nLater Revisions to These Promises\n\nWe may extend or refine these promises throughout the v1.x series in order to describe promises related to new features or to clarify existing promises if we find via feedback that our earlier statements had been unclear.\n\nPromises for new features will be additive in the sense that they will add further promises without retracting any existing ones. For promises that only apply to later v1.x releases we will mention the earliest version(s) those promises apply to.\n\nEven if we don't add an explicit statement to this document, we intend that any non-experimental features added in later v1.x releases will remain compatible at least through the remainder of the v1.x series, unless otherwise stated.\n\nAppendices\nProtected Workflow Commands\n\nThe following is the list of Terraform CLI subcommands and options that are subject to these compatibility promises. If you build automation around these commands then it should be compatible with all later v1.x releases.\n\nAs noted above, compatibility with external software is limited to explicitly-machine-readable output (-json and -raw modes) and exit codes. Any natural-language output from these commands might change in later releases.\n\ninit\n-backend=false\n-backend-config=FILE\n-backend-config=\"KEY=VALUE\"\n-force-copy\n-get=false\n-input=false\n-migrate-state\n-no-color\n-plugin-dir=DIR\n-reconfigure\n-upgrade\nvalidate\n-json\n-no-color\nplan\n-compact-warnings\n-destroy\n-detailed-exitcode\n-lock=false\n-lock-timeout=DURATION\n-input=false\n-json\n-no-color\n-out=FILE\n-parallelism=N\n-refresh=false\n-refresh-only\n-replace=ADDRESS\n-target=ADDRESS\n-var 'NAME=VALUE'\n-var-file=FILE\napply\n-auto-approve\n-compact-warnings\n-lock=false\n-lock-timeout=DURATION\n-input=false\n-json\n-no-color\n-parallelism=N\n-refresh=false\n-refresh-only\n-replace=ADDRESS\n-target=ADDRESS\n-var 'NAME=VALUE'\n-var-file=FILE\nshow\n-no-color\n-json\n(both with and without a plan file)\nproviders (with no subcommand)\nproviders lock\n-fs-mirror=PATH\n-net-mirror=URL\n-platform=OS_ARCH\nproviders mirror\n-platform=OS_ARCH\nproviders schema\n-json\nfmt\n-list=false\n-write=false\n-diff\n-recursive\n-check\nversion\n-json\noutput\n-no-color\n-json\n-raw\ntaint\n-allow-missing\n-lock=false\n-lock-timeout=DURATION\n-ignore-remote-version\nuntaint\n-allow-missing\n-lock=false\n-lock-timeout=DURATION\n-ignore-remote-version\nforce-unlock\n-force\nstate list\n-id=ID\nstate pull\nstate push\n-force\n-lock=false\n-lock-timeout=DURATION\nstate show\n-ignore-remote-version\nlogin\n\nFor commands or options not in the above list, we will still avoid breaking changes where possible, but can't promise full compatibility throughout the v1.x series. If you are building automation around Terraform, use only the commands above to avoid the need for changes when upgrading.\n\nPlease note that although Terraform's internal logs (via the TF_LOG environment variable) are available in a JSON format, the particular syntax or structure of those log lines is not a supported integration interface. The logs are available as JSON only to help with ad-hoc filtering and processing of logs by Terraform developers.\n\nCommands That Might Change\n\nAll of the following commands and their subcommands/options are not subject to compatibility promises, either because we have existing plans to improve them during the v1.x series or because we are aware of shortcomings in their design that might require breaking changes for ongoing maintenence.\n\nWhile you can freely use these commands when running Terraform interactively as long as they remain supported, we don't recommend using them as part of any automation unless you are willing to potentially update that automation when upgrading to a later v1.x release.\n\ndestroy (consider terraform apply -destroy instead)\nconsole\nget (consider terraform init instead)\ngraph\nimport\npush\nrefresh (consider terraform apply -refresh-only instead)\nstate mv\nstate replace-provider\nstate rm\nall subcommands of workspace (and its deprecated alias env)\n\nWhile we do intend to retain support for the main use-cases associated with these commands in future releases, we cannot promise to retain the exact command names or options used to meet those use-cases.\n\nHow We Will Keep These Promises\nAutomated Regression Testing\n\nThe Terraform codebase includes various unit and integration tests intended to help us to notice accidental behavior regressions before they ship in a stable version.\n\nHowever, Terraform is a relatively complex system with many different features that can interact in interesting ways. In the past we've seen reports of behavior differences that appeared only when combining two or more features in a way we hadn't previously anticipated or written automated tests for.\n\nIn each case we have both implemented a change to resolve the compatibility problem and added one or more integration tests representing the behavior of that combination of features. We intend to continue this approach, so we can improve Terraform's test coverage over time.\n\nPrerelease Versions\n\nWe intend that most accidental changes in behavior covered by these promises will be caught by existing tests. However, we also accept that our test suite can never have perfect coverage of all possible feature interactions or other edge cases, and so we aim for each significant change to be included in both alpha and beta releases before eventual inclusion in a final release.\n\nFor minor releases we will typically also issue at least one release candidate prior to the final release. A release candidate represents that planned development is concluded and that we've fixed any regressions reported based on the alpha and beta releases, and thus the final release that follows should typically match exactly or almost exactly its most recent release candidate.\n\nRegressions in Final Releases\n\nFor more obscure combinations of features it is possible that a regression could be undetected during prerelease the prerelease periods and thus included in a final release.\n\nIf someone finds and reports such a regression soon after its release then we will treat it as a bug and fix it to restore the previous behavior in future releases, unless there is a very significant justification such as a security advisory. In these cases, we'll typically recommend anyone affected by the regression remain on the previous version until the problem is fixed and then skip forward directly to the new release containing that fix.\n\nYou can minimize the risk of being affected by missed regressions in final releases by proactively testing modules against alpha, beta, and release candidate packages. We recommend doing so only in isolated development or staging environments rather than against your production infrastructure. If you find a change in behavior in a prerelease build that seems contrary to the promises in this document, please open an issue in Terraform's GitHub repository to discuss it.\n\nLate-reported Regressions\n\nIn the most extreme case, there may be a regression with a combination of features that is so rare that it remains undetected for a long time.\n\nAfter a change has been included in more releases it becomes increasingly likely that other users will have depended on the newer behavior and thus we will need to make a tradeoff to decide whether restoring the behavior would have a greater negative impact than retaining the new behavior. We will always make this decision with due consideration to the implications of each unique situation.\n\nYou can minimize the risk of your modules being affected by late-reported regressions by upgrading promptly to new minor and patch releases of Terraform and reporting any compatibility problems you encounter in Terraform's GitHub repository.\n\nPragmatic Exceptions\n\nWe are making the promises above in good faith, with the intent that your investment in writing Terraform modules or automation will not be invalidated by future changes to Terraform. However, broad promises like the above can't possibly cover all nuances of practical problems that might arise as we continue to develop Terraform.\n\nFor that reason, there are some situations where we may still need to make changes that may impact existing modules or automation:\n\nSecurity problems: We may become aware of a design problem that has an important security impact. Depending on our determination of the associated risk, we may choose to break compatibility to achieve a more secure system.\nExternal Dependencies: Terraform's behavior depends on interfaces provided by external codebases, including your chosen operating system and including some remote network services for situations such as module and provider installation. These external systems can change outside of our control, including potentially removing or changing features that Terraform's own features depend on. In that case, if there is no suitable replacement mechanism then we may need to change Terraform's design to work within the new constraints.\nOpt-in Compatibility Breaks: The design of a language new feature may require changing the behavior or configuration representation of an existing feature. If so, we will typically make the new feature opt-in only in order to avoid breaking existing modules, but if you change your module to opt in to the new feature then you may also then be required to change other parts of your configuration to work with the new language design.\nBugs in New Features: If we introduce a new feature to Terraform and the initial implementation has problems that cause it to not match the documented design intent at release, we may make a follow-up release that corrects the implementation to match the documented design, even if that represents a minor compatibility regression compared to the initial implementation. However, once a feature is well-established and in common use we will usually defer to the implemented behavior and instead change the documentation to reflect it.\nRegressions in Existing Features: If we learn that a new Terraform release includes a regression for an existing feature that wasn't detected during the development and prerelease periods, and that learning comes promptly after the new release, we will typically restore the previous behavior at the expense of technically therefore breaking compatibility with the behavior of the new release, under the assumption that more users will have systems affected by the regression than will have systems depending on the newly-introduced behavior.\nLate-reported regressions: As described in the previous section, if we learn that there was an unintentional regression of a rarely-used feature or combination of features in a much earlier release then restoring the previous behavior may appear as a regression to later adopters. If we believe that fixing the regression would affect more users than the regression itself affects then we may choose to accept the regression as the new promised behavior.\nSituations we cannot anticipate: Although we've made an effort to consider various specific exceptional situations here, Terraform and its development process are not isolated from broader context, and so we must consider that there may be situations that we cannot possibly anticipate that would affect the future of Terraform. In those situations, we will always do our best to find a course of action that will minimize as much as possible the impact to existing modules and automation.\n\nOur intent with these pragmatic exceptions is only to acknowledge that there will always be situations that general compatibility promises cannot address. We will use these exceptions only with due consideration and as a last resort.\n\nEdit this page on GitHub\n\nOn this page:\n\nTerraform v1.x Compatibility Promises\nThe Terraform Language\nWorkflow\nUpgrading and Downgrading\nProviders\nExternal Modules\nProvisioners\nState Storage Backends\nCommunity-maintained State Storage Backends\nSupported Platforms\nLater Revisions to These Promises\nAppendices\nHow We Will Keep These Promises\nTheme\nDark\nLight\nSystem\nCertifications\nSystem Status\nCookie Manager\nTerms of Use\nSecurity\nPrivacy\nTrademark Policy\nTrade Controls\nGive Feedback\n(opens in new tab)\nWe use cookies & other similar technology to collect data to improve your experience on our site, as described in our Privacy Policy and Cookie Policy.\nManage PreferencesACCEPT"
  },
  {
    "title": "Overview - Configuration Language | Terraform | HashiCorp Developer",
    "url": "https://developer.hashicorp.com/terraform/language/v1.9.x",
    "html": "Skip to main content\nNews\nHashiCorp to join IBM to accelerate multi-cloud automation.\nRead the blog →\nDismiss alert\nTerraform\nInstall\nTutorials\nDocumentation\nRegistry\n(opens in new tab)\nTry Cloud\n(opens in new tab)\nSearch\nCommand or control key\nK key\nTerraform Home\nConfiguration Language\nConfiguration Language\nTerraform Language\nOverview\nStyle Guide\nFiles and Directories\nSyntax\nResources\nData Sources\nProviders\nVariables and Outputs\nModules\nChecks\nImport\nExpressions\nFunctions\nTerraform Settings\nState\nTests\nUpgrading to Terraform v1.7\nv1.x Compatibility Promises\nTerraform Internals\nResources\nTutorial Library\nCertifications\nCommunity Forum\n(opens in new tab)\nSupport\n(opens in new tab)\nGitHub\n(opens in new tab)\nTerraform Registry\n(opens in new tab)\n\nYou are viewing documentation for pre-release version v1.9.0 (alpha). View latest version.\n\nDeveloper\nTerraform\nConfiguration Language v1.9.x\nv1.9.0 (alpha)\nTerraform Language Documentation\n\nThis is the documentation for Terraform's configuration language. It is relevant to users of Terraform CLI, HCP Terraform, and Terraform Enterprise. Terraform's language is its primary user interface. Configuration files you write in Terraform language tell Terraform what plugins to install, what infrastructure to create, and what data to fetch. Terraform language also lets you define dependencies between resources and create multiple similar resources from a single configuration block.\n\nHands-on: Try the Write Terraform Configuration tutorials.\n\nAbout the Terraform Language\n\nThe main purpose of the Terraform language is declaring resources, which represent infrastructure objects. All other language features exist only to make the definition of resources more flexible and convenient.\n\nA Terraform configuration is a complete document in the Terraform language that tells Terraform how to manage a given collection of infrastructure. A configuration can consist of multiple files and directories.\n\nThe syntax of the Terraform language consists of only a few basic elements:\n\nresource \"aws_vpc\" \"main\" {\n\n  cidr_block = var.base_cidr_block\n\n}\n\n\n\n<BLOCK TYPE> \"<BLOCK LABEL>\" \"<BLOCK LABEL>\" {\n\n  # Block body\n\n  <IDENTIFIER> = <EXPRESSION> # Argument\n\n}\n\nCopy\nBlocks are containers for other content and usually represent the configuration of some kind of object, like a resource. Blocks have a block type, can have zero or more labels, and have a body that contains any number of arguments and nested blocks. Most of Terraform's features are controlled by top-level blocks in a configuration file.\nArguments assign a value to a name. They appear within blocks.\nExpressions represent a value, either literally or by referencing and combining other values. They appear as values for arguments, or within other expressions.\n\nThe Terraform language is declarative, describing an intended goal rather than the steps to reach that goal. The ordering of blocks and the files they are organized into are generally not significant; Terraform only considers implicit and explicit relationships between resources when determining an order of operations.\n\nExample\n\nThe following example describes a simple network topology for Amazon Web Services, just to give a sense of the overall structure and syntax of the Terraform language. Similar configurations can be created for other virtual network services, using resource types defined by other providers, and a practical network configuration will often contain additional elements not shown here.\n\nterraform {\n\n  required_providers {\n\n    aws = {\n\n      source  = \"hashicorp/aws\"\n\n      version = \"~> 1.0.4\"\n\n    }\n\n  }\n\n}\n\n\n\nvariable \"aws_region\" {}\n\n\n\nvariable \"base_cidr_block\" {\n\n  description = \"A /16 CIDR range definition, such as 10.1.0.0/16, that the VPC will use\"\n\n  default = \"10.1.0.0/16\"\n\n}\n\n\n\nvariable \"availability_zones\" {\n\n  description = \"A list of availability zones in which to create subnets\"\n\n  type = list(string)\n\n}\n\n\n\nprovider \"aws\" {\n\n  region = var.aws_region\n\n}\n\n\n\nresource \"aws_vpc\" \"main\" {\n\n  # Referencing the base_cidr_block variable allows the network address\n\n  # to be changed without modifying the configuration.\n\n  cidr_block = var.base_cidr_block\n\n}\n\n\n\nresource \"aws_subnet\" \"az\" {\n\n  # Create one subnet for each given availability zone.\n\n  count = length(var.availability_zones)\n\n\n\n  # For each subnet, use one of the specified availability zones.\n\n  availability_zone = var.availability_zones[count.index]\n\n\n\n  # By referencing the aws_vpc.main object, Terraform knows that the subnet\n\n  # must be created only after the VPC is created.\n\n  vpc_id = aws_vpc.main.id\n\n\n\n  # Built-in functions and operators can be used for simple transformations of\n\n  # values, such as computing a subnet address. Here we create a /20 prefix for\n\n  # each subnet, using consecutive addresses for each availability zone,\n\n  # such as 10.1.16.0/20 .\n\n  cidr_block = cidrsubnet(aws_vpc.main.cidr_block, 4, count.index+1)\n\n}\n\nCopy\n\nOn this page:\n\nTerraform Language Documentation\nAbout the Terraform Language\nTheme\nDark\nLight\nSystem\nCertifications\nSystem Status\nCookie Manager\nTerms of Use\nSecurity\nPrivacy\nTrademark Policy\nTrade Controls\nGive Feedback\n(opens in new tab)\nWe use cookies & other similar technology to collect data to improve your experience on our site, as described in our Privacy Policy and Cookie Policy.\nManage PreferencesACCEPT"
  },
  {
    "title": "Upgrading to Terraform v1.8 | Terraform | HashiCorp Developer",
    "url": "https://developer.hashicorp.com/terraform/language/upgrade-guides",
    "html": "Skip to main content\nNews\nHashiCorp to join IBM to accelerate multi-cloud automation.\nRead the blog →\nDismiss alert\nTerraform\nInstall\nTutorials\nDocumentation\nRegistry\n(opens in new tab)\nTry Cloud\n(opens in new tab)\nSearch\nCommand or control key\nK key\nTerraform Home\nConfiguration Language\nConfiguration Language\nStyle Guide\nFiles and Directories\nSyntax\nResources\nData Sources\nProviders\nVariables and Outputs\nModules\nMoved block\nChecks\nImport\nExpressions\nFunctions\nTerraform Settings\nState\nTests\nUpgrading to Terraform v1.7\nv1.x Compatibility Promises\nTerraform Internals\nResources\nTutorial Library\nCertifications\nCommunity Forum\n(opens in new tab)\nSupport\n(opens in new tab)\nGitHub\n(opens in new tab)\nTerraform Registry\n(opens in new tab)\nDeveloper\nTerraform\nConfiguration Language\nUpgrading to Terraform v1.7\nv1.8.x (latest)\nUpgrading to Terraform v1.8\n\nTip: Use the version selector to view the upgrade guides for older Terraform versions.\n\nTerraform v1.8 is a minor release in the stable Terraform v1.0 series.\n\nTerraform v1.8 honors the Terraform v1.0 Compatibility Promises, but there are some behavior changes outside of those promises that may affect a small number of users. Specifically, the following updates may require additional upgrade steps:\n\nuse_legacy_workflow is no longer available for the S3 backend\nPossible spurious changes when refreshing\nEnding support for macOS 10.15 Catalina\nMinor change to jsonencode function results\n\nSee the full changelog for more details. If you encounter any problems during upgrading which are not covered this guide, please start a new topic in the Terraform community forum to discuss it.\n\nS3 Backend authentication changes\n\nTerraform v1.7 began the deprecation of a legacy approach to authentication, making the use_legacy_workflow argument default to false and thus making the old authentication workflow opt-in.\n\nTerraform v1.8 completes this deprecation process by removing the use_legacy_workflow argument. The old behavior is no longer available, and so you will need to adopt the new behavior when upgrading to Terraform v1.8.\n\nThe new implementation follows the authentication process implemented in the official AWS SDK for Go, which is therefore more consistent with other AWS tools such as the official AWS CLI.\n\nPossible spurious changes when refreshing\n\nIf you use the -refresh-only or -refresh=false planning options for your first plan after upgrading, Terraform might show resource instance diffs without any visible changes. This does not affect plans created with both of those options disabled.\n\nPrevious versions of Terraform used a mixture of both dynamic and static tracking of sensitive values in resource instance attributes. That meant that, for example, correctly honoring sensitive values when interpreting the terraform show -json output required considering both the dynamic sensitivity information directly in the output and static sensitivity information in the provider schema.\n\nTo simplify handling of sensitivity in these cases, Terraform now copies the schema-based sensitivity information into the state along with the dynamic information. Terraform must therefore perform a one-time backfill update of the state metadata for resource types which have sensitive attributes.\n\nWhen using the default planning options Terraform should handle this update quietly, as part of the refresh step performed during planning. However, if you use the -refresh-only or -refresh=false option then you will effectively disable one half of this process, causing the UI to report spurious changes that affect only the metadata in the state.\n\nThese no-change metadata updates should not cause any problems, and will be resolved once a plan has been applied using Terraform v1.8. If you are concerned about a particular plan then try removing the -refresh-only or -refresh=false option, which should then quiet the spurious change.\n\nEnding support for macOS 10.15 Catalina\n\nTerraform v1.8 is the last series that will support macOS 10.15 Catalina. The next minor release series will require macOS 11 Big Sur or later.\n\nMinor change to jsonencode function results\n\nIn previous versions of Terraform, the jsonencode function encoded the control characters U+0008 (backspace) and U+000C (form feed) in strings using the unicode escape syntax: \\u0008 and \\u000c respectively.\n\nTerraform now follows the JSON idiom more closely by using \\b for backspace and \\f for form feed. These shorter encodings are equivalent for a correct JSON parser, but are more readable for humans due to being mnemonics.\n\nThese two control characters are relatively rarely used in practical JSON and so we don't expect that this change will have significant impact. If you are using them then this may cause the following effects:\n\nIf you are using jsonencode to produce JSON-encoded data for consumption by a JSON parser that doesn't correctly support these short encoding forms then it may not be able to parse the new results. Terraform implements JSON encoding as defined in IETF RFC 7159, which requires that parsers support these shorter encodings.\n\nIf you are using jsonencode to populate a resource argument where the underlying provider does not perform JSON normalization, the provider might propose changing the affected object to use the new encoding form. As long as the remote system correctly implements JSON, this update should not change the meaning of the JSON document.\n\nThis change only affects strings that include these two specific control characters. If you do not use these control characters in the strings you pass to jsonencode then this change will have no effect for you.\n\nEdit this page on GitHub\n\nOn this page:\n\nUpgrading to Terraform v1.8\nS3 Backend authentication changes\nPossible spurious changes when refreshing\nEnding support for macOS 10.15 Catalina\nMinor change to jsonencode function results\nTheme\nDark\nLight\nSystem\nCertifications\nSystem Status\nCookie Manager\nTerms of Use\nSecurity\nPrivacy\nTrademark Policy\nTrade Controls\nGive Feedback\n(opens in new tab)\nWe use cookies & other similar technology to collect data to improve your experience on our site, as described in our Privacy Policy and Cookie Policy.\nManage PreferencesACCEPT"
  },
  {
    "title": "Checks - Configuration Language | Terraform | HashiCorp Developer",
    "url": "https://developer.hashicorp.com/terraform/language/checks",
    "html": "Skip to main content\nNews\nHashiCorp to join IBM to accelerate multi-cloud automation.\nRead the blog →\nDismiss alert\nTerraform\nInstall\nTutorials\nDocumentation\nRegistry\n(opens in new tab)\nTry Cloud\n(opens in new tab)\nSearch\nCommand or control key\nK key\nTerraform Home\nConfiguration Language\nConfiguration Language\nStyle Guide\nFiles and Directories\nSyntax\nResources\nData Sources\nProviders\nVariables and Outputs\nModules\nMoved block\nChecks\nImport\nExpressions\nFunctions\nTerraform Settings\nState\nTests\nUpgrading to Terraform v1.7\nv1.x Compatibility Promises\nTerraform Internals\nResources\nTutorial Library\nCertifications\nCommunity Forum\n(opens in new tab)\nSupport\n(opens in new tab)\nGitHub\n(opens in new tab)\nTerraform Registry\n(opens in new tab)\nDeveloper\nTerraform\nConfiguration Language\nChecks\nv1.8.x (latest)\nChecks\n\nNote: Check blocks are only available in Terraform v1.5.0 and later.\n\nThe check block can validate your infrastructure outside the usual resource lifecycle. Check blocks address a gap between post-apply and functional validation of infrastructure.\n\nHands-on: Try the Validate Infrastructure Using Checks tutorial.\n\nCheck blocks allow you to define custom conditions that execute on every Terraform plan or apply operation without affecting the overall status of an operation. Check blocks execute as the last step of a plan or apply after Terraform has planned or provisioned your infrastructure.\n\nSyntax\n\nYou can declare a check block with a local name, zero-to-one scoped data sources, and one-to-many assertions.\n\nThe following example loads the Terraform website and validates that it returns the expected status code 200.\n\ncheck \"health_check\" {\n\n  data \"http\" \"terraform_io\" {\n\n    url = \"https://www.terraform.io\"\n\n  }\n\n\n\n  assert {\n\n    condition = data.http.terraform_io.status_code == 200\n\n    error_message = \"${data.http.terraform_io.url} returned an unhealthy status code\"\n\n  }\n\n}\n\nCopy\nScoped data sources\n\nYou can use any data source from any provider as a scoped data source within a check block.\n\nA check block can optionally contain a nested (a.k.a. scoped) data source. This data block behaves like an external data source, except you can not reference it outside its enclosing check block. Additionally, if a scoped data source's provider raises any errors, they are masked as warnings and do not prevent Terraform from continuing operation execution.\n\nYou can use a scoped data source to validate the status of a piece of infrastructure outside of the usual Terraform resource lifecycle. In the above example, if the terraform_io data source fails to load, you receive a warning instead of a blocking error, which would occur if you declared this data source outside of a check block.\n\nMeta-Arguments\n\nScoped data sources support the depends_on and provider meta-arguments. Scoped data sources do not support the count orfor_each meta-arguments.\n\ndepends_on\n\nThe depends_on meta-argument can be particularly powerful when used within scoped data sources.\n\nThe first time Terraform creates the initial plan for our previous example, the plan fails because Terraform has not applied its configuration yet. Meaning this test fails because Terraform must still create the resources to make this website exist. Therefore, the first time Terraform runs this check, it always throws a potentially distracting error message.\n\nYou can fix this by adding depends_on to your scoped data source, ensuring it depends on an essential piece of your site's infrastructure, such as the load balancer. The check returns known after apply until that crucial piece of your website is ready. This strategy avoids producing unnecessary warnings during setup, and the check executes during subsequent plans and applies.\n\nOne problem with this strategy is that if the resource your scoped data source depends_on changes, the check block returns known after apply until Terraform has updated that resource. Depending on your use case, this behavior could be acceptable or problematic.\n\nWe recommend implementing the depends_on meta-argument if your scoped data source depends on the existence of another resource without referencing it directly.\n\nAssertions\n\nCheck blocks validate your custom assertions using assert blocks. Each check block must have at least one, but potentially many, assert blocks. Each assert block has a condition attribute and an error_message attribute.\n\nUnlike other custom conditions, assertions do not affect Terraform's execution of an operation. A failed assertion reports a warning without halting the ongoing operation. This contrasts with other custom conditions, such as a postcondition, where Terraform produces an error immediately, halting the operation and blocking the application or planning of future resources.\n\nCondition arguments within assert blocks can refer to scoped data sources within the enclosing check block and any variables, resources, data sources, or module outputs within the current module.\n\nLearn more about assertions.\n\nMeta-Arguments\n\nCheck blocks do not currently support meta-arguments. We are still collecting feedback on this feature, so if your use case would benefit from check blocks supporting meta-arguments, please let us know.\n\nContinuous validation in HCP Terraform\n\nHCP Terraform can automatically validate whether checks in a workspace’s configuration continue to pass after Terraform provisions new infrastructure. See Continuous Validation for details.\n\nChoosing Checks or other Custom Conditions\n\nCheck blocks offer the most flexible validation solution within Terraform. You can reference outputs, variables, resources, and data sources within check assertions. You can also use checks to model every alternate Custom Condition. However, that does not mean you should replace all your custom conditions with check blocks.\n\nThere are major behavioral differences between check block assertions and other custom conditions, the main one being that check blocks do not affect Terraform's execution of an operation. You can use this non-blocking behavior to decide the best type of validation for your use case.\n\nOutputs and variables\n\nOutput postconditions and variable validations both make assertions around inputs and outputs.\n\nThis is one of the cases where you might want Terraform to block further execution.\n\nFor example, it is not helpful for Terraform to warn that an input variable is invalid after it applies an entire configuration with that input variable. In this case, a check block would warn of the invalid input variable without interrupting the operation. A validation block for the same input variable would alert you of the invalid variable and halt the plan or apply operation.\n\nResource Preconditions and Postconditions\n\nThe difference between preconditions and postconditions and check blocks is more nuanced.\n\nPreconditions are unique amongst the custom conditions in that they execute before a resource change is applied or planned. Choosing Between Preconditions and Postconditions offers guidance on choosing between a precondition and a postcondition, and the same topics also apply to choosing between a precondition and a check block.\n\nYou can often use postconditions interchangeably with check blocks to validate resources and data sources.\n\nFor example, you can rewrite the above check block example to use a postcondition instead. The below code uses a postcondition block to validate that the Terraform website returns the expected status code of 200.\n\ndata \"http\" \"terraform_io\" {\n\n  url = \"https://www.terraform.io\"\n\n\n\n  lifecycle {\n\n    postcondition {\n\n        condition = self.status_code == 200\n\n        error_message = \"${self.url} returned an unhealthy status code\"\n\n    }\n\n  }\n\n}\n\nCopy\n\nBoth the check and postcondition block examples validate that the Terraform website returns a 200 status code during a plan or an apply operation. The difference between the two blocks is how each handles failure.\n\nIf a postcondition block fails, it blocks Terraform from executing the current operation. If a check block fails, it does not block Terraform from executing an operation.\n\nIf the above example's postcondition fails, it is impossible to recover from. Terraform blocks any future plan or apply operations if your postcondition is unsatisfied during the planning stage. This problem occurs because the postcondition does not directly depend on Terraform configuration, but instead on the complex interactions between multiple resources.\n\nWe recommend using check blocks to validate the status of infrastructure as a whole. We only recommend using postconditions when you want a guarantee on a single resource based on that resource's configuration.\n\nEdit this page on GitHub\n\nOn this page:\n\nChecks\nSyntax\nContinuous validation in HCP Terraform\nChoosing Checks or other Custom Conditions\nTheme\nDark\nLight\nSystem\nCertifications\nSystem Status\nCookie Manager\nTerms of Use\nSecurity\nPrivacy\nTrademark Policy\nTrade Controls\nGive Feedback\n(opens in new tab)\nWe use cookies & other similar technology to collect data to improve your experience on our site, as described in our Privacy Policy and Cookie Policy.\nManage PreferencesACCEPT"
  },
  {
    "title": "Data Sources - Configuration Language | Terraform | HashiCorp Developer",
    "url": "https://developer.hashicorp.com/terraform/language/data-sources",
    "html": "Skip to main content\nNews\nHashiCorp to join IBM to accelerate multi-cloud automation.\nRead the blog →\nDismiss alert\nTerraform\nInstall\nTutorials\nDocumentation\nRegistry\n(opens in new tab)\nTry Cloud\n(opens in new tab)\nSearch\nCommand or control key\nK key\nTerraform Home\nConfiguration Language\nConfiguration Language\nStyle Guide\nFiles and Directories\nSyntax\nResources\nData Sources\nProviders\nVariables and Outputs\nModules\nMoved block\nChecks\nImport\nExpressions\nFunctions\nTerraform Settings\nState\nTests\nUpgrading to Terraform v1.7\nv1.x Compatibility Promises\nTerraform Internals\nResources\nTutorial Library\nCertifications\nCommunity Forum\n(opens in new tab)\nSupport\n(opens in new tab)\nGitHub\n(opens in new tab)\nTerraform Registry\n(opens in new tab)\nDeveloper\nTerraform\nConfiguration Language\nData Sources\nv1.8.x (latest)\nData Sources\n\nData sources allow Terraform to use information defined outside of Terraform, defined by another separate Terraform configuration, or modified by functions.\n\nHands-on: Try the Query Data Sources tutorial.\n\nEach provider may offer data sources alongside its set of resource types.\n\nUsing Data Sources\n\nA data source is accessed via a special kind of resource known as a data resource, declared using a data block:\n\ndata \"aws_ami\" \"example\" {\n\n  most_recent = true\n\n\n\n  owners = [\"self\"]\n\n  tags = {\n\n    Name   = \"app-server\"\n\n    Tested = \"true\"\n\n  }\n\n}\n\nCopy\n\nA data block requests that Terraform read from a given data source (\"aws_ami\") and export the result under the given local name (\"example\"). The name is used to refer to this resource from elsewhere in the same Terraform module, but has no significance outside of the scope of a module.\n\nThe data source and name together serve as an identifier for a given resource and so must be unique within a module.\n\nWithin the block body (between { and }) are query constraints defined by the data source. Most arguments in this section depend on the data source, and indeed in this example most_recent, owners and tags are all arguments defined specifically for the aws_ami data source.\n\nWhen distinguishing from data resources, the primary kind of resource (as declared by a resource block) is known as a managed resource. Both kinds of resources take arguments and export attributes for use in configuration, but while managed resources cause Terraform to create, update, and delete infrastructure objects, data resources cause Terraform only to read objects. For brevity, managed resources are often referred to just as \"resources\" when the meaning is clear from context.\n\nData Source Arguments\n\nEach data resource is associated with a single data source, which determines the kind of object (or objects) it reads and what query constraint arguments are available.\n\nEach data source in turn belongs to a provider, which is a plugin for Terraform that offers a collection of resource types and data sources that most often belong to a single cloud or on-premises infrastructure platform.\n\nMost of the items within the body of a data block are defined by and specific to the selected data source, and these arguments can make full use of expressions and other dynamic Terraform language features.\n\nHowever, there are some \"meta-arguments\" that are defined by Terraform itself and apply across all data sources. These arguments often have additional restrictions on what language features can be used with them, and are described in more detail in the following sections.\n\nData Resource Behavior\n\nTerraform reads data resources during the planning phase when possible, but announces in the plan when it must defer reading resources until the apply phase to preserve the order of operations. Terraform defers reading data resources in the following situations:\n\nAt least one of the given arguments is a managed resource attribute or other value that Terraform cannot predict until the apply step.\nThe data resource depends directly on a managed resource that itself has planned changes in the current plan.\nThe data resource has custom conditions and it depends directly or indirectly on a managed resource that itself has planned changes in the current plan.\n\nRefer to Data Resource Dependencies for details on what it means for a data resource to depend on other objects. Any resulting attribute of such a data resource will be unknown during planning, so it cannot be used in situations where values must be fully known.\n\nLocal-only Data Sources\n\nWhile many data sources correspond to an infrastructure object type that is accessed via a remote network API, some specialized data sources operate only within Terraform itself, calculating some results and exposing them for use elsewhere.\n\nFor example, local-only data sources exist for rendering templates, reading local files, and rendering AWS IAM policies.\n\nThe behavior of local-only data sources is the same as all other data sources, but their result data exists only temporarily during a Terraform operation, and is re-calculated each time a new plan is created.\n\nData Resource Dependencies\n\nData resources have the same dependency resolution behavior as defined for managed resources. Setting the depends_on meta-argument within data blocks defers reading of the data source until after all changes to the dependencies have been applied.\n\nIn order to ensure that data sources are accessing the most up to date information possible in a wide variety of use cases, arguments directly referencing managed resources are treated the same as if the resource was listed in depends_on. This behavior can be avoided when desired by indirectly referencing the managed resource values through a local value, unless the data resource itself has custom conditions.\n\nNOTE: In Terraform 0.12 and earlier, due to the data resource behavior of deferring the read until the apply phase when depending on values that are not yet known, using depends_on with data resources will force the read to always be deferred to the apply phase, and therefore a configuration that uses depends_on with a data resource can never converge. Due to this behavior, we do not recommend using depends_on with data resources.\n\nCustom Condition Checks\n\nYou can use precondition and postcondition blocks to specify assumptions and guarantees about how the data source operates. The following examples creates a postcondition that checks whether the AMI has the correct tags.\n\ndata \"aws_ami\" \"example\" {\n\n  id = var.aws_ami_id\n\n\n\n  lifecycle {\n\n    # The AMI ID must refer to an existing AMI that has the tag \"nomad-server\".\n\n    postcondition {\n\n      condition     = self.tags[\"Component\"] == \"nomad-server\"\n\n      error_message = \"tags[\\\"Component\\\"] must be \\\"nomad-server\\\".\"\n\n    }\n\n  }\n\n}\n\nCopy\n\nCustom conditions can help capture assumptions, helping future maintainers understand the configuration design and intent. They also return useful information about errors earlier and in context, helping consumers more easily diagnose issues in their configurations.\n\nRefer to Custom Condition Checks for more details.\n\nMultiple Resource Instances\n\nData resources support count and for_each meta-arguments as defined for managed resources, with the same syntax and behavior.\n\nAs with managed resources, when count or for_each is present it is important to distinguish the resource itself from the multiple resource instances it creates. Each instance will separately read from its data source with its own variant of the constraint arguments, producing an indexed result.\n\nSelecting a Non-default Provider Configuration\n\nData resources support the provider meta-argument as defined for managed resources, with the same syntax and behavior.\n\nLifecycle Customizations\n\nData resources do not have any customization settings available for their lifecycle. However, the lifecycle block is reserved for future versions.\n\nExample\n\nA data source configuration looks like the following:\n\n# Find the latest available AMI that is tagged with Component = web\n\ndata \"aws_ami\" \"web\" {\n\n  filter {\n\n    name   = \"state\"\n\n    values = [\"available\"]\n\n  }\n\n\n\n  filter {\n\n    name   = \"tag:Component\"\n\n    values = [\"web\"]\n\n  }\n\n\n\n  most_recent = true\n\n}\n\nCopy\nDescription\n\nThe data block creates a data instance of the given type (first block label) and name (second block label). The combination of the type and name must be unique.\n\nWithin the block (the { }) is configuration for the data instance. The configuration is dependent on the type; as with resources, each provider on the Terraform Registry has its own documentation for configuring and using the data types it provides.\n\nEach data instance will export one or more attributes, which can be used in other resources as reference expressions of the form data.<TYPE>.<NAME>.<ATTRIBUTE>. For example:\n\nresource \"aws_instance\" \"web\" {\n\n  ami           = data.aws_ami.web.id\n\n  instance_type = \"t1.micro\"\n\n}\n\nCopy\nMeta-Arguments\n\nAs data sources are essentially a read only subset of resources, they also support the same meta-arguments of resources with the exception of the lifecycle configuration block.\n\nNon-Default Provider Configurations\n\nSimilarly to resources, when a module has multiple configurations for the same provider you can specify which configuration to use with the provider meta-argument:\n\ndata \"aws_ami\" \"web\" {\n\n  provider = aws.west\n\n\n\n  # ...\n\n}\n\nCopy\n\nSee The Resource provider Meta-Argument for more information.\n\nData Source Lifecycle\n\nIf the arguments of a data instance contain no references to computed values, such as attributes of resources that have not yet been created, then the data instance will be read and its state updated during Terraform's \"refresh\" phase, which by default runs prior to creating a plan. This ensures that the retrieved data is available for use during planning and the diff will show the real values obtained.\n\nData instance arguments may refer to computed values, in which case the attributes of the instance itself cannot be resolved until all of its arguments are defined. In this case, refreshing the data instance will be deferred until the \"apply\" phase, and all interpolations of the data instance attributes will show as \"computed\" in the plan since the values are not yet known.\n\nEdit this page on GitHub\n\nOn this page:\n\nData Sources\nUsing Data Sources\nData Source Arguments\nData Resource Behavior\nLocal-only Data Sources\nData Resource Dependencies\nCustom Condition Checks\nMultiple Resource Instances\nSelecting a Non-default Provider Configuration\nLifecycle Customizations\nExample\nDescription\nMeta-Arguments\nData Source Lifecycle\nTheme\nDark\nLight\nSystem\nCertifications\nSystem Status\nCookie Manager\nTerms of Use\nSecurity\nPrivacy\nTrademark Policy\nTrade Controls\nGive Feedback\n(opens in new tab)\nWe use cookies & other similar technology to collect data to improve your experience on our site, as described in our Privacy Policy and Cookie Policy.\nManage PreferencesACCEPT"
  },
  {
    "title": "moved block configuration reference | Terraform | HashiCorp Developer",
    "url": "https://developer.hashicorp.com/terraform/language/moved",
    "html": "Skip to main content\nNews\nHashiCorp to join IBM to accelerate multi-cloud automation.\nRead the blog →\nDismiss alert\nTerraform\nInstall\nTutorials\nDocumentation\nRegistry\n(opens in new tab)\nTry Cloud\n(opens in new tab)\nSearch\nCommand or control key\nK key\nTerraform Home\nConfiguration Language\nConfiguration Language\nStyle Guide\nFiles and Directories\nSyntax\nResources\nData Sources\nProviders\nVariables and Outputs\nModules\nMoved block\nChecks\nImport\nExpressions\nFunctions\nTerraform Settings\nState\nTests\nUpgrading to Terraform v1.7\nv1.x Compatibility Promises\nTerraform Internals\nResources\nTutorial Library\nCertifications\nCommunity Forum\n(opens in new tab)\nSupport\n(opens in new tab)\nGitHub\n(opens in new tab)\nTerraform Registry\n(opens in new tab)\nDeveloper\nTerraform\nConfiguration Language\nMoved block\nv1.8.x (latest)\nMoved block configuration reference\n\nThis topic provides reference information for the moved block.\n\nIntroduction\n\nThe moved block programmatically changes the address of a resource. Refer to Refactoring for details about how to use the moved block in your Terraform configurations.\n\nConfiguration model\n\nThe following list outlines field hierarchy, language-specific data types, and requirements in the moved block.\n\nmoved: map\nfrom: string\nto: string\nComplete configuration\n\nWhen every field is defined, a moved block has the following form:\n\nmoved = {\n\n    from = <old address for the resource>\n\n    to = <new address for the resource>\n\n}\n\nCopy\nSpecification\n\nThis section provides details about the fields you can configure in the moved block.\n\nmoved\n\nMap that specifies addresses for the resource. The following table describes the fields you can set in the moved block.\n\nField\tDescription\tType\tRequired\nfrom\tSpecifies a resource's previous address. The syntax allows Terraform to select modules, resources, and resources inside child modules.\tstring\trequired\nto\tSpecifies the new address to relocate the resource to. The syntax allows Terraform to select modules, resources, and resources inside child modules.\tstring\trequired\n\nBefore creating a new plan for the resource specified in the to field, Terraform checks the state for an existing object at the address specified in the from field. Terraform renames existing objects to the string specified in the to field and then creates a plan. The plan directs Terraform to provision the resource specified in the from field as the resource specified in the to field. As a result, Terraform does not destroy the resource during the Terraform run.\n\nExample\n\nThe following example moves an AWS instance from address aws_instance.a to aws_instance.b:\n\nmoved {\n\n  from = aws_instance.a\n\n  to   = aws_instance.b\n\n}\n\nCopy\nEdit this page on GitHub\n\nOn this page:\n\nMoved block configuration reference\nIntroduction\nConfiguration model\nComplete configuration\nSpecification\nExample\nTheme\nDark\nLight\nSystem\nCertifications\nSystem Status\nCookie Manager\nTerms of Use\nSecurity\nPrivacy\nTrademark Policy\nTrade Controls\nGive Feedback\n(opens in new tab)\nWe use cookies & other similar technology to collect data to improve your experience on our site, as described in our Privacy Policy and Cookie Policy.\nManage PreferencesACCEPT"
  },
  {
    "title": "Providers - Configuration Language | Terraform | HashiCorp Developer",
    "url": "https://developer.hashicorp.com/terraform/language/providers",
    "html": "Skip to main content\nNews\nHashiCorp to join IBM to accelerate multi-cloud automation.\nRead the blog →\nDismiss alert\nTerraform\nInstall\nTutorials\nDocumentation\nRegistry\n(opens in new tab)\nTry Cloud\n(opens in new tab)\nSearch\nCommand or control key\nK key\nTerraform Home\nConfiguration Language\nConfiguration Language\nStyle Guide\nFiles and Directories\nSyntax\nResources\nData Sources\nProviders\nOverview\nProvider Configuration\nProvider Requirements\nDependency Lock File\nVariables and Outputs\nModules\nMoved block\nChecks\nImport\nExpressions\nFunctions\nTerraform Settings\nState\nTests\nUpgrading to Terraform v1.7\nv1.x Compatibility Promises\nTerraform Internals\nResources\nTutorial Library\nCertifications\nCommunity Forum\n(opens in new tab)\nSupport\n(opens in new tab)\nGitHub\n(opens in new tab)\nTerraform Registry\n(opens in new tab)\nDeveloper\nTerraform\nConfiguration Language\nProviders\nv1.8.x (latest)\nProviders\n\nHands-on: Try the Perform CRUD Operations with Providers tutorial.\n\nTerraform relies on plugins called providers to interact with cloud providers, SaaS providers, and other APIs.\n\nTerraform configurations must declare which providers they require so that Terraform can install and use them. Additionally, some providers require configuration (like endpoint URLs or cloud regions) before they can be used.\n\nWhat Providers Do\n\nEach provider adds a set of resource types and/or data sources that Terraform can manage.\n\nEvery resource type is implemented by a provider; without providers, Terraform can't manage any kind of infrastructure.\n\nMost providers configure a specific infrastructure platform (either cloud or self-hosted). Providers can also offer local utilities for tasks like generating random numbers for unique resource names.\n\nWhere Providers Come From\n\nProviders are distributed separately from Terraform itself, and each provider has its own release cadence and version numbers.\n\nThe Terraform Registry is the main directory of publicly available Terraform providers, and hosts providers for most major infrastructure platforms.\n\nProvider Documentation\n\nEach provider has its own documentation, describing its resource types and their arguments.\n\nThe Terraform Registry includes documentation for a wide range of providers developed by HashiCorp, third-party vendors, and our Terraform community. Use the \"Documentation\" link in a provider's header to browse its documentation.\n\nProvider documentation in the Registry is versioned; you can use the version menu in the header to change which version you're viewing.\n\nFor details about writing, generating, and previewing provider documentation, see the provider publishing documentation.\n\nHow to Use Providers\n\nProviders are released separately from Terraform itself and have their own version numbers. In production we recommend constraining the acceptable provider versions in the configuration's provider requirements block, to make sure that terraform init does not install newer versions of the provider that are incompatible with the configuration.\n\nTo use resources from a given provider, you need to include some information about it in your configuration. See the following pages for details:\n\nProvider Requirements documents how to declare providers so Terraform can install them.\n\nProvider Configuration documents how to configure settings for providers.\n\nDependency Lock File documents an additional HCL file that can be included with a configuration, which tells Terraform to always use a specific set of provider versions.\n\nProvider Installation\n\nHCP Terraform and Terraform Enterprise install providers as part of every run.\n\nTerraform CLI finds and installs providers when initializing a working directory. It can automatically download providers from a Terraform registry, or load them from a local mirror or cache. If you are using a persistent working directory, you must reinitialize whenever you change a configuration's providers.\n\nTo save time and bandwidth, Terraform CLI supports an optional plugin cache. You can enable the cache using the plugin_cache_dir setting in the CLI configuration file.\n\nTo ensure Terraform always installs the same provider versions for a given configuration, you can use Terraform CLI to create a dependency lock file and commit it to version control along with your configuration. If a lock file is present, HCP Terraform, CLI, and Enterprise will all obey it when installing providers.\n\nHands-on: Try the Lock and Upgrade Provider Versions tutorial.\n\nHow to Find Providers\n\nTo find providers for the infrastructure platforms you use, browse the providers section of the Terraform Registry.\n\nSome providers on the Registry are developed and published by HashiCorp, some are published by platform maintainers, and some are published by users and volunteers. The provider listings use the following badges to indicate who develops and maintains a given provider.\n\nTier\tDescription\tNamespace\n\nOfficial\n\tOfficial providers are owned and maintained by HashiCorp\thashicorp\n\nPartner\n\tPartner providers are written, maintained, validated and published by third-party companies against their own APIs. To earn a partner provider badge the partner must participate in the HashiCorp Technology Partner Program.\tThird-party organization, e.g. mongodb/mongodbatlas\n\nCommunity\n\tCommunity providers are published to the Terraform Registry by individual maintainers, groups of maintainers, or other members of the Terraform community.\tMaintainer’s individual or organization account, e.g. DeviaVir/gsuite\n\nArchived\n\tArchived Providers are Official or Partner Providers that are no longer maintained by HashiCorp or the community. This may occur if an API is deprecated or interest was low.\thashicorp or third-party\n\nHow to Develop Providers\n\nProviders are written in Go, using the Terraform Plugin SDK. For more information on developing providers, see:\n\nThe Plugin Development documentation\nThe Call APIs with Terraform Providers tutorials\nEdit this page on GitHub\n\nOn this page:\n\nProviders\nWhat Providers Do\nWhere Providers Come From\nProvider Documentation\nHow to Use Providers\nProvider Installation\nHow to Find Providers\nHow to Develop Providers\nTheme\nDark\nLight\nSystem\nCertifications\nSystem Status\nCookie Manager\nTerms of Use\nSecurity\nPrivacy\nTrademark Policy\nTrade Controls\nGive Feedback\n(opens in new tab)\nWe use cookies & other similar technology to collect data to improve your experience on our site, as described in our Privacy Policy and Cookie Policy.\nManage PreferencesACCEPT"
  },
  {
    "title": "Style Guide - Configuration Language | Terraform | HashiCorp Developer",
    "url": "https://developer.hashicorp.com/terraform/language/style",
    "html": "Skip to main content\nNews\nHashiCorp to join IBM to accelerate multi-cloud automation.\nRead the blog →\nDismiss alert\nTerraform\nInstall\nTutorials\nDocumentation\nRegistry\n(opens in new tab)\nTry Cloud\n(opens in new tab)\nSearch\nCommand or control key\nK key\nTerraform Home\nConfiguration Language\nConfiguration Language\nStyle Guide\nFiles and Directories\nSyntax\nResources\nData Sources\nProviders\nVariables and Outputs\nModules\nMoved block\nChecks\nImport\nExpressions\nFunctions\nTerraform Settings\nState\nTests\nUpgrading to Terraform v1.7\nv1.x Compatibility Promises\nTerraform Internals\nResources\nTutorial Library\nCertifications\nCommunity Forum\n(opens in new tab)\nSupport\n(opens in new tab)\nGitHub\n(opens in new tab)\nTerraform Registry\n(opens in new tab)\nDeveloper\nTerraform\nConfiguration Language\nStyle Guide\nv1.8.x (latest)\nStyle Guide\n\nThe flexibility of Terraform's configuration language gives you many options to choose from as you write your code, structure your directories, and test your configuration. While some design decisions depend on your organization's needs or preferences, there are some common patterns that we suggest you adopt. Adopting and adhering to a style guide keeps your Terraform code legible, scalable, and maintainable.\n\nThis article discusses best practices and some considerations to keep in mind as you develop your organization's style guide. The article is split into two sections. The first section covers code style recommendations, such as formatting and resource organization. The second section covers operations and workflow recommendations, such as lifecycle management through meta-arguments, versioning, and sensitive data management.\n\nCode style\n\nWriting Terraform code in a consistent style makes it easier to read and maintain. The following sections discuss code style recommendations, including the following:\n\nRun terraform fmt and terraform validate before committing your code to version control.\nUse a linter such as TFLint to enforce your organization's own coding best practices.\nUse # for single and multi-line comments.\nUse nouns for resource names and do not include the resource type in the name.\nUse underscores to separate multiple words in names. Wrap the resource type and name in double quotes in your resource definition.\nLet your code build on itself: define dependent resources after the resources that reference them.\nInclude a type and description for every variable.\nInclude a description for every output.\nAvoid overuse of variables and local values.\nAlways include a default provider configuration.\nUse count and for_each sparingly.\nCode formatting\n\nThe Terraform parser allows you some flexibility in how you lay out the elements in your configuration files, but the Terraform language also has some idiomatic style conventions which we recommend users always follow for consistency between files and modules written by different teams.\n\nIndent two spaces for each nesting level\n\nWhen multiple arguments with single-line values appear on consecutive lines at the same nesting level, align their equals signs:\n\nami           = \"abc123\"\n\ninstance_type = \"t2.micro\"\n\n\nWhen both arguments and blocks appear together inside a block body, place all of the arguments together at the top and then place nested blocks below them. Use one blank line to separate the arguments from the blocks.\n\nUse empty lines to separate logical groups of arguments within a block.\n\nFor blocks that contain both arguments and \"meta-arguments\" (as defined by the Terraform language semantics), list meta-arguments first and separate them from other arguments with one blank line. Place meta-argument blocks last and separate them from other blocks with one blank line. Refer to dynamic resource count for more information on meta-arguments.\n\nresource \"aws_instance\" \"example\" {\n\n  # meta-argument first\n\n  count = 2\n\n\n\n  ami           = \"abc123\"\n\n  instance_type = \"t2.micro\"\n\n\n\n  network_interface {\n\n    # ...\n\n  }\n\n\n\n  # meta-argument block last\n\n  lifecycle {\n\n    create_before_destroy = true\n\n  }\n\n}\n\n\nTop-level blocks should always be separated from one another by one blank line. Nested blocks should also be separated by blank lines, except when grouping together related blocks of the same type (like multiple provisioner blocks in a resource).\n\nAvoid grouping multiple blocks of the same type with other blocks of a different type, unless the block types are defined by semantics to form a family. (For example: root_block_device, ebs_block_device and ephemeral_block_device on aws_instance form a family of block types describing AWS block devices, and can therefore be grouped together and mixed.)\n\nThe terraform fmt command formats your Terraform configuration to a subset of the above recommendations. By default, the terraform fmt command will only modify your Terraform code in the directory that you execute it in, but you can include the -recursive flag to modify code in all subdirectories as well.\n\nWe recommend that you run terraform fmt before each commit to version control. You can use mechanisms such as Git pre-commit hooks to automatically run this command each time you commit your code.\n\nIf you use Microsoft VS Code, use the Terraform VS Code extension to enable features such as syntax highlighting and validation, automatic code formatting, and integration with HCP Terraform. If your development environment or text editor supports the Language Server Protocol, you can use the Terraform Language Server to access most of the VS Code extension features.\n\nCode validation\n\nThe terraform validate command checks that your configuration is syntactically valid and internally consistent. The validate command does not check if argument values are valid for a specific provider, but it will verify that they are the correct type. It does not evaluate any existing state.\n\nThe terraform validate command is safe to run automatically and frequently. You can configure your text editor to run this command as a post-save check, define it as a pre-commit hook in a Git repository, or run it as a step in a CI/CD pipeline.\n\nFor more information, refer to the Terraform validate documentation.\n\nFile names\n\nWe recommend the following file naming conventions:\n\nA backend.tf file that contains your backend configuration. You can define multiple terraform blocks in your configuration to separate your backend configuration from your Terraform and provider versioning configuration.\nA main.tf file that contains all resource and data source blocks.\nA outputs.tf file that contains all output blocks in alphabetical order.\nA providers.tf file that contains all provider blocks and configuration.\nA terraform.tf file that contains a single terraform block which defines your required_version and required_providers.\nA variables.tf file that contains all variable blocks in alphabetical order.\nA locals.tf file that contains local values. Refer to local values for more information.\nA override.tf file that contains override definitions for your configuration. Terraform loads this and all files ending with _override.tf last. Use them sparingly and add comments to the original resource definitions, as these overrides make your code harder to reason about. Refer to the override files documentation for more information.\n\nAs your codebase grows, limiting it to just these files can become difficult to maintain. If your code becomes hard to navigate due to its size, we recommend that you organize resources and data sources in separate files by logical groups. For example, if your web application requires networking, storage, and compute resources, you might create the following files:\n\nA network.tf file that contains your VPC, subnets, load balancers, and all other networking resources.\nA storage.tf file that contains your object storage and related permissions configuration.\nA compute.tf file that contains your compute instances.\n\nNo matter how you decide to split your code, it should be immediately clear where a maintainer can find a specific resource or data source definition.\n\nAs your configuration grows, you may need to separate it into multiple state files. The HashiCorp Well-Architected Framework provides more guidance about configuration structure and scope.\n\nLinting and static code analysis\n\nTerraform does not have a built-in linter, but many organizations rely on a third party linting tool such as TFLint to enforce code standards. A linter uses static code analysis to compare your Terraform code against a set of rules. Most linters ship with a default set of rules, but also let you write your own.\n\nComments\n\nWrite your code so it is easy to understand. Only when necessary, use comments to clarify complexity for other maintainers.\n\nUse # for both single- and multi-line comments. The // and /* */ comment syntaxes are not considered idiomatic, but Terraform supports them to remain backwards-compatible with earlier versions of HCL.\n\n# Each tunnel is responsible for encrypting and decrypting traffic exiting\n\n# and leaving its associated gateway.\n\nresource \"google_compute_vpn_tunnel\" \"tunnel1\" {\n\n  ## ...\n\nResource naming\n\nEvery resource within a configuration must have a unique name. For consistency and readability, use a descriptive noun and separate words with underscores. Do not include the resource type in the resource identifier since the resource address already includes it. Wrap the resource type and name in double quotes.\n\n❌ Bad:\n\nresource aws_instance webAPI-aws-instance {...}\n\n\n✅ Good:\n\nresource \"aws_instance\" \"web_api\" {...}\n\nResource order\n\nThe order of the resources and data sources in your code does not affect how Terraform builds them, so organize your resources for readability. Terraform determines the creation order based on cross-resource dependencies.\n\nHow you order your resources largely depends on the size and complexity of your code, but we recommend defining data sources alongside the resources that reference them. For readability, your Terraform code should “build on itself” — you should define a data source before the resource that references it.\n\nThe following example defines an aws_instance that relies on two data sources, aws_ami and aws_availability_zone. For readability and continuity, it defines the data sources before the aws_instance resource.\n\ndata \"aws_ami\" \"web\" {\n\n  ##...\n\n}\n\n\n\ndata \"aws_availability_zones\" \"available\" {\n\n  ##...\n\n}\n\n\n\nresource \"aws_instance\" \"web\" {\n\n  ami               = data.aws_ami.web.id\n\n  availability_zone = data.aws_availability_zones.available.names[0]\n\n  ##...\n\n}\n\n\nWe recommend following a consistent order for resource parameters:\n\nIf present, The count or for_each meta-argument.\nResource-specific non-block parameters.\nResource-specific block parameters.\nIf required, a lifecycle block.\nIf required, the depends_on parameter.\nVariables\n\nWhile variables make your modules more flexible, overusing variables can make code difficult to understand. When deciding whether to expose a variable for a resource setting, consider whether that parameter will change between deployments.\n\nDefine a type and a description for every variable.\n\nIf the variable is optional, define a reasonable default.\n\nFor sensitive variables, such as passwords and private keys, set the sensitive parameter to true. Remember that Terraform will still store this value in plain text in its state, but it will not display it when you run terraform plan or terraform apply. Refer to secrets management for more information on how to securely handle sensitive values.\n\nUse input variable validation to create additional rules for your variable values in addition to Terraform's type validation. Only use variable validation when your variable values have uniquely restrictive requirements. For example, if your Terraform configuration requires two web instances, add a validation block to enforce it:\n\nvariable \"web_instance_count\" {\n\n  type        = number\n\n  description = \"Number of web instances to deploy. This application requires at least two instances.\"\n\n\n\n  validation {\n\n    condition     = var.web_instance_count > 1\n\n    error_message = \"This application requires at least two web instances.\"\n\n  }\n\n}\n\n\nWe recommend following a consistent order for variable parameters:\n\nType\nDescription\nDefault (optional)\nSensitive (optional)\nValidation blocks\nOutputs\n\nOutput values let you expose data about your infrastructure on the command line and make it easy to reference in other Terraform configurations. Like you would for variables, provide a description for each output.\n\nWe recommend that you use the following order for your output parameters:\n\nDescription\nValue\nSensitive (optional)\n\nEvery variable and output requires a unique name. For consistency and readability, we recommend that you use a descriptive noun and separate words with underscores.\n\nvariable \"db_disk_size\" {\n\n type        = number\n\n description = \"Disk size for the API database\"\n\n default     = 100\n\n}\n\n\n\nvariable \"db_password\" {\n\n type        = string\n\n description = \"Database password\"\n\n sensitive   = true\n\n}\n\n\n\noutput \"web_public_ip\" {\n\n description = \"Public IP of the web instance\"\n\n value       = aws_instance.web.public_ip\n\n}\n\nLocal values\n\nLocal values let you reference an expression or value multiple times. Use local values sparingly, as overuse can make your code harder to understand.\n\nFor example, you can use a local value to create a suffix for the region and environment (for example, development or test), and append it to multiple resources.\n\nlocals {\n\n  name_suffix = \"${var.region}-${var.environment}\"\n\n}\n\n\n\nresource \"aws_instance\" \"web\" {\n\n  ami           = data.aws_ami.ubuntu.id\n\n  instance_type = \"t3.micro\"\n\n\n\n  tags = {\n\n    Name = \"web-${local.name_suffix}\"\n\n  }\n\n}\n\n\nDefine local values in one of two places:\n\nIf you reference the local value in multiple files, define it in a file named locals.tf.\nIf the local is specific to a file, define it at the top of that file.\n\nAs for other Terraform objects, use descriptive nouns for local value names and underscores to separate multiple words.\n\nFor more information, refer to the local values documentation and the Simplify Terraform configuration with locals tutorial.\n\nProvider aliasing\n\nProvider aliasing lets you define multiple provider blocks for the same Terraform provider. Potential use cases for aliases include provisioning resources in multiple regions within a single configuration. The provider meta-argument for resources and the providers meta-argument for modules specifies which provider to use.\n\nproviders.tf\nprovider \"aws\" {\n\n  region = \"us-east-1\"\n\n}\n\n\n\nprovider \"aws\" {\n\n  alias  = \"west\"\n\n  region = \"us-west-2\"\n\n}\n\nmain.tf\nresource \"aws_instance\" \"example\" {\n\n  provider = aws.west\n\n  # ...\n\n}\n\n\n\nmodule \"aws_vpc\" {\n\n  source = \"./aws_vpc\"\n\n  providers = {\n\n    aws = aws.west\n\n  }\n\n}\n\nAny provider block that does not define the alias parameter is the default provider configuration.\nAlways include a default provider configuration and define all of your providers in the same file.\nIf you define multiple instances of a provider, define the default first.\nFor non-default providers, define the alias as the first parameter of the provider block.\nDynamic resource count\n\nThe for_each and count meta-arguments let you create multiple resources from a single resource block depending on run-time conditions. You can use these meta-arguments to make your code flexible and reduce duplicate resource blocks. If the resources are almost identical, use count. If some of arguments need distinct values that you cannot derive from an integer, use for_each.\n\nThe for_each meta-argument accepts a map or set value, and Terraform will create an instance of that resource for each element in the value you provide. In the following example, Terraform creates an aws_instance for each of the strings defined in the web_instances variable: \"ui\", \"api\", \"db\" and \"metrics\". The example uses each.key to give each instance a unique name. The web_private_ips output uses a for expression to create a map of instance names and their private IP addresses, while the web_ui_public_ip output addresses the instance with the key \"ui\" directly.\n\nvariable \"web_instances\" {\n\n  type        = list(string)\n\n  description = \"A list of instances for the web application\"\n\n  default = [\n\n    \"ui\",\n\n    \"api\",\n\n    \"db\",\n\n    \"metrics\"\n\n  ]\n\n}\n\nresource \"aws_instance\" \"web\" {\n\n  for_each = toset(var.web_instances)\n\n  ami           = data.aws_ami.webapp.id\n\n  instance_type = \"t3.micro\"\n\n  tags = {\n\n    Name = \"web_${each.key}\"\n\n  }\n\n}\n\noutput \"web_private_ips\" {\n\n  description = \"Private IPs of the web instances\"\n\n  value = {\n\n    for k, v in aws_instance.web : k => v.private_ip\n\n  }\n\n}\n\noutput \"web_ui_public_ip\" {\n\n  description = \"Public IP of the web UI instance\"\n\n  value       = aws_instance.web[\"ui\"].public_ip\n\n}\n\n\nThe above example will create the following output:\n\nweb_private_ips = {\n\n  \"api\" = \"172.31.25.29\"\n\n  \"db\" = \"172.31.18.33\"\n\n  \"metrics\" = \"172.31.26.112\"\n\n  \"ui\" = \"172.31.20.142\"\n\n}\n\nweb_ui_public_ip = \"18.216.208.182\"\n\n\nRefer to the for_each meta-argument documentation for more examples.\n\nThe count meta-argument lets you create multiple instances of a resource from a single resource block. Refer to the count meta-argument documentation for examples.\n\nA common practice to conditionally create resources is to use the count meta-argument with a conditional expression. In the following example, Terraform will only create the aws_instance if var.enable_metrics is true.\n\nvariable \"enable_metrics\" {\n\n  description = \"True if the metrics server should be deployed\"\n\n  type        = bool\n\n  default     = true\n\n}\n\n\n\nresource \"aws_instance\" \"web\" {\n\n  count = var.enable_metrics ? 1 : 0\n\n\n\n  ami           = data.aws_ami.webapp.id\n\n  instance_type = \"t3.micro\"\n\n  ##...\n\n}\n\n\nMeta-arguments simplify your code but add complexity, so use them in moderation. If the effect of the meta-argument is not immediately obvious, use a comment for clarification.\n\nTo learn more about these meta-arguments, refer to the for_each and count documentation.\n\n.gitignore\n\nDefine a .gitignore file for your repository to exclude files that you should not publish to version control, such as your state file.\n\nDo not commit:\n\nYour terraform.tfstate state file, including terraform.tfstate.* backup state files.\nYour .terraform.tfstate.lock.info file. Terraform creates and deletes this file automatically when you run a terraform apply command and contains info about your state lock\nYour .terraform directory, where Terraform downloads providers and child modules. Saved plan files that you create when you include the -out flag when you run terraform plan.\nAny .tfvars files that contain sensitive information.\n\nAlways commit:\n\nAll Terraform code files\nYour .terraform.lock.hcl dependency lock file\nA .gitignore file that excludes the files listed below\nA README.md to describe the code, input variables, and outputs\n\nFor an example, refer to GitHub's Terraform .gitignore file.\n\nWorkflow style\n\nThis section reviews standards that enable predictable and secure Terraform workflows, such as:\n\nPin your Terraform, provider, and module versions.\nName your module repositories using this three-part name terraform-<PROVIDER>-<NAME> when using the HCP Terraform registry.\nStore local modules at ./modules/<module_name>.\nUse the tfe_outputs data source or provider-specific data sources to share state between two state files.\nProtect credentials by using dynamic provider credentials or a secrets manager such as HashiCorp Vault.\nWrite tests for your modules.\nUse policy enforcement on HCP Terraform to set guardrails for infrastructure operations.\nVersion pinning\n\nTo prevent providers and modules upgrades from introducing unintentional changes to your infrastructure, use version pinning.\n\nSpecify provider versions using the required_providers block. Terraform version constraints support a range of accepted versions.\n\nPin modules to a specific major and minor version as shown in the example below to ensure stability. You can use looser restrictions if you are certain that the module does not introduce breaking changes outside of major version updates.\n\nWe also recommend that you set a minimum required version of the Terraform binary using the required_version in your terraform block. This requires all operators to use a Terraform version that has all of your configuration's required features.\n\nterraform {\n\n  required_providers {\n\n    aws = {\n\n      source  = \"hashicorp/aws\"\n\n      version = \"5.34.0\"\n\n    }\n\n  }\n\n\n\n  required_version = \">= 1.7\"\n\n}\n\n\nThe above example pins the version of the hashicorp/aws provider to version 5.34.0, and requires that operators use Terraform 1.7 or newer.\n\nFor modules sourced from a registry, use the version parameter in the module block to pin the version. For local modules, Terraform ignores the version parameter.\n\nmodule \"vault_starter\" {\n\n  source  = \"hashicorp/vault-starter/aws\"\n\n  version = \"1.0.0\"\n\n  ##...\n\n}\n\nModule repository names\n\nThe Terraform registry requires that repositories match a naming convention for all modules that you publish to the registry. Module repositories must use this three-part name terraform-<PROVIDER>-<NAME>, where <NAME> reflects the type of infrastructure the module manages and <PROVIDER> is the main provider the module uses. The <NAME> segment can contain additional hyphens, for example, terraform-google-vault or terraform-aws-ec2-instance.\n\nModule structure\n\nTerraform modules define self-contained, reusable pieces of infrastructure-as-code.\n\nUse modules to group together logically related resources that you need to provision together. For example:\n\nA networking module that defines a VPC, along with its subnets, gateway, and security groups.\nAn application module defining all resources required for each deployment. This stack could include web servers, databases, storage, and supported networking.\n\nReview the module creation recommended pattern documentation and standard module structure for guidance on how to structure your modules.\n\nLocal modules\n\nLocal modules are sourced from local disk rather than a remote module registry. We recommend publishing your modules to a module registry, such as the HCP Terraform private registry, to easily version, share, and reuse modules across your organization. If you cannot use a module registry, using local modules can simplify maintaining and updating your code.\n\nWe recommend that you define child modules in the ./modules/<module_name> directory.\n\nRepository structure\n\nHow you structure your modules and Terraform configuration in version control significantly impacts versioning and operations. We recommend that you store your actual infrastructure configuration separately from your module code.\n\nStore each module in an individual repository. This lets you independently version each module and makes it easier to publish your modules in the private Terraform registry.\n\nOrganize your infrastructure configuration in repositories that group together logically-related resources. For example, a single repository for a web application that requires compute, networking, and database resources . By separating your resources into groups, you limit the number of resources that may be impacted by failures for any operation.\n\nAnother approach is to group all modules and infrastructure configuration into a single monolithic repository, or monorepo. For example, a monorepo may define a collection of local modules for each component of the infrastructure stack, and deploy them in the root module.\n\n.\n\n├── modules\n\n│   ├── function\n\n│   │   ├── main.tf      # contains aws_iam_role, aws_lambda_function\n\n│   │   ├── outputs.tf\n\n│   │   └── variables.tf\n\n│   ├── queue\n\n│   │   ├── main.tf      # contains aws_sqs_queue\n\n│   │   ├── outputs.tf\n\n│   │   └── variables.tf\n\n│   └── vpc\n\n│       ├── main.tf      # contains aws_vpc, aws_subnet\n\n│       ├── outputs.tf\n\n│       └── variables.tf\n\n├── main.tf\n\n├── outputs.tf\n\n└── variables.tf\n\n\nThe advantage of monolithic repositories is having a single source of truth that tracks every infrastructure change. However, monolithic repositories can complicate your CI/CD automation: since any code change triggers a deployment that operates on your entire repository, your workflow must target only the modified directories. You also lose the granular access control, since anyone with repository access can modify any file in it.\n\nIf your organization requires a monolithic approach, HCP Terraform and Terraform Enterprise let you scope a workspace to a specific directory in a repository, simplifying your workflows.\n\nBranching strategy\n\nTo collaborate on your Terraform code, we recommend using the GitHub flow. This approach uses short-lived branches to help your team quickly review, test, and merge changes to your code. To make changes to your code, you would:\n\nCreate a new branch from your main branch\nWrite, commit, and push your changes to the new branch\nCreate a pull request\nReview the changes with your team\nMerge the pull request\nDelete the branch\n\nHCP Terraform and Terraform Enterprise can run speculative plans for pull requests. These speculative plans run automatically when you create or update a pull request, and you can use them to see the effect that your changes will have on your infrastructure before you merge them to your main branch. When you merge your pull request, HCP Terraform will start a new run to apply these changes.\n\nMultiple environments\n\nWe recommend that your repository's main branch be the source of truth for all environments. For HCP Terraform and Terraform Enterprise users, we recommend that you use separate workspaces for each environment. For larger codebases, we recommend that you split your resources across multiple workspaces to prevent large state files and limit unintended consequences from changes. For example, you could structure your code as follows:\n\n.\n\n├── compute\n\n│   ├── main.tf\n\n│   ├── outputs.tf\n\n│   └── variables.tf\n\n├── database\n\n│   ├── main.tf\n\n│   ├── outputs.tf\n\n│   └── variables.tf\n\n└── networking\n\n    ├── main.tf\n\n    ├── outputs.tf\n\n    └── variables.tf\n\n\nIn this scenario, you would create three workspaces per environment. For example, your production environment would have a prod-compute, prod-database, and prod-networking workspace. Read more about Terraform workspace and project best practices.\n\nIf you do not use HCP Terraform or Terraform Enterprise, we recommend that you use modules to encapsulate your configuration, and use a directory for each environment so that each one has a separate state file. The configuration in each of these directories would call the local modules, each with parameters specific to their environment. This also lets you maintain separate variable and backend configurations for each environment.\n\n├── modules\n\n│   ├── compute\n\n│   │   └── main.tf\n\n│   ├── database\n\n│   │   └── main.tf\n\n│   └── network\n\n│       └── main.tf\n\n├── dev\n\n│   ├── backend.tf\n\n│   ├── main.tf\n\n│   └── variables.tf\n\n├── prod\n\n│   ├── backend.tf\n\n│   ├── main.tf\n\n│   └── variables.tf\n\n└── staging\n\n    ├── backend.tf\n\n    ├── main.tf\n\n    └── variables.tf\n\nState sharing\n\nSince your state contains sensitive information, avoid sharing full state files when possible.\n\nIf you use HCP Terraform or Terraform Enterprise and need to reference resources across workspaces, use the tfe_outputs data source.\n\nIf you do not use HCP Terraform or Terraform Enterprise but still need to reference data about other infrastructure resources, use data sources to query the provider. For example, you can use the aws_instance data source to look up an AWS EC2 instance by its ID or tags.\n\nSecrets management\n\nIf you do not configure remote state storage, the Terraform CLI stores the entire state in plaintext on the local disk. State can include sensitive data, such as passwords and private keys. HCP Terraform and Terraform Enterprise provide state encryption through HashiCorp Vault.\n\nIf you use HCP Terraform or Terraform Enterprise, we recommend the following:\n\nWhen using Terraform Enterprise, define and enforce a Sentinel policy to prevent use of the local_exec provisioner or external data sources.\nWhen using HCP Terraform or Terraform Enterprise, use dynamic provider credentials to avoid using long-lived static credentials.\n\nIf you use Terraform Community Edition, we recommend the following:\n\nConfigure provider credentials using provider-specific environment variables.\nAccess secrets from a secrets management system such as HashiCorp Vault with the Terraform Vault provider. Be aware that Terraform will still write these values in plaintext to your state file.\n\nIf you use a custom CI/CD pipeline, review your CI/CD tool's best practices for managing sensitive values. Most tools let you access sensitive values as environment variables. For more information, refer to your CI/CD documentation.\n\nUsing secrets in GitHub Actions\nGitlab pipeline security\nIntegrate Vault into your CI/CD pipeline\nIntegration and unit testing\n\nTerraform tests let you validate your modules and catch breaking changes. We recommend that you write tests for your Terraform modules and run them just as you run your tests for your application code, such as pre-merge check in your pull requests or as a prerequisite step in your automated CI/CD pipeline.\n\nTests differ from validation methods such as variable validation, preconditions, postconditions, and check blocks. These features focus on verifying the infrastructure deployed by your code, while tests validate the behavior and logic of your code itself. For more information, refer to the Terraform test documentation and the Write Terraform tests tutorial.\n\nPolicy\n\nPolicies are rules that HCP Terraform enforces on Terraform runs. You can use policies to validate that the Terraform plan complies with your organization's best practices. For example, you can write policies that:\n\nLimit the size of a web instance\nCheck for required resource tags\nBlock deployments on Fridays\nEnforce security configuration and cost management\n\nWe recommend that you store policies in a separate VCS repository from your Terraform code.\n\nFor more information, refer to the policy enforcement documentation, as well as the enforce policy with Sentinel and detect infrastructure drift and enforce OPA policies tutorials.\n\nNext steps\n\nThis article introduces some considerations to keep in mind as you standardize your organization's Terraform style guidelines. Enforcing a standard way of writing and organizing your Terraform code across your organization ensures that it is readable, maintainable, and shareable.\n\nThe HashiCorp Well-Architected Framework provides more guidance on adapting your Terraform workflows for scale.\n\nEdit this page on GitHub\n\nOn this page:\n\nStyle Guide\nCode style\nCode formatting\nCode validation\nFile names\nLinting and static code analysis\nComments\nResource naming\nResource order\nVariables\nOutputs\nLocal values\nProvider aliasing\nDynamic resource count\n.gitignore\nWorkflow style\nVersion pinning\nModule repository names\nModule structure\nLocal modules\nRepository structure\nBranching strategy\nMultiple environments\nState sharing\nSecrets management\nIntegration and unit testing\nPolicy\nNext steps\nTheme\nDark\nLight\nSystem\nCertifications\nSystem Status\nCookie Manager\nTerms of Use\nSecurity\nPrivacy\nTrademark Policy\nTrade Controls\nGive Feedback\n(opens in new tab)\nWe use cookies & other similar technology to collect data to improve your experience on our site, as described in our Privacy Policy and Cookie Policy.\nManage PreferencesACCEPT"
  },
  {
    "title": "Overview - Configuration Language | Terraform | HashiCorp Developer",
    "url": "https://developer.hashicorp.com/terraform/language",
    "html": "Skip to main content\nNews\nHashiCorp to join IBM to accelerate multi-cloud automation.\nRead the blog →\nDismiss alert\nTerraform\nInstall\nTutorials\nDocumentation\nRegistry\n(opens in new tab)\nTry Cloud\n(opens in new tab)\nSearch\nCommand or control key\nK key\nTerraform Home\nConfiguration Language\nConfiguration Language\nStyle Guide\nFiles and Directories\nSyntax\nResources\nData Sources\nProviders\nVariables and Outputs\nModules\nMoved block\nChecks\nImport\nExpressions\nFunctions\nTerraform Settings\nState\nTests\nUpgrading to Terraform v1.7\nv1.x Compatibility Promises\nTerraform Internals\nResources\nTutorial Library\nCertifications\nCommunity Forum\n(opens in new tab)\nSupport\n(opens in new tab)\nGitHub\n(opens in new tab)\nTerraform Registry\n(opens in new tab)\nDeveloper\nTerraform\nConfiguration Language\nv1.8.x (latest)\nTerraform Language Documentation\n\nUse the Terraform configuration language to describe the infrastructure that Terraform manages.\n\nThis is the documentation for Terraform's configuration language. It is relevant to users of Terraform CLI, HCP Terraform, and Terraform Enterprise. Terraform's language is its primary user interface. Configuration files you write in Terraform language tell Terraform what plugins to install, what infrastructure to create, and what data to fetch. Terraform language also lets you define dependencies between resources and create multiple similar resources from a single configuration block.\n\nHands-on: Try the Write Terraform Configuration tutorials.\n\nAbout the Terraform Language\n\nThe main purpose of the Terraform language is declaring resources, which represent infrastructure objects. All other language features exist only to make the definition of resources more flexible and convenient.\n\nA Terraform configuration is a complete document in the Terraform language that tells Terraform how to manage a given collection of infrastructure. A configuration can consist of multiple files and directories.\n\nThe syntax of the Terraform language consists of only a few basic elements:\n\nresource \"aws_vpc\" \"main\" {\n\n  cidr_block = var.base_cidr_block\n\n}\n\n\n\n<BLOCK TYPE> \"<BLOCK LABEL>\" \"<BLOCK LABEL>\" {\n\n  # Block body\n\n  <IDENTIFIER> = <EXPRESSION> # Argument\n\n}\n\nCopy\nBlocks are containers for other content and usually represent the configuration of some kind of object, like a resource. Blocks have a block type, can have zero or more labels, and have a body that contains any number of arguments and nested blocks. Most of Terraform's features are controlled by top-level blocks in a configuration file.\nArguments assign a value to a name. They appear within blocks.\nExpressions represent a value, either literally or by referencing and combining other values. They appear as values for arguments, or within other expressions.\n\nThe Terraform language is declarative, describing an intended goal rather than the steps to reach that goal. The ordering of blocks and the files they are organized into are generally not significant; Terraform only considers implicit and explicit relationships between resources when determining an order of operations.\n\nExample\n\nThe following example describes a simple network topology for Amazon Web Services, just to give a sense of the overall structure and syntax of the Terraform language. Similar configurations can be created for other virtual network services, using resource types defined by other providers, and a practical network configuration will often contain additional elements not shown here.\n\nterraform {\n\n  required_providers {\n\n    aws = {\n\n      source  = \"hashicorp/aws\"\n\n      version = \"~> 1.0.4\"\n\n    }\n\n  }\n\n}\n\n\n\nvariable \"aws_region\" {}\n\n\n\nvariable \"base_cidr_block\" {\n\n  description = \"A /16 CIDR range definition, such as 10.1.0.0/16, that the VPC will use\"\n\n  default = \"10.1.0.0/16\"\n\n}\n\n\n\nvariable \"availability_zones\" {\n\n  description = \"A list of availability zones in which to create subnets\"\n\n  type = list(string)\n\n}\n\n\n\nprovider \"aws\" {\n\n  region = var.aws_region\n\n}\n\n\n\nresource \"aws_vpc\" \"main\" {\n\n  # Referencing the base_cidr_block variable allows the network address\n\n  # to be changed without modifying the configuration.\n\n  cidr_block = var.base_cidr_block\n\n}\n\n\n\nresource \"aws_subnet\" \"az\" {\n\n  # Create one subnet for each given availability zone.\n\n  count = length(var.availability_zones)\n\n\n\n  # For each subnet, use one of the specified availability zones.\n\n  availability_zone = var.availability_zones[count.index]\n\n\n\n  # By referencing the aws_vpc.main object, Terraform knows that the subnet\n\n  # must be created only after the VPC is created.\n\n  vpc_id = aws_vpc.main.id\n\n\n\n  # Built-in functions and operators can be used for simple transformations of\n\n  # values, such as computing a subnet address. Here we create a /20 prefix for\n\n  # each subnet, using consecutive addresses for each availability zone,\n\n  # such as 10.1.16.0/20 .\n\n  cidr_block = cidrsubnet(aws_vpc.main.cidr_block, 4, count.index+1)\n\n}\n\nCopy\nEdit this page on GitHub\n\nOn this page:\n\nTerraform Language Documentation\nAbout the Terraform Language\nTheme\nDark\nLight\nSystem\nCertifications\nSystem Status\nCookie Manager\nTerms of Use\nSecurity\nPrivacy\nTrademark Policy\nTrade Controls\nGive Feedback\n(opens in new tab)\nWe use cookies & other similar technology to collect data to improve your experience on our site, as described in our Privacy Policy and Cookie Policy.\nManage PreferencesACCEPT"
  }
]